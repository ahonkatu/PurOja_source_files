{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800d699c",
   "metadata": {},
   "source": [
    "I decided to share this too despite the model needs more practising with RF. This saves the results as images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 22:02:11,240 - __main__ - INFO - Starting export process...\n",
      "2025-04-12 22:02:11,240 - __main__ - ERROR - Error during export: name 'predictions' is not defined\n",
      "2025-04-12 22:02:11,240 - __main__ - ERROR - Traceback (most recent call last):\n",
      "  File \"C:\\Users\\OMISTAJA\\AppData\\Local\\Temp\\ipykernel_11712\\3165423215.py\", line 977, in <module>\n",
      "    export_predictions(predictions, zone_boundaries, zone_name_to_id, output_path)\n",
      "NameError: name 'predictions' is not defined\n",
      "\n",
      "2025-04-12 22:02:11,240 - __main__ - INFO - Exporting predictions to GeoTIFF in ../02_Results/1204/\n",
      "2025-04-12 22:02:11,240 - __main__ - INFO - Fixing zarr file by adding missing features: zones_data_2.zarr\n",
      "2025-04-12 22:02:11,341 - __main__ - INFO - Finished checking/fixing features in zones_data_2.zarr\n",
      "2025-04-12 22:02:11,342 - __main__ - INFO - Processing zone: zone_1\n",
      "2025-04-12 22:02:11,342 - __main__ - INFO - Predict for new zone zone_1 using models from ../02_Results/models\n",
      "2025-04-12 22:02:11,342 - __main__ - INFO - Loading and fixing data for zone_1\n",
      "2025-04-12 22:02:18,336 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:02:19,182 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:02:21,171 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:02:21,670 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:02:22,140 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:02:22,409 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:02:22,456 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:02:22,480 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:02:22,614 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:02:22,624 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:02:22,660 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:02:22,662 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:02:22,662 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:02:23,541 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:02:23,541 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:03:03,769 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:03:43,667 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:04:23,166 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 22:04:59,435 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 22:05:06,333 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 22:05:13,105 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 22:05:36,488 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 22:05:36,488 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 22:05:43,245 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 22:05:50,050 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 22:07:53,815 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_1\n",
      "2025-04-12 22:07:55,215 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_1.png\n",
      "2025-04-12 22:08:07,631 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 22:08:07,983 - __main__ - INFO - Memory usage: 1850.39 MB\n",
      "2025-04-12 22:08:07,983 - __main__ - INFO - Successfully processed zone: zone_1\n",
      "2025-04-12 22:08:07,984 - __main__ - INFO - Processing zone: zone_2\n",
      "2025-04-12 22:08:07,984 - __main__ - INFO - Predict for new zone zone_2 using models from ../02_Results/models\n",
      "2025-04-12 22:08:07,985 - __main__ - INFO - Loading and fixing data for zone_2\n",
      "2025-04-12 22:08:14,815 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:08:15,648 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:08:16,098 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:08:16,598 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:08:17,108 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:08:17,392 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:08:17,420 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:08:17,440 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:08:17,565 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:08:17,597 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:08:17,617 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:08:17,617 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:08:17,617 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:08:18,464 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:08:18,481 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:09:00,987 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:09:43,280 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:10:25,325 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 22:11:03,794 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 22:11:10,412 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 22:11:17,045 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 22:11:40,428 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 22:11:40,428 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 22:11:47,117 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 22:11:53,881 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 22:14:07,627 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_2\n",
      "2025-04-12 22:14:08,128 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_2.png\n",
      "2025-04-12 22:14:21,577 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 22:14:21,983 - __main__ - INFO - Memory usage: 3479.02 MB\n",
      "2025-04-12 22:14:21,983 - __main__ - INFO - Successfully processed zone: zone_2\n",
      "2025-04-12 22:14:21,983 - __main__ - INFO - Processing zone: zone_3\n",
      "2025-04-12 22:14:21,983 - __main__ - INFO - Predict for new zone zone_3 using models from ../02_Results/models\n",
      "2025-04-12 22:14:21,983 - __main__ - INFO - Loading and fixing data for zone_3\n",
      "2025-04-12 22:14:29,286 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:14:30,167 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:14:30,648 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:14:31,214 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:14:31,732 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:14:32,012 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:14:32,050 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:14:32,075 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:14:32,209 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:14:32,255 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:14:32,276 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:14:32,276 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:14:32,279 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:14:33,105 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:14:33,105 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:15:17,454 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:16:01,124 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:16:44,569 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 22:17:24,662 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 22:17:31,419 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 22:17:40,229 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 22:18:04,781 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 22:18:04,781 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 22:18:11,357 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 22:18:18,153 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 22:20:22,590 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_3\n",
      "2025-04-12 22:20:23,104 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_3.png\n",
      "2025-04-12 22:20:35,905 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 22:20:36,326 - __main__ - INFO - Memory usage: 5106.86 MB\n",
      "2025-04-12 22:20:36,326 - __main__ - INFO - Successfully processed zone: zone_3\n",
      "2025-04-12 22:20:36,326 - __main__ - INFO - Processing zone: zone_4\n",
      "2025-04-12 22:20:36,326 - __main__ - INFO - Predict for new zone zone_4 using models from ../02_Results/models\n",
      "2025-04-12 22:20:36,326 - __main__ - INFO - Loading and fixing data for zone_4\n",
      "2025-04-12 22:20:43,488 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:20:44,304 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:20:44,801 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:20:45,289 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:20:45,771 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:20:46,033 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:20:46,062 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:20:46,102 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:20:46,237 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:20:46,278 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:20:46,310 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:20:46,311 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:20:46,312 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:20:47,104 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:20:47,104 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:21:30,756 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:22:14,503 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:22:57,738 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 22:23:36,952 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 22:23:43,487 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 22:23:50,216 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 22:24:14,251 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 22:24:14,251 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 22:24:20,868 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 22:24:27,601 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 22:26:33,604 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_4\n",
      "2025-04-12 22:26:34,088 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_4.png\n",
      "2025-04-12 22:26:47,434 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 22:26:47,816 - __main__ - INFO - Memory usage: 6737.54 MB\n",
      "2025-04-12 22:26:47,816 - __main__ - INFO - Successfully processed zone: zone_4\n",
      "2025-04-12 22:26:47,816 - __main__ - INFO - Processing zone: zone_5\n",
      "2025-04-12 22:26:47,816 - __main__ - INFO - Predict for new zone zone_5 using models from ../02_Results/models\n",
      "2025-04-12 22:26:47,816 - __main__ - INFO - Loading and fixing data for zone_5\n",
      "2025-04-12 22:26:54,921 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:26:55,766 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:26:56,249 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:26:56,782 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:26:57,266 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:26:57,553 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:26:57,565 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:26:57,604 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:26:57,753 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:26:57,788 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:26:57,810 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:26:57,812 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:26:57,812 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:26:58,651 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:26:58,652 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:27:44,272 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:28:29,214 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:29:14,123 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 22:29:55,202 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 22:30:01,810 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 22:30:08,390 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 22:30:34,404 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 22:30:34,404 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 22:30:40,995 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 22:30:47,891 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 22:32:53,421 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_5\n",
      "2025-04-12 22:32:53,917 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_5.png\n",
      "2025-04-12 22:33:07,369 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 22:33:07,810 - __main__ - INFO - Memory usage: 8363.60 MB\n",
      "2025-04-12 22:33:07,810 - __main__ - INFO - Successfully processed zone: zone_5\n",
      "2025-04-12 22:33:07,810 - __main__ - INFO - Processing zone: zone_6\n",
      "2025-04-12 22:33:07,810 - __main__ - INFO - Predict for new zone zone_6 using models from ../02_Results/models\n",
      "2025-04-12 22:33:07,810 - __main__ - INFO - Loading and fixing data for zone_6\n",
      "2025-04-12 22:33:11,977 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:33:12,804 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:33:13,329 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:33:13,827 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:33:14,294 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:33:14,577 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:33:14,615 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:33:14,645 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:33:14,794 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:33:14,826 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:33:14,844 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:33:14,844 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:33:14,844 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:33:15,694 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:33:15,694 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:34:00,318 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:34:44,671 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:35:28,625 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 22:36:08,234 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 22:36:15,068 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 22:36:21,810 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 22:36:47,562 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 22:36:47,564 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 22:36:54,228 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 22:37:00,979 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 22:39:13,189 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_6\n",
      "2025-04-12 22:39:13,704 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_6.png\n",
      "2025-04-12 22:39:27,486 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 22:39:27,868 - __main__ - INFO - Memory usage: 9991.91 MB\n",
      "2025-04-12 22:39:27,869 - __main__ - INFO - Successfully processed zone: zone_6\n",
      "2025-04-12 22:39:27,869 - __main__ - INFO - Processing zone: zone_7\n",
      "2025-04-12 22:39:27,872 - __main__ - INFO - Predict for new zone zone_7 using models from ../02_Results/models\n",
      "2025-04-12 22:39:27,874 - __main__ - INFO - Loading and fixing data for zone_7\n",
      "2025-04-12 22:39:32,129 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:39:33,025 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:39:33,528 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:39:34,042 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:39:34,568 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:39:34,855 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:39:34,896 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:39:34,926 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:39:35,057 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:39:35,073 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:39:35,118 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:39:35,119 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:39:35,121 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:39:35,955 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:39:35,958 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:40:20,504 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:41:04,118 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:41:47,735 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 22:42:26,831 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 22:42:33,583 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 22:42:40,252 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 22:43:05,337 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 22:43:05,339 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 22:43:12,081 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 22:43:18,985 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 22:45:25,249 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_7\n",
      "2025-04-12 22:45:25,750 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_7.png\n",
      "2025-04-12 22:45:39,350 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 22:45:39,742 - __main__ - INFO - Memory usage: 11623.41 MB\n",
      "2025-04-12 22:45:39,750 - __main__ - INFO - Successfully processed zone: zone_7\n",
      "2025-04-12 22:45:39,750 - __main__ - INFO - Processing zone: zone_8\n",
      "2025-04-12 22:45:39,750 - __main__ - INFO - Predict for new zone zone_8 using models from ../02_Results/models\n",
      "2025-04-12 22:45:39,750 - __main__ - INFO - Loading and fixing data for zone_8\n",
      "2025-04-12 22:45:44,006 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:45:44,855 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:45:45,341 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:45:45,842 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:45:46,333 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:45:46,576 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:45:46,583 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:45:46,630 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:45:46,750 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:45:46,786 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:45:46,814 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:45:46,815 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:45:46,817 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:45:47,640 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:45:47,640 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:46:31,417 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:47:14,072 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:47:57,664 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 22:48:38,215 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 22:48:45,092 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 22:48:51,917 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 22:49:17,144 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 22:49:17,144 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 22:49:23,997 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 22:49:30,999 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 22:51:35,161 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_8\n",
      "2025-04-12 22:51:35,595 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_8.png\n",
      "2025-04-12 22:51:46,667 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 22:51:47,004 - __main__ - INFO - Memory usage: 13250.08 MB\n",
      "2025-04-12 22:51:47,004 - __main__ - INFO - Successfully processed zone: zone_8\n",
      "2025-04-12 22:51:47,004 - __main__ - INFO - Processing zone: zone_9\n",
      "2025-04-12 22:51:47,004 - __main__ - INFO - Predict for new zone zone_9 using models from ../02_Results/models\n",
      "2025-04-12 22:51:47,004 - __main__ - INFO - Loading and fixing data for zone_9\n",
      "2025-04-12 22:51:50,828 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:51:51,623 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:51:52,031 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:51:52,470 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:51:52,874 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:51:53,128 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:51:53,150 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:51:53,170 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:51:53,291 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:51:53,312 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:51:53,335 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:51:53,337 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:51:53,337 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:51:54,044 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:51:54,044 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:52:37,233 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:53:20,137 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:54:02,969 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 22:54:43,721 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 22:54:50,559 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 22:54:57,355 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 22:55:22,179 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 22:55:22,179 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 22:55:28,908 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 22:55:35,685 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 22:57:48,390 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_9\n",
      "2025-04-12 22:57:48,926 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_9.png\n",
      "2025-04-12 22:58:02,330 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 22:58:02,673 - __main__ - INFO - Memory usage: 14880.20 MB\n",
      "2025-04-12 22:58:02,673 - __main__ - INFO - Successfully processed zone: zone_9\n",
      "2025-04-12 22:58:02,673 - __main__ - INFO - Processing zone: zone_10\n",
      "2025-04-12 22:58:02,673 - __main__ - INFO - Predict for new zone zone_10 using models from ../02_Results/models\n",
      "2025-04-12 22:58:02,673 - __main__ - INFO - Loading and fixing data for zone_10\n",
      "2025-04-12 22:58:06,556 - __main__ - INFO - Using features: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:58:07,390 - __main__ - INFO - Loading 9 models...\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\Gradu\\PurOja\\.conda\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-12 22:58:07,906 - __main__ - INFO - Loaded model: rf_model_0.joblib\n",
      "2025-04-12 22:58:08,435 - __main__ - INFO - Loaded model: rf_model_1.joblib\n",
      "2025-04-12 22:58:08,945 - __main__ - INFO - Loaded model: rf_model_2.joblib\n",
      "2025-04-12 22:58:09,206 - __main__ - INFO - Loaded model: rf_model_3.joblib\n",
      "2025-04-12 22:58:09,243 - __main__ - INFO - Loaded model: rf_model_4.joblib\n",
      "2025-04-12 22:58:09,273 - __main__ - INFO - Loaded model: rf_model_5.joblib\n",
      "2025-04-12 22:58:09,397 - __main__ - INFO - Loaded model: rf_model_6.joblib\n",
      "2025-04-12 22:58:09,435 - __main__ - INFO - Loaded model: rf_model_7.joblib\n",
      "2025-04-12 22:58:09,467 - __main__ - INFO - Loaded model: rf_model_8.joblib\n",
      "2025-04-12 22:58:09,467 - __main__ - INFO - Model expects: ['zone_id', 'row_idx', 'col_idx', 'impoundment_amplified', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:58:09,467 - __main__ - INFO - Data has: ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', 'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels']\n",
      "2025-04-12 22:58:10,347 - __main__ - INFO - Making parallel predictions with 9 models\n",
      "2025-04-12 22:58:10,347 - __main__ - INFO - Running predictions with model 1/9\n",
      "2025-04-12 22:58:45,713 - __main__ - INFO - Running predictions with model 2/9\n",
      "2025-04-12 22:59:20,228 - __main__ - INFO - Running predictions with model 3/9\n",
      "2025-04-12 22:59:55,873 - __main__ - INFO - Running predictions with model 4/9\n",
      "2025-04-12 23:00:29,221 - __main__ - INFO - Running predictions with model 5/9\n",
      "2025-04-12 23:00:36,039 - __main__ - INFO - Running predictions with model 6/9\n",
      "2025-04-12 23:00:42,808 - __main__ - INFO - Running predictions with model 7/9\n",
      "2025-04-12 23:01:06,312 - __main__ - ERROR - Error making predictions with model 7: operands could not be broadcast together with shapes (25000000,3) (25000000,2) (25000000,3) \n",
      "2025-04-12 23:01:06,312 - __main__ - INFO - Running predictions with model 8/9\n",
      "2025-04-12 23:01:13,071 - __main__ - INFO - Running predictions with model 9/9\n",
      "2025-04-12 23:01:20,014 - __main__ - INFO - Reshaping predictions for visualization\n",
      "2025-04-12 23:03:28,735 - __main__ - INFO - Plotting predictions with title prefix: Predicted Water Features - zone_10\n",
      "2025-04-12 23:03:29,202 - __main__ - INFO - Saving figure to: ../02_Results/1204/predicted_water_features_-_zone_10.png\n",
      "2025-04-12 23:03:41,968 - __main__ - INFO - Figure saved successfully\n",
      "2025-04-12 23:03:42,318 - __main__ - INFO - Memory usage: 16506.62 MB\n",
      "2025-04-12 23:03:42,319 - __main__ - INFO - Successfully processed zone: zone_10\n",
      "2025-04-12 23:03:42,321 - __main__ - INFO - Exporting 10 zone predictions...\n",
      "2025-04-12 23:03:42,323 - __main__ - INFO - Found 10 prediction entries to process\n",
      "2025-04-12 23:03:42,324 - __main__ - INFO - Processing zone-by-zone predictions\n",
      "2025-04-12 23:03:42,325 - __main__ - WARNING - ⚠️ Missing probability data for zone_1\n",
      "2025-04-12 23:03:42,327 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,328 - __main__ - WARNING - ⚠️ Missing probability data for zone_2\n",
      "2025-04-12 23:03:42,329 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,331 - __main__ - WARNING - ⚠️ Missing probability data for zone_3\n",
      "2025-04-12 23:03:42,332 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,335 - __main__ - WARNING - ⚠️ Missing probability data for zone_4\n",
      "2025-04-12 23:03:42,336 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,338 - __main__ - WARNING - ⚠️ Missing probability data for zone_5\n",
      "2025-04-12 23:03:42,339 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,340 - __main__ - WARNING - ⚠️ Missing probability data for zone_6\n",
      "2025-04-12 23:03:42,340 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,343 - __main__ - WARNING - ⚠️ Missing probability data for zone_7\n",
      "2025-04-12 23:03:42,347 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,347 - __main__ - WARNING - ⚠️ Missing probability data for zone_8\n",
      "2025-04-12 23:03:42,347 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,347 - __main__ - WARNING - ⚠️ Missing probability data for zone_9\n",
      "2025-04-12 23:03:42,351 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,353 - __main__ - WARNING - ⚠️ Missing probability data for zone_10\n",
      "2025-04-12 23:03:42,353 - __main__ - INFO - Available keys: ['raw', 'spatial']\n",
      "2025-04-12 23:03:42,354 - __main__ - INFO - Export complete. Successfully processed 0 zones.\n",
      "2025-04-12 23:03:42,357 - __main__ - INFO - Saving predictions to Zarr store: ../02_Results/1204/predictions.zarr\n",
      "2025-04-12 23:03:42,784 - __main__ - INFO - Saving zone-by-zone predictions to Zarr\n",
      "2025-04-12 23:03:44,624 - __main__ - INFO - ✅ Saved zone_1 to Zarr\n",
      "2025-04-12 23:03:46,377 - __main__ - INFO - ✅ Saved zone_2 to Zarr\n",
      "2025-04-12 23:03:48,215 - __main__ - INFO - ✅ Saved zone_3 to Zarr\n",
      "2025-04-12 23:03:50,172 - __main__ - INFO - ✅ Saved zone_4 to Zarr\n",
      "2025-04-12 23:03:52,051 - __main__ - INFO - ✅ Saved zone_5 to Zarr\n",
      "2025-04-12 23:03:54,140 - __main__ - INFO - ✅ Saved zone_6 to Zarr\n",
      "2025-04-12 23:03:56,059 - __main__ - INFO - ✅ Saved zone_7 to Zarr\n",
      "2025-04-12 23:03:58,059 - __main__ - INFO - ✅ Saved zone_8 to Zarr\n",
      "2025-04-12 23:03:59,926 - __main__ - INFO - ✅ Saved zone_9 to Zarr\n",
      "2025-04-12 23:04:01,778 - __main__ - INFO - ✅ Saved zone_10 to Zarr\n",
      "2025-04-12 23:04:01,778 - __main__ - INFO - Saving predictions to Zarr in ../02_Results/1204/predictions.zarr\n",
      "2025-04-12 23:04:01,784 - __main__ - INFO - Successfully exported 10 zones.\n",
      "2025-04-12 23:04:01,784 - __main__ - INFO - Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zarr\n",
    "import psutil\n",
    "import joblib\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import re\n",
    "import logging\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from rasterio.transform import from_bounds\n",
    "import rasterio\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.transform import from_bounds\n",
    "import os\n",
    "\n",
    "\n",
    "# Finnish coordinate system (ETRS89 / TM35FIN)\n",
    "# Define using WKT string\n",
    "crs = CRS.from_wkt(\"\"\"\n",
    "PROJCS[\"ETRS89 / TM35FIN\",\n",
    "    GEOGCS[\"ETRS89\",\n",
    "        DATUM[\"European_Terrestrial_Reference_System_1989\",\n",
    "            SPHEROID[\"GRS 1980\",6378137,298.257222101]],\n",
    "        PRIMEM[\"Greenwich\",0],\n",
    "        UNIT[\"degree\",0.0174532925199433]],\n",
    "    PROJECTION[\"Transverse_Mercator\"],\n",
    "    PARAMETER[\"latitude_of_origin\",0],\n",
    "    PARAMETER[\"central_meridian\",27],\n",
    "    PARAMETER[\"scale_factor\",0.9996],\n",
    "    PARAMETER[\"false_easting\",500000],\n",
    "    PARAMETER[\"false_northing\",0],\n",
    "    UNIT[\"metre\",1]]\n",
    "\"\"\")\n",
    "\n",
    "# Or using PROJ string\n",
    "crs = CRS.from_string(\"+proj=utm +zone=35 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs +type=crs\")\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration variables\n",
    "#zarr_file = \"zones_data_2.zarr\"  # train and practicing zarr file\n",
    "zarr_file = \"zones_data_11-21.zarr\"\n",
    "total_cpus = min(4, multiprocessing.cpu_count())\n",
    "input_path = \"../02_Results/\"  # model is in here, results also are saved in here\n",
    "output_path = \"../02_Results/1204_2/\"  # New results from using the model\n",
    "models_dir = os.path.join(input_path, \"models\")\n",
    "cache_dir = os.path.join(input_path, \"joblib_cache\")\n",
    "#zones_to_process = [\"zone_1\", \"zone_2\", \"zone_3\", \"zone_4\", \"zone_5\", \n",
    "#                    \"zone_6\", \"zone_7\", \"zone_8\", \"zone_9\", \"zone_10\"]\n",
    "zones_to_process = [\"zone_11\", \"zone_12\", \"zone_13\", \"zone_14\", \"zone_15\", \n",
    "                    \"zone_16\", \"zone_17\", \"zone_18\", \"zone_19\", \"zone_20\", \"zone_21\"]\n",
    "\n",
    "selected_features = ['row_idx', 'col_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor',\n",
    "                    'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f',\n",
    "                    'slope_channels']\n",
    "\n",
    "# Global dictionary to store zone name to ID mapping\n",
    "zone_name_to_id = {f\"zone_{i}\": i for i in range(1, 22)}\n",
    "# Inverse mapping from ID to name\n",
    "zone_id_to_name = {i: f\"zone_{i}\" for i in range(1, 22)}\n",
    "# Global dictionary to store zone boundaries\n",
    "zone_boundaries = {f\"zone_{i}\": {\"upper_left\": (0, 0), \"lower_right\": (5000, 5000)} for i in range(1, 22)}\n",
    "\n",
    "# Define CRS (Coordinate Reference System) for output files\n",
    "crs = CRS.from_epsg(3067)  # Finnish coordinate system, update as needed\n",
    "\n",
    "def save_tif(output_path, data, transform, crs, dtype='float32'):\n",
    "    \"\"\"Save array data to a GeoTIFF file.\"\"\"\n",
    "    height, width = data.shape\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=dtype,\n",
    "        crs=crs,\n",
    "        transform=transform,\n",
    "        compress='lzw'\n",
    "    ) as dst:\n",
    "        dst.write(data, 1)\n",
    "    \n",
    "    logger.info(f\"Saved GeoTIFF: {output_path}\")\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current memory usage.\"\"\"\n",
    "    process = psutil.Process()\n",
    "    logger.info(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def setup_joblib_cache(cache_dir):\n",
    "    \"\"\"Set up cache for joblib.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    memory = joblib.Memory(cache_dir, verbose=0)\n",
    "    return memory\n",
    "\n",
    "def fix_zarr_file_features(zarr_file):\n",
    "    \"\"\"Fix the zarr file by adding missing features to all zones.\"\"\"\n",
    "    logger.info(f\"Fixing zarr file by adding missing features: {zarr_file}\")\n",
    "    \n",
    "    # Open the zarr file\n",
    "    root = zarr.open(zarr_file, mode='r+')  # Use 'r+' for read/write\n",
    "    \n",
    "    # Get all zone names\n",
    "    zone_names = [name for name in root.keys() if name.startswith('zone_')]\n",
    "    \n",
    "    for zone_name in zone_names:\n",
    "        zone = root[zone_name]\n",
    "        \n",
    "        # Add zone_id if missing\n",
    "        if 'zone_id' not in zone:\n",
    "            try:\n",
    "                # Get zone_id from zone_name\n",
    "                if zone_name in zone_name_to_id:\n",
    "                    zone_id = zone_name_to_id[zone_name]\n",
    "                else:\n",
    "                    # Extract zone number from name\n",
    "                    zone_id = int(zone_name.split('_')[1])  # Keep 1-indexed for data consistency\n",
    "                \n",
    "                # Create full array with zone_id\n",
    "                logger.info(f\"Adding zone_id feature to {zone_name}: {zone_id}\")\n",
    "                zone.create_dataset('zone_id', data=np.full(25000000, zone_id, dtype=np.int32))\n",
    "                \n",
    "            except (IndexError, ValueError) as e:\n",
    "                logger.warning(f\"Could not add zone_id to {zone_name}: {e}\")\n",
    "        \n",
    "        # Add spatial indices if missing\n",
    "        if 'row_idx' not in zone or 'col_idx' not in zone:\n",
    "            logger.info(f\"Adding spatial indices to {zone_name}\")\n",
    "            \n",
    "            # Standard 5000x5000 grid\n",
    "            spatial_shape = (5000, 5000)\n",
    "            total_pixels = spatial_shape[0] * spatial_shape[1]\n",
    "            \n",
    "            # Generate row indices\n",
    "            if 'row_idx' not in zone:\n",
    "                row_indices = np.repeat(np.arange(spatial_shape[0]), spatial_shape[1])\n",
    "                zone.create_dataset('row_idx', data=row_indices, dtype=np.int32)\n",
    "            \n",
    "            # Generate column indices\n",
    "            if 'col_idx' not in zone:\n",
    "                col_indices = np.tile(np.arange(spatial_shape[1]), spatial_shape[0])\n",
    "                zone.create_dataset('col_idx', data=col_indices, dtype=np.int32)\n",
    "            \n",
    "            logger.info(f\"Added spatial indices to {zone_name}\")\n",
    "    \n",
    "    logger.info(f\"Finished checking/fixing features in {zarr_file}\")\n",
    "\n",
    "def fix_load_zone_data(zarr_file, zone_name, selected_features):\n",
    "    \"\"\"Load zone data from zarr file and fix if needed.\"\"\"\n",
    "    logger.info(f\"Loading and fixing data for {zone_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Open the zarr file\n",
    "        root = zarr.open(zarr_file, mode='r')\n",
    "        \n",
    "        # Check if the zone exists in the zarr file\n",
    "        if zone_name not in root:\n",
    "            logger.warning(f\"Zone {zone_name} not found in zarr file\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get the zone data\n",
    "        zone_group = root[zone_name]\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        data_dict = {}\n",
    "        for feature in selected_features:\n",
    "            if feature in zone_group:\n",
    "                data_dict[feature] = zone_group[feature][:]\n",
    "            else:\n",
    "                logger.warning(f\"Feature {feature} not found in zone {zone_name}\")\n",
    "        \n",
    "        if not data_dict:\n",
    "            logger.warning(f\"No feature data found for zone {zone_name}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data_dict)\n",
    "        \n",
    "        # Add any missing features with default values if needed\n",
    "        for feature in selected_features:\n",
    "            if feature not in df.columns:\n",
    "                if feature == 'zone_id':\n",
    "                    df[feature] = zone_name_to_id.get(zone_name, -1)\n",
    "                # Add other defaults as needed\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data for zone {zone_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_models(models_dir):\n",
    "    \"\"\"Load all trained models from directory.\"\"\"\n",
    "    model_files = sorted(glob.glob(os.path.join(models_dir, \"rf_model_*.joblib\")))\n",
    "    if not model_files:\n",
    "        logger.error(f\"No model files found in {models_dir}\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Loading {len(model_files)} models...\")\n",
    "    models = []\n",
    "    for model_file in model_files:\n",
    "        try:\n",
    "            model = joblib.load(model_file)\n",
    "            models.append(model)\n",
    "            logger.info(f\"Loaded model: {os.path.basename(model_file)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model {model_file}: {str(e)}\")\n",
    "    \n",
    "    if not models:\n",
    "        logger.error(\"No models could be loaded successfully\")\n",
    "        return None\n",
    "        \n",
    "    return models\n",
    "\n",
    "def make_parallel_predictions(models, X):\n",
    "    \"\"\"Make predictions in parallel using multiple models.\"\"\"\n",
    "    if not models:\n",
    "        logger.error(\"No models provided for prediction\")\n",
    "        return np.zeros((len(X), 3))\n",
    "        \n",
    "    logger.info(f\"Making parallel predictions with {len(models)} models\")\n",
    "    \n",
    "    # Get the number of classes from the first model\n",
    "    n_classes = len(models[0].classes_) if hasattr(models[0], 'classes_') else 3\n",
    "    \n",
    "    # Initialize array to store probabilities\n",
    "    all_probabilities = np.zeros((len(X), n_classes))\n",
    "    \n",
    "    # Make predictions with each model and average them\n",
    "    for i, model in enumerate(models):\n",
    "        try:\n",
    "            logger.info(f\"Running predictions with model {i+1}/{len(models)}\")\n",
    "            y_prob = model.predict_proba(X)\n",
    "            all_probabilities += y_prob\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error making predictions with model {i+1}: {str(e)}\")\n",
    "    \n",
    "    # Average the predictions across all models\n",
    "    if models:\n",
    "        all_probabilities /= len(models)\n",
    "    \n",
    "    return all_probabilities\n",
    "\n",
    "def reshape_predictions_for_visualization(y_prob, spatial_indices, spatial_shape):\n",
    "    \"\"\"Reshape predictions for visualization.\"\"\"\n",
    "    logger.info(\"Reshaping predictions for visualization\")\n",
    "    \n",
    "    # Extract class probabilities\n",
    "    if y_prob.shape[1] >= 3:\n",
    "        prob_background = y_prob[:, 0]\n",
    "        prob_streams = y_prob[:, 1]\n",
    "        prob_ditches = y_prob[:, 2]\n",
    "    else:\n",
    "        # Handle case where we have fewer classes\n",
    "        prob_background = 1.0 - np.sum(y_prob, axis=1)\n",
    "        prob_streams = y_prob[:, 0] if y_prob.shape[1] > 0 else np.zeros(y_prob.shape[0])\n",
    "        prob_ditches = y_prob[:, 1] if y_prob.shape[1] > 1 else np.zeros(y_prob.shape[0])\n",
    "    \n",
    "    # Initialize spatial arrays\n",
    "    prob_streams_spatial = np.zeros(spatial_shape)\n",
    "    prob_ditches_spatial = np.zeros(spatial_shape)\n",
    "    prob_combined_spatial = np.zeros(spatial_shape)\n",
    "    \n",
    "    # Apply threshold for binary predictions (0.5 by default)\n",
    "    threshold = 0.5\n",
    "    pred_streams_spatial = np.zeros(spatial_shape, dtype=np.int8)\n",
    "    pred_ditches_spatial = np.zeros(spatial_shape, dtype=np.int8)\n",
    "    pred_combined_spatial = np.zeros(spatial_shape, dtype=np.int8)\n",
    "    \n",
    "    # Map predictions to spatial grid using indices\n",
    "    for i in range(len(spatial_indices)):\n",
    "        row, col = spatial_indices[i]\n",
    "        if 0 <= row < spatial_shape[0] and 0 <= col < spatial_shape[1]:\n",
    "            prob_streams_spatial[row, col] = prob_streams[i]\n",
    "            prob_ditches_spatial[row, col] = prob_ditches[i]\n",
    "            prob_combined_spatial[row, col] = max(prob_streams[i], prob_ditches[i])\n",
    "            \n",
    "            # Binary predictions\n",
    "            pred_streams_spatial[row, col] = 1 if prob_streams[i] > threshold else 0\n",
    "            pred_ditches_spatial[row, col] = 1 if prob_ditches[i] > threshold else 0\n",
    "            pred_combined_spatial[row, col] = 1 if prob_streams[i] > threshold else (2 if prob_ditches[i] > threshold else 0)\n",
    "    \n",
    "    # Create predictions structure\n",
    "    predictions = {\n",
    "        'raw': {\n",
    "            'probabilities': y_prob\n",
    "        },\n",
    "        'spatial': {\n",
    "            'prob_streams': prob_streams_spatial,\n",
    "            'prob_ditches': prob_ditches_spatial,\n",
    "            'prob_combined': prob_combined_spatial,\n",
    "            'pred_streams': pred_streams_spatial,\n",
    "            'pred_ditches': pred_ditches_spatial,\n",
    "            'pred_combined': pred_combined_spatial\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_ground_truth_visualization(zone_data, spatial_shape):\n",
    "    \"\"\"Create ground truth visualization from zone data.\"\"\"\n",
    "    logger.info(\"Creating ground truth visualization\")\n",
    "    \n",
    "    # Initialize ground truth arrays\n",
    "    ground_truth_streams = np.zeros(spatial_shape, dtype=np.int8)\n",
    "    ground_truth_ditches = np.zeros(spatial_shape, dtype=np.int8)\n",
    "    \n",
    "    # Check if we have ground truth labels\n",
    "    if 'label_3m' in zone_data.columns:\n",
    "        # Get spatial indices\n",
    "        row_indices = zone_data['row_idx'].values\n",
    "        col_indices = zone_data['col_idx'].values\n",
    "        labels = zone_data['label_3m'].values\n",
    "        \n",
    "        # Map labels to spatial grid\n",
    "        for i in range(len(labels)):\n",
    "            row, col = row_indices[i], col_indices[i]\n",
    "            if 0 <= row < spatial_shape[0] and 0 <= col < spatial_shape[1]:\n",
    "                if labels[i] == 1:  # Stream\n",
    "                    ground_truth_streams[row, col] = 1\n",
    "                elif labels[i] == 2:  # Ditch\n",
    "                    ground_truth_ditches[row, col] = 1\n",
    "    else:\n",
    "        logger.warning(\"No ground truth labels found in zone data\")\n",
    "    \n",
    "    ground_truth = {\n",
    "        'streams': ground_truth_streams,\n",
    "        'ditches': ground_truth_ditches\n",
    "    }\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "def plot_predictions(predictions, title_prefix=\"Predicted Water Features\", output_path=\".\"):\n",
    "    \"\"\"Plot predictions.\"\"\"\n",
    "    logger.info(f\"Plotting predictions with title prefix: {title_prefix}\")\n",
    "    \n",
    "    if not isinstance(predictions, dict) or 'spatial' not in predictions:\n",
    "        logger.error(\"Invalid predictions structure\")\n",
    "        return\n",
    "    \n",
    "    spatial_data = predictions['spatial']\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Plot probability maps\n",
    "    if 'prob_streams' in spatial_data:\n",
    "        stream_im = axes[0].imshow(spatial_data['prob_streams'], cmap=plt.cm.Blues, vmin=0, vmax=1)\n",
    "        axes[0].set_title('Stream Probability')\n",
    "        plt.colorbar(stream_im, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    if 'prob_ditches' in spatial_data:\n",
    "        ditch_im = axes[1].imshow(spatial_data['prob_ditches'], cmap=plt.cm.Greens, vmin=0, vmax=1)\n",
    "        axes[1].set_title('Ditch Probability')\n",
    "        plt.colorbar(ditch_im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    if 'pred_combined' in spatial_data:\n",
    "        # Combined class visualization\n",
    "        cmap_combined = plt.cm.colors.ListedColormap(['white', 'blue', 'green'])\n",
    "        bounds_combined = [-0.5, 0.5, 1.5, 2.5]\n",
    "        norm_combined = plt.cm.colors.BoundaryNorm(bounds_combined, cmap_combined.N)\n",
    "        \n",
    "        combined_im = axes[2].imshow(spatial_data['pred_combined'], cmap=cmap_combined, norm=norm_combined)\n",
    "        axes[2].set_title('Predicted Classes')\n",
    "        cbar_combined = plt.colorbar(combined_im, ax=axes[2], ticks=[0, 1, 2], fraction=0.046, pad=0.04)\n",
    "        cbar_combined.set_ticklabels(['None', 'Stream', 'Ditch'])\n",
    "    \n",
    "    # Remove ticks from all subplots\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.suptitle(f\"{title_prefix}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Create a safe filename for output\n",
    "    safe_filename = re.sub(r'[^\\w\\-_\\.]', '_', title_prefix.lower().replace(' ', '_'))\n",
    "    output_file = os.path.join(output_path, f\"{safe_filename}.png\")\n",
    "    \n",
    "    # Save the figure\n",
    "    try:\n",
    "        logger.info(f\"Saving figure to: {output_file}\")\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"Figure saved successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving figure: {e}\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "def compare_predictions_with_ground_truth(predictions, ground_truth, title_prefix=\"Comparison with Ground Truth\", output_path=\".\"):\n",
    "    \"\"\"Compare predictions with ground truth.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Define simple colormaps\n",
    "    stream_cmap = plt.cm.Blues\n",
    "    ditch_cmap = plt.cm.Greens\n",
    "    combined_cmap = plt.cm.colors.ListedColormap(['white', 'blue', 'green'])\n",
    "    \n",
    "    # Check if the ground_truth has the expected structure\n",
    "    if isinstance(ground_truth, dict) and 'streams' in ground_truth and 'ditches' in ground_truth:\n",
    "        # Ground truth visualizations\n",
    "        axes[0, 0].imshow(ground_truth['streams'], cmap=stream_cmap)\n",
    "        axes[0, 0].set_title('Ground Truth - Streams')\n",
    "        \n",
    "        axes[0, 1].imshow(ground_truth['ditches'], cmap=ditch_cmap)\n",
    "        axes[0, 1].set_title('Ground Truth - Ditches')\n",
    "        \n",
    "        # Create combined ground truth visualization\n",
    "        combined_gt = np.zeros_like(ground_truth['streams'], dtype=np.int8)\n",
    "        combined_gt[ground_truth['streams'] == 1] = 1\n",
    "        combined_gt[ground_truth['ditches'] == 1] = 2\n",
    "        \n",
    "        cmap_gt = plt.cm.colors.ListedColormap(['white', 'blue', 'green'])\n",
    "        bounds_gt = [-0.5, 0.5, 1.5, 2.5]\n",
    "        norm_gt = plt.cm.colors.BoundaryNorm(bounds_gt, cmap_gt.N)\n",
    "        \n",
    "        im_gt = axes[0, 2].imshow(combined_gt, cmap=cmap_gt, norm=norm_gt)\n",
    "        axes[0, 2].set_title('Ground Truth - Combined')\n",
    "        cbar_gt = plt.colorbar(im_gt, ax=axes[0, 2], ticks=[0, 1, 2], fraction=0.046, pad=0.04)\n",
    "        cbar_gt.set_ticklabels(['None', 'Stream', 'Ditch'])\n",
    "    else:\n",
    "        logger.warning(\"Ground truth data is not in the expected format. Expected 'streams' and 'ditches' keys.\")\n",
    "        for i in range(3):\n",
    "            axes[0, i].text(0.5, 0.5, 'Ground Truth Not Available', \n",
    "                          horizontalalignment='center', verticalalignment='center',\n",
    "                          transform=axes[0, i].transAxes)\n",
    "            axes[0, i].set_title(f\"Ground Truth - {'Streams' if i==0 else 'Ditches' if i==1 else 'Combined'}\")\n",
    "    \n",
    "    # Check if predictions has the expected structure\n",
    "    if isinstance(predictions, dict) and 'spatial' in predictions:\n",
    "        spatial_data = predictions['spatial']\n",
    "        \n",
    "        # Predicted streams visualization\n",
    "        if 'pred_streams' in spatial_data:\n",
    "            axes[1, 0].imshow(spatial_data['pred_streams'], cmap=stream_cmap)\n",
    "            axes[1, 0].set_title('Predicted - Streams')\n",
    "        else:\n",
    "            logger.warning(\"Missing 'pred_streams' in predictions\")\n",
    "            axes[1, 0].text(0.5, 0.5, 'Predictions Not Available', \n",
    "                          horizontalalignment='center', verticalalignment='center',\n",
    "                          transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('Predicted - Streams')\n",
    "        \n",
    "        # Predicted ditches visualization\n",
    "        if 'pred_ditches' in spatial_data:\n",
    "            axes[1, 1].imshow(spatial_data['pred_ditches'], cmap=ditch_cmap)\n",
    "            axes[1, 1].set_title('Predicted - Ditches')\n",
    "        else:\n",
    "            logger.warning(\"Missing 'pred_ditches' in predictions\")\n",
    "            axes[1, 1].text(0.5, 0.5, 'Predictions Not Available', \n",
    "                          horizontalalignment='center', verticalalignment='center',\n",
    "                          transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title('Predicted - Ditches')\n",
    "        \n",
    "        # Combined prediction visualization\n",
    "        if 'pred_combined' in spatial_data:\n",
    "            pred_combined = spatial_data['pred_combined']\n",
    "        elif 'pred_streams' in spatial_data and 'pred_ditches' in spatial_data:\n",
    "            # Create combined prediction if not already present\n",
    "            pred_combined = np.zeros_like(spatial_data['pred_streams'], dtype=np.int8)\n",
    "            pred_combined[spatial_data['pred_streams'] == 1] = 1\n",
    "            pred_combined[spatial_data['pred_ditches'] == 1] = 2\n",
    "        else:\n",
    "            pred_combined = None\n",
    "            \n",
    "        if pred_combined is not None:\n",
    "            cmap_pred = plt.cm.colors.ListedColormap(['white', 'blue', 'green'])\n",
    "            bounds_pred = [-0.5, 0.5, 1.5, 2.5]\n",
    "            norm_pred = plt.cm.colors.BoundaryNorm(bounds_pred, cmap_pred.N)\n",
    "            \n",
    "            im_pred = axes[1, 2].imshow(pred_combined, cmap=cmap_pred, norm=norm_pred)\n",
    "            axes[1, 2].set_title('Predicted - Combined')\n",
    "            cbar_pred = plt.colorbar(im_pred, ax=axes[1, 2], ticks=[0, 1, 2], fraction=0.046, pad=0.04)\n",
    "            cbar_pred.set_ticklabels(['None', 'Stream', 'Ditch'])\n",
    "        else:\n",
    "            logger.warning(\"Could not create combined prediction visualization\")\n",
    "            axes[1, 2].text(0.5, 0.5, 'Predictions Not Available', \n",
    "                          horizontalalignment='center', verticalalignment='center',\n",
    "                          transform=axes[1, 2].transAxes)\n",
    "            axes[1, 2].set_title('Predicted - Combined')\n",
    "    else:\n",
    "        logger.warning(\"Predictions data is not in the expected format. Expected 'spatial' key with prediction data.\")\n",
    "        for i in range(3):\n",
    "            axes[1, i].text(0.5, 0.5, 'Predictions Not Available', \n",
    "                          horizontalalignment='center', verticalalignment='center',\n",
    "                          transform=axes[1, i].transAxes)\n",
    "            axes[1, i].set_title(f\"Predicted - {'Streams' if i==0 else 'Ditches' if i==1 else 'Combined'}\")\n",
    "    \n",
    "    # Remove ticks from all subplots\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.suptitle(f\"{title_prefix}\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Create a safe filename for output\n",
    "    safe_filename = re.sub(r'[^\\w\\-_\\.]', '_', title_prefix.lower().replace(' ', '_'))\n",
    "    output_file = os.path.join(output_path, f\"{safe_filename}.png\")\n",
    "    \n",
    "    # Save the figure\n",
    "    try:\n",
    "        logger.info(f\"Saving figure to: {output_file}\")\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"Figure saved successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving figure: {e}\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "def predict_for_new_zone(zarr_file, zone_name, models_dir, selected_features, output_path):\n",
    "    \"\"\"Predict water features for a new zone.\"\"\"\n",
    "    logger.info(f\"Predict for new zone {zone_name} using models from {models_dir}\")\n",
    "   \n",
    "    # Load and fix zone data\n",
    "    zone_data = fix_load_zone_data(zarr_file, zone_name, selected_features)\n",
    "    if zone_data is None or len(zone_data) == 0:\n",
    "        logger.warning(f\"No data available for zone {zone_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Check and add missing zone_id feature\n",
    "    if 'zone_id' not in zone_data.columns:\n",
    "        logger.info(f\"Adding missing zone_id feature for {zone_name}\")\n",
    "        zone_data['zone_id'] = zone_name_to_id.get(zone_name, -1)  # Use -1 as fallback if name not found\n",
    "\n",
    "    # Check and add missing spatial index features\n",
    "    if 'row_idx' not in zone_data.columns:\n",
    "        logger.info(f\"Adding missing row_idx feature for {zone_name}\")\n",
    "        # Create row indices based on the data shape (assuming data is ordered)\n",
    "        num_rows = 5000  # Based on spatial_shape=(5000, 5000)\n",
    "        zone_data['row_idx'] = np.repeat(np.arange(num_rows), num_rows)\n",
    "    \n",
    "    if 'col_idx' not in zone_data.columns:\n",
    "        logger.info(f\"Adding missing col_idx feature for {zone_name}\")\n",
    "        # Create column indices based on the data shape (assuming data is ordered)\n",
    "        num_cols = 5000  # Based on spatial_shape=(5000, 5000)\n",
    "        num_rows = 5000\n",
    "        zone_data['col_idx'] = np.tile(np.arange(num_cols), num_rows)\n",
    "    \n",
    "    # Extract feature columns for model input\n",
    "    feature_cols = [col for col in selected_features if col in zone_data.columns]\n",
    "    logger.info(f\"Using features: {feature_cols}\")\n",
    "    X = zone_data[feature_cols]\n",
    "    \n",
    "    # Make sure spatial indices exist\n",
    "    if 'row_idx' in zone_data.columns and 'col_idx' in zone_data.columns:\n",
    "        spatial_indices = zone_data[['row_idx', 'col_idx']].values\n",
    "    else:\n",
    "        logger.warning(\"Missing spatial indices after fixing data - generating default\")\n",
    "        spatial_shape = (5000, 5000)\n",
    "        row_indices = np.repeat(np.arange(spatial_shape[0]), spatial_shape[1])\n",
    "        col_indices = np.tile(np.arange(spatial_shape[1]), spatial_shape[0])\n",
    "        spatial_indices = np.column_stack((row_indices, col_indices))\n",
    "    \n",
    "    # Load trained models\n",
    "    models = load_models(models_dir)\n",
    "    if not models:\n",
    "        logger.error(f\"No models could be loaded from {models_dir}\")\n",
    "        return None\n",
    "    \n",
    "    # Make sure we have the right features for the model\n",
    "    feature_names = models[0].feature_names_in_.tolist()\n",
    "    logger.info(f\"Model expects: {feature_names}\")\n",
    "    logger.info(f\"Data has: {X.columns.tolist()}\")\n",
    "    \n",
    "    # Make sure X has all the features the model expects\n",
    "    missing_features = set(feature_names) - set(X.columns)\n",
    "    if missing_features:\n",
    "        logger.warning(f\"Missing features in data: {missing_features}\")\n",
    "        # Add missing features with zeros\n",
    "        for feature in missing_features:\n",
    "            X[feature] = 0\n",
    "    \n",
    "    # Select only the features the model expects\n",
    "    X = X[feature_names]\n",
    "    \n",
    "    # Make predictions\n",
    "    final_probabilities = make_parallel_predictions(models, X)\n",
    "    \n",
    "    if final_probabilities.shape[1] >= 3:\n",
    "        y_prob_final = final_probabilities[:, 1:3]  # Extract streams and ditches probabilities\n",
    "    else:\n",
    "        logger.warning(f\"Unexpected probability shape: {final_probabilities.shape}. Using all columns.\")\n",
    "        y_prob_final = final_probabilities\n",
    "    \n",
    "    # Reshape predictions to spatial format\n",
    "    spatial_shape = (5000, 5000) \n",
    "    predictions = reshape_predictions_for_visualization(final_probabilities, spatial_indices, spatial_shape)\n",
    "    \n",
    "    # Visualize predictions\n",
    "    if 'label_3m' in zone_data.columns:\n",
    "        ground_truth = create_ground_truth_visualization(zone_data, spatial_shape)\n",
    "        \n",
    "        plot_predictions(predictions, \n",
    "                        title_prefix=f\"Predicted Water Features - {zone_name}\",\n",
    "                        output_path=output_path)\n",
    "\n",
    "        compare_predictions_with_ground_truth(predictions, ground_truth, \n",
    "                                            title_prefix=f\"Comparison with Ground Truth - {zone_name}\",\n",
    "                                            output_path=output_path)\n",
    "    else:\n",
    "        plot_predictions(predictions, \n",
    "                        title_prefix=f\"Predicted Water Features - {zone_name}\",\n",
    "                        output_path=output_path)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def transform_predictions(predictions):\n",
    "    \"\"\"Transform predictions to the format needed for export.\"\"\"\n",
    "    transformed = {}\n",
    "    \n",
    "    for zone_name, zone_data in predictions.items():\n",
    "        if 'spatial' in zone_data:\n",
    "            spatial_data = zone_data['spatial']\n",
    "\n",
    "            transformed[zone_name] = {\n",
    "                'prob_streams': spatial_data.get('prob_streams'),\n",
    "                'prob_ditches': spatial_data.get('prob_ditches'),\n",
    "                'pred_combined': spatial_data.get('pred_combined')\n",
    "            }\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "def calculate_spatial_metrics(predictions, ground_truth):\n",
    "    \"\"\"Calculate spatial metrics for predictions vs ground truth.\"\"\"\n",
    "    pred_streams = predictions['spatial']['pred_streams']\n",
    "    pred_ditches = predictions['spatial']['pred_ditches']\n",
    "    \n",
    "    gt_streams = ground_truth['streams']\n",
    "    gt_ditches = ground_truth['ditches']\n",
    "    \n",
    "    stream_metrics = {\n",
    "        'precision': precision_score(gt_streams.flatten(), pred_streams.flatten(), zero_division=0),\n",
    "        'recall': recall_score(gt_streams.flatten(), pred_streams.flatten(), zero_division=0),\n",
    "        'f1': f1_score(gt_streams.flatten(), pred_streams.flatten(), zero_division=0), \n",
    "    }\n",
    "\n",
    "    ditch_metrics = {\n",
    "        'precision': precision_score(gt_ditches.flatten(), pred_ditches.flatten(), zero_division=0),\n",
    "        'recall': recall_score(gt_ditches.flatten(), pred_ditches.flatten(), zero_division=0),\n",
    "        'f1': f1_score(gt_ditches.flatten(), pred_ditches.flatten(), zero_division=0)\n",
    "    }\n",
    "    \n",
    "    pred_combined = (pred_streams + pred_ditches > 0).astype(np.int8)\n",
    "    gt_combined = (gt_streams + gt_ditches > 0).astype(np.int8)\n",
    "    \n",
    "    combined_metrics = {\n",
    "        'precision': precision_score(gt_combined.flatten(), pred_combined.flatten(), zero_division=0),\n",
    "        'recall': recall_score(gt_combined.flatten(), pred_combined.flatten(), zero_division=0),\n",
    "        'f1': f1_score(gt_combined.flatten(), pred_combined.flatten(), zero_division=0)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'streams': stream_metrics,\n",
    "        'ditches': ditch_metrics,\n",
    "        'combined': combined_metrics\n",
    "    }\n",
    "\n",
    "def setup_zarr_output(output_path, zone_names=None):\n",
    "    \"\"\"Set up zarr output structure.\"\"\"\n",
    "    compression_params = dict(\n",
    "        compressor=zarr.Blosc(cname='zstd', clevel=3, shuffle=zarr.Blosc.SHUFFLE)\n",
    "    )\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    root = zarr.open(output_path, mode=\"w\")\n",
    "\n",
    "    root.attrs['description'] = 'Water feature predictions from Random Forest models'\n",
    "    root.attrs['creation_date'] = datetime.now().isoformat()\n",
    "    root.attrs['model_type'] = 'Random Forest with SMOTE'\n",
    "\n",
    "    predictions_group = root.create_group(\"predictions\")\n",
    "    predictions_group.attrs['class_labels'] = ['background', 'stream', 'ditch']\n",
    "\n",
    "    raw_group = predictions_group.create_group(\"raw\")\n",
    "    spatial_group = predictions_group.create_group(\"spatial\")\n",
    "\n",
    "    if zone_names is None:\n",
    "        zone_names = []\n",
    "\n",
    "    root.attrs['zone_names'] = zone_names\n",
    "\n",
    "    for zone in zone_names:\n",
    "        spatial_group.create_group(zone)\n",
    "    \n",
    "    return root, predictions_group, raw_group, spatial_group, compression_params\n",
    "\n",
    "def process_and_visualize_zone(zarr_file, zone_name, selected_features, output_path, \n",
    "                               trained_models, X_test, y_test, test_indices, \n",
    "                               zarr_output, compression_params):\n",
    "    \"\"\"Process and visualize a zone.\"\"\"\n",
    "    # Placeholder implementation\n",
    "    logger.info(f\"Processing and visualizing zone {zone_name}\")\n",
    "    return None\n",
    "\n",
    "def process_selected_zones(zarr_file, selected_features, output_path, trained_models, X_test, y_test, test_indices, target_zones=None):\n",
    "    \"\"\"Process selected zones.\"\"\"\n",
    "    if target_zones is None:\n",
    "        target_zones = [\"zone_1\", \"zone_2\", \"zone_3\", \"zone_4\", \"zone_5\", \n",
    "                        \"zone_6\", \"zone_7\", \"zone_8\", \"zone_9\", \"zone_10\"]\n",
    "    \n",
    "    output_zarr_file = os.path.join(output_path, \"prediction_results_smote.zarr\")\n",
    "    \n",
    "    with zarr.open_group(output_zarr_file, mode=\"w\") as root:\n",
    "        predictions_group = root.create_group(\"predictions\")\n",
    "        predictions_group.attrs['class_labels'] = ['background', 'stream', 'ditch']\n",
    "\n",
    "        raw_group = predictions_group.create_group(\"raw\")\n",
    "        spatial_group = predictions_group.create_group(\"spatial\")\n",
    "        root.attrs['description'] = 'Water feature predictions from Random Forest models'\n",
    "        root.attrs['creation_date'] = datetime.now().isoformat()\n",
    "        root.attrs['model_type'] = 'Random Forest with SMOTE'\n",
    "        root.attrs['zone_names'] = target_zones\n",
    "\n",
    "        compression_params = {\n",
    "            'compressor': zarr.Blosc(cname='zstd', clevel=3, shuffle=zarr.Blosc.SHUFFLE)\n",
    "        }\n",
    "        \n",
    "        raw_group.create_dataset(\"y_test\", data=y_test.to_numpy().astype(np.int8), \n",
    "                                chunks=True, **compression_params)\n",
    "\n",
    "        if hasattr(X_test, 'to_numpy'):\n",
    "            raw_group.create_dataset(\"X_test\", data=X_test.to_numpy(), \n",
    "                                    chunks=True, **compression_params)\n",
    "\n",
    "        if test_indices is not None:\n",
    "            raw_group.create_dataset(\"test_indices\", data=test_indices, \n",
    "                                    chunks=True, **compression_params)\n",
    "\n",
    "        all_zone_details = {}\n",
    "        all_zone_bounds = {}  # Ensure this is defined\n",
    "\n",
    "        for zone_name in target_zones:\n",
    "            logger.info(f\"Processing {zone_name}\")\n",
    "\n",
    "            if zone_name in spatial_group:\n",
    "                del spatial_group[zone_name]\n",
    "            \n",
    "            zone_group = spatial_group.create_group(zone_name)\n",
    "            \n",
    "            try:\n",
    "                zone_number = int(zone_name.split('_')[1])\n",
    "            except (IndexError, ValueError):\n",
    "                logger.error(f\"Invalid zone name: {zone_name}\")\n",
    "                zone_number = None\n",
    "            \n",
    "            zone_metadata = {\n",
    "                'name': zone_name,\n",
    "                'number': zone_number,\n",
    "                'processed_date': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            zone_group.attrs.update(zone_metadata)\n",
    "            \n",
    "            all_zone_details[zone_name] = zone_metadata\n",
    "\n",
    "            predictions = process_and_visualize_zone(\n",
    "                zarr_file=zarr_file,\n",
    "                zone_name=zone_name,\n",
    "                selected_features=selected_features,\n",
    "                output_path=output_path,\n",
    "                trained_models=trained_models,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                test_indices=test_indices,\n",
    "                zarr_output=(root, predictions_group, raw_group, spatial_group),\n",
    "                compression_params=compression_params\n",
    "            )\n",
    "\n",
    "            if predictions and 'spatial' in predictions:\n",
    "                for pred_type in ['pred_streams', 'pred_ditches', 'pred_combined', \n",
    "                                'prob_streams', 'prob_ditches', 'prob_combined']:\n",
    "                    if pred_type in predictions['spatial']:\n",
    "                        zone_group.create_dataset(\n",
    "                            pred_type, \n",
    "                            data=predictions['spatial'][pred_type], \n",
    "                            dtype=np.float32 if 'prob' in pred_type else np.int8,\n",
    "                            chunks=True, \n",
    "                            **compression_params\n",
    "                        )\n",
    "\n",
    "                zone_group.attrs['processed_date'] = datetime.now().isoformat()\n",
    "                zone_group.attrs['zone_name'] = zone_name\n",
    "                \n",
    "                if 'zone_boundaries' in globals():\n",
    "                    zone_boundary = zone_boundaries.get(zone_name, {})\n",
    "                    if zone_boundary:\n",
    "                        zone_group.attrs['upper_left'] = zone_boundary.get('upper_left')\n",
    "                        zone_group.attrs['lower_right'] = zone_boundary.get('lower_right')\n",
    "                        all_zone_bounds[zone_name] = zone_boundary\n",
    "            \n",
    "            logger.info(f\"Saved data for {zone_name}\")\n",
    "        \n",
    "        predictions_group.attrs['processed_zones'] = target_zones\n",
    "        \n",
    "        if all_zone_bounds:\n",
    "            predictions_group.attrs['zone_boundaries'] = all_zone_bounds\n",
    "\n",
    "        root.attrs['zone_details'] = all_zone_details\n",
    "        logger.info(f\"Saved prediction results to Zarr file: {output_zarr_file}\")\n",
    "    \n",
    "    return output_zarr_file\n",
    "\n",
    "def export_predictions(predictions, zone_boundaries, zone_name_to_id, output_path):\n",
    "    \"\"\"Export predictions to GeoTIFF files.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary with zone names as keys and prediction data as values,\n",
    "                    or dictionary with a 'spatial' key containing prediction data.\n",
    "        zone_boundaries: Dictionary mapping zone names to boundary coordinates.\n",
    "        zone_name_to_id: Dictionary mapping zone names to numeric IDs.\n",
    "        output_path: Directory where GeoTIFF files will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    if not predictions:\n",
    "        logger.error(\"No predictions data to export!\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Found {len(predictions)} prediction entries to process\")\n",
    "    \n",
    "    # Check if we have a 'spatial' key in the predictions\n",
    "    if 'spatial' in predictions:\n",
    "        logger.info(\"Found 'spatial' key in predictions - using spatial prediction data\")\n",
    "        \n",
    "        # Check if spatial contains the expected probability maps\n",
    "        spatial_data = predictions['spatial']\n",
    "        required_keys = ['prob_streams', 'prob_ditches']\n",
    "        if not all(key in spatial_data for key in required_keys):\n",
    "            logger.error(f\"Spatial data missing required fields. Available keys: {list(spatial_data.keys())}\")\n",
    "            return\n",
    "        \n",
    "        # If the zone ID is included in the predictions, use it; otherwise use zone_1 as default\n",
    "        zone_id = spatial_data.get('zone_id', 0)  # Default to zone_1 (index 0)\n",
    "        zone_name = zone_id_to_name.get(zone_id)\n",
    "        \n",
    "        if zone_name is None:\n",
    "            logger.warning(f\"Invalid zone_id {zone_id} in spatial data, defaulting to zone_1\")\n",
    "            zone_name = \"zone_1\"\n",
    "            \n",
    "        logger.info(f\"Processing spatial predictions for {zone_name}\")\n",
    "        \n",
    "        # Get zone boundaries\n",
    "        if zone_name not in zone_boundaries:\n",
    "            logger.error(f\"No boundaries found for {zone_name}\")\n",
    "            return\n",
    "            \n",
    "        ul_x, ul_y = zone_boundaries[zone_name][\"upper_left\"]\n",
    "        lr_x, lr_y = zone_boundaries[zone_name][\"lower_right\"]\n",
    "        \n",
    "        # Get array dimensions\n",
    "        try:\n",
    "            grid_height, grid_width = spatial_data[\"prob_streams\"].shape\n",
    "            logger.info(f\"Array shape: {grid_height}x{grid_width}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting array shape: {str(e)}\")\n",
    "            return\n",
    "            \n",
    "        transform = from_bounds(ul_x, lr_y, lr_x, ul_y, grid_width, grid_height)\n",
    "        \n",
    "        # Save probability maps as float32\n",
    "        save_tif(os.path.join(output_path, f\"{zone_name}_stream_prob.tif\"),\n",
    "                 spatial_data[\"prob_streams\"].astype(np.float32), \n",
    "                 transform, crs, dtype='float32')\n",
    "        \n",
    "        save_tif(os.path.join(output_path, f\"{zone_name}_ditch_prob.tif\"),\n",
    "                 spatial_data[\"prob_ditches\"].astype(np.float32), \n",
    "                 transform, crs, dtype='float32')\n",
    "        \n",
    "        # Handle classification\n",
    "        if \"pred_combined\" in spatial_data:\n",
    "            classification = spatial_data[\"pred_combined\"].astype(np.uint8)\n",
    "        else:\n",
    "            # Compute classification from probabilities\n",
    "            probs = np.zeros((3, grid_height, grid_width), dtype=np.float32)\n",
    "            probs[0] = 1.0 - (spatial_data[\"prob_streams\"] + spatial_data[\"prob_ditches\"])\n",
    "            probs[1] = spatial_data[\"prob_streams\"]\n",
    "            probs[2] = spatial_data[\"prob_ditches\"]\n",
    "            classification = np.argmax(probs, axis=0).astype(np.uint8)\n",
    "            \n",
    "        save_tif(os.path.join(output_path, f\"{zone_name}_classification.tif\"),\n",
    "                 classification, transform, crs, dtype='uint8')\n",
    "                 \n",
    "        logger.info(f\"✅ Exported spatial predictions for {zone_name}\")\n",
    "        return\n",
    "    \n",
    "    # If no 'spatial' key, process each zone separately\n",
    "    logger.info(\"Processing zone-by-zone predictions\")\n",
    "    success_count = 0\n",
    "    \n",
    "    for zone_name, zone_data in predictions.items():\n",
    "        # Skip non-zone entries\n",
    "        if zone_name == 'spatial':\n",
    "            continue\n",
    "            \n",
    "        # Check if this is a valid zone\n",
    "        if zone_name not in zone_boundaries:\n",
    "            logger.warning(f\"⚠️ No boundaries for zone: {zone_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Check if this zone has prediction data\n",
    "        if not isinstance(zone_data, dict):\n",
    "            logger.warning(f\"⚠️ Zone data for {zone_name} is not a dictionary\")\n",
    "            continue\n",
    "            \n",
    "        if \"prob_streams\" not in zone_data or \"prob_ditches\" not in zone_data:\n",
    "            logger.warning(f\"⚠️ Missing probability data for {zone_name}\")\n",
    "            if isinstance(zone_data, dict):\n",
    "                logger.info(f\"Available keys: {list(zone_data.keys())}\")\n",
    "            continue\n",
    "\n",
    "        # Get zone boundaries\n",
    "        ul_x, ul_y = zone_boundaries[zone_name][\"upper_left\"]\n",
    "        lr_x, lr_y = zone_boundaries[zone_name][\"lower_right\"]\n",
    "        \n",
    "        # Verify shape of arrays\n",
    "        try:\n",
    "            grid_height, grid_width = zone_data[\"prob_streams\"].shape\n",
    "            logger.info(f\"Processing {zone_name}: array shape = {grid_height}x{grid_width}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting array shape for {zone_name}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        transform = from_bounds(ul_x, lr_y, lr_x, ul_y, grid_width, grid_height)\n",
    "\n",
    "        # Save probability rasters as float32\n",
    "        save_tif(os.path.join(output_path, f\"{zone_name}_stream_prob.tif\"),\n",
    "                 zone_data[\"prob_streams\"].astype(np.float32), \n",
    "                 transform, crs, dtype='float32')\n",
    "        \n",
    "        save_tif(os.path.join(output_path, f\"{zone_name}_ditch_prob.tif\"),\n",
    "                 zone_data[\"prob_ditches\"].astype(np.float32), \n",
    "                 transform, crs, dtype='float32')\n",
    "\n",
    "        # Handle classification\n",
    "        if \"pred_combined\" in zone_data:\n",
    "            classification = zone_data[\"pred_combined\"].astype(np.uint8)\n",
    "        else:\n",
    "            # Compute classification from probabilities\n",
    "            probs = np.zeros((3, grid_height, grid_width), dtype=np.float32)\n",
    "            probs[0] = 1.0 - (zone_data[\"prob_streams\"] + zone_data[\"prob_ditches\"])\n",
    "            probs[1] = zone_data[\"prob_streams\"]\n",
    "            probs[2] = zone_data[\"prob_ditches\"]\n",
    "            classification = np.argmax(probs, axis=0).astype(np.uint8)\n",
    "            \n",
    "        save_tif(os.path.join(output_path, f\"{zone_name}_classification.tif\"),\n",
    "                 classification, transform, crs, dtype='uint8')\n",
    "        \n",
    "        success_count += 1\n",
    "        logger.info(f\"✅ Exported {zone_name}\")\n",
    "    \n",
    "    logger.info(f\"Export complete. Successfully processed {success_count} zones.\")\n",
    "\n",
    "# Main execution\n",
    "logger.info(\"Starting export process...\")\n",
    "try:\n",
    "    export_predictions(predictions, zone_boundaries, zone_name_to_id, output_path)\n",
    "    logger.info(\"Export process completed successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during export: {str(e)}\")\n",
    "    import traceback\n",
    "    logger.error(traceback.format_exc())\n",
    "\n",
    "    logger.info(f\"Exporting predictions to GeoTIFF in {output_path}\")\n",
    "    pass\n",
    "\n",
    "def save_predictions_to_zarr(predictions, output_path, zone_boundaries, zone_name_to_id):\n",
    "    \"\"\"Save prediction results to a Zarr file format.\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Saving predictions to Zarr store: {output_path}\")\n",
    "    \n",
    "    # Define compression parameters\n",
    "    compression_params = {\n",
    "        'compressor': zarr.Blosc(cname='zstd', clevel=3)\n",
    "    }\n",
    "    \n",
    "    # Create root zarr group\n",
    "    root = zarr.open(output_path, mode='w')\n",
    "    \n",
    "    # Create a group for zones\n",
    "    zones_group = root.create_group('zones')\n",
    "    \n",
    "    # Create metadata group and add attributes\n",
    "    metadata = root.create_group('metadata')\n",
    "    metadata.attrs['crs'] = \"EPSG:3067\"\n",
    "    metadata.attrs['created'] = np.datetime64('now').astype(str)\n",
    "    \n",
    "    # Add zone boundaries as attributes\n",
    "    boundaries_group = metadata.create_group('boundaries')\n",
    "    for zone_name, bounds in zone_boundaries.items():\n",
    "        zone_bounds = boundaries_group.create_group(zone_name)\n",
    "        zone_bounds.attrs['upper_left'] = bounds['upper_left']\n",
    "        zone_bounds.attrs['lower_right'] = bounds['lower_right']\n",
    "    \n",
    "    # Process each zone separately\n",
    "    logger.info(f\"Saving zone-by-zone predictions to Zarr\")\n",
    "    success_count = 0\n",
    "    \n",
    "    for zone_name, zone_data in predictions.items():\n",
    "        # Skip non-zone entries or invalid zones\n",
    "        if zone_name not in zone_boundaries:\n",
    "            continue\n",
    "            \n",
    "        # Check if this zone has prediction data\n",
    "        if not isinstance(zone_data, dict):\n",
    "            logger.warning(f\"⚠️ Zone data for {zone_name} is not a dictionary\")\n",
    "            continue\n",
    "            \n",
    "        if \"prob_streams\" not in zone_data or \"prob_ditches\" not in zone_data:\n",
    "            logger.warning(f\"⚠️ Missing probability data for {zone_name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get zone ID\n",
    "            zone_id = zone_name_to_id.get(zone_name)\n",
    "            \n",
    "            # Create zone group\n",
    "            zone_group = zones_group.create_group(zone_name)\n",
    "            zone_group.attrs['zone_id'] = zone_id\n",
    "            \n",
    "            # Save probability arrays\n",
    "            zone_group.create_dataset('prob_streams', \n",
    "                                    data=zone_data[\"prob_streams\"].astype(np.float32),\n",
    "                                    chunks=(1250, 1250),\n",
    "                                    **compression_params)\n",
    "            \n",
    "            zone_group.create_dataset('prob_ditches', \n",
    "                                    data=zone_data[\"prob_ditches\"].astype(np.float32),\n",
    "                                    chunks=(1250, 1250),\n",
    "                                    **compression_params)\n",
    "            \n",
    "            # Add additional datasets if they exist\n",
    "            if \"pred_streams\" in zone_data:\n",
    "                zone_group.create_dataset(\"pred_streams\", \n",
    "                                     data=zone_data[\"pred_streams\"], \n",
    "                                     dtype=np.float32, \n",
    "                                     chunks=(1250, 1250),\n",
    "                                     **compression_params)\n",
    "                                     \n",
    "            if \"pred_ditches\" in zone_data:\n",
    "                zone_group.create_dataset(\"pred_ditches\", \n",
    "                                     data=zone_data[\"pred_ditches\"], \n",
    "                                     dtype=np.float32, \n",
    "                                     chunks=(1250, 1250),\n",
    "                                     **compression_params)\n",
    "                                     \n",
    "            if \"pred_combined\" in zone_data:\n",
    "                zone_group.create_dataset(\"pred_combined\", \n",
    "                                     data=zone_data[\"pred_combined\"], \n",
    "                                     dtype=np.float32, \n",
    "                                     chunks=(1250, 1250),\n",
    "                                     **compression_params)\n",
    "                                     \n",
    "            if \"prob_combined\" in zone_data:\n",
    "                zone_group.create_dataset(\"prob_combined\", \n",
    "                                     data=zone_data[\"prob_combined\"], \n",
    "                                     dtype=np.float32, \n",
    "                                     chunks=(1250, 1250),\n",
    "                                     **compression_params)\n",
    "            \n",
    "            # Save or compute classification\n",
    "            grid_height, grid_width = zone_data[\"prob_streams\"].shape\n",
    "            \n",
    "            if \"pred_combined\" in zone_data:\n",
    "                classification = zone_data[\"pred_combined\"].astype(np.uint8)\n",
    "            else:\n",
    "                # Compute classification from probabilities\n",
    "                probs = np.zeros((3, grid_height, grid_width), dtype=np.float32)\n",
    "                probs[0] = 1.0 - (zone_data[\"prob_streams\"] + zone_data[\"prob_ditches\"])\n",
    "                probs[1] = zone_data[\"prob_streams\"]\n",
    "                probs[2] = zone_data[\"prob_ditches\"]\n",
    "                classification = np.argmax(probs, axis=0).astype(np.uint8)\n",
    "            \n",
    "            zone_group.create_dataset('classification', \n",
    "                                    data=classification,\n",
    "                                    chunks=(1250, 1250),\n",
    "                                    **compression_params)\n",
    "            \n",
    "            success_count += 1\n",
    "            logger.info(f\"✅ Saved {zone_name} to Zarr\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving {zone_name} to Zarr: {str(e)}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "    \n",
    "\n",
    "    logger.info(f\"Saving predictions to Zarr in {output_path}\")\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the script.\"\"\"\n",
    "    # Set up directories and memory cache\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    os.makedirs(models_dir, exist_ok=True)  # Ensure models directory exists\n",
    "    memory = setup_joblib_cache(cache_dir)\n",
    "    \n",
    "    # Fix the zarr file features if needed\n",
    "    fix_zarr_file_features(zarr_file)\n",
    "    \n",
    "    # Process each zone\n",
    "    all_predictions = {}\n",
    "    zones_processed = 0\n",
    "    \n",
    "    for zone_name in zones_to_process:\n",
    "        logger.info(f\"Processing zone: {zone_name}\")\n",
    "        \n",
    "        predictions = predict_for_new_zone(\n",
    "            zarr_file=zarr_file,\n",
    "            zone_name=zone_name,\n",
    "            models_dir=models_dir,\n",
    "            selected_features=selected_features,\n",
    "            output_path=output_path\n",
    "        )\n",
    "        print_memory_usage()\n",
    "        \n",
    "        if predictions:\n",
    "            all_predictions[zone_name] = predictions\n",
    "            logger.info(f\"Successfully processed zone: {zone_name}\")\n",
    "            zones_processed += 1\n",
    "        else:\n",
    "            logger.warning(f\"Failed to process zone: {zone_name}\")\n",
    "    \n",
    "    # After processing all zones, export everything to both formats - only if we have predictions\n",
    "    if zones_processed > 0 and all_predictions:\n",
    "        logger.info(f\"Exporting {len(all_predictions)} zone predictions...\")\n",
    "        \n",
    "        # Transform the predictions\n",
    "        transformed_predictions = transform_predictions(all_predictions)\n",
    "        \n",
    "        # Export to GeoTIFF\n",
    "        export_predictions(all_predictions, zone_boundaries, zone_name_to_id, output_path)\n",
    "        \n",
    "        # Export to Zarr using the existing function, but with transformed data\n",
    "        zarr_output_path = os.path.join(output_path, \"predictions.zarr\")\n",
    "        try:\n",
    "            # Note the parameter order matches the existing function\n",
    "            save_predictions_to_zarr(transformed_predictions, zarr_output_path, zone_boundaries, zone_name_to_id)\n",
    "            logger.info(f\"Successfully exported {zones_processed} zones.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving to Zarr: {str(e)}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            logger.warning(\"Export process encountered errors.\")\n",
    "    else:\n",
    "        logger.warning(\"No zones were successfully processed. Nothing to export.\")\n",
    "    \n",
    "    logger.info(\"Processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
