{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the trained model to create probabilities and to predict\n",
    "\n",
    "These zones were not used in training. Python 3.11.9 was in use in here. \n",
    "\n",
    "- First code makes predictions\n",
    "- Taking the results to GeoTifs\n",
    "- Last code makes the probabilities\n",
    "- Taking the results to GeoTifs\n",
    "\n",
    "It's faster to create the codes with AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude code, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zarr\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, precision_score, f1_score, \n",
    "    confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_spatial_indices(spatial_shape):\n",
    "    \"\"\"Generate row and column indices for a given spatial shape.\"\"\"\n",
    "    total_pixels = spatial_shape[0] * spatial_shape[1]\n",
    "    row_indices = np.array([i // spatial_shape[1] for i in range(total_pixels)])\n",
    "    col_indices = np.array([i % spatial_shape[1] for i in range(total_pixels)])\n",
    "    return row_indices, col_indices\n",
    "\n",
    "def load_zone_data(zarr_file, zone, selected_features):\n",
    "    \"\"\"Load data for a specific zone with selected features.\"\"\"\n",
    "    try:\n",
    "        root = zarr.open(zarr_file, mode='r')\n",
    "        zone_data = root[zone]\n",
    "        \n",
    "        # Check if this zone has the target variable\n",
    "        if 'label_3m' not in zone_data.keys():\n",
    "            logger.warning(f\"Zone {zone} does not have label_3m, skipping\")\n",
    "            return None\n",
    "        \n",
    "        # Create a dictionary to store data\n",
    "        zone_dict = {'zone_id': []}\n",
    "        \n",
    "        # Get spatial indices\n",
    "        if 'row_idx' in zone_data and 'col_idx' in zone_data:\n",
    "            row_idx = zone_data['row_idx'][:]\n",
    "            col_idx = zone_data['col_idx'][:]\n",
    "        else:\n",
    "            # Generate spatial indices if they don't exist\n",
    "            spatial_shape = (5000, 5000)  # Adjust based on your actual data\n",
    "            row_idx, col_idx = generate_spatial_indices(spatial_shape)\n",
    "            \n",
    "        # Load the target variable\n",
    "        labels = zone_data['label_3m'][:]\n",
    "        \n",
    "        # Add zone_id (extracted from zone_name)\n",
    "        try:\n",
    "            zone_id = int(zone.split('_')[1]) - 1  # Convert to 0-indexed\n",
    "            zone_dict['zone_id'] = [zone_id] * len(labels)\n",
    "        except (IndexError, ValueError):\n",
    "            logger.warning(f\"Could not extract zone ID from {zone}\")\n",
    "            zone_dict['zone_id'] = [0] * len(labels)  # Default to 0\n",
    "        \n",
    "        # Add spatial indices\n",
    "        zone_dict['row_idx'] = row_idx\n",
    "        zone_dict['col_idx'] = col_idx\n",
    "        \n",
    "        # Add labels\n",
    "        zone_dict['label_3m'] = labels\n",
    "        \n",
    "        # Add selected features\n",
    "        for feature in selected_features:\n",
    "            if feature in zone_data.keys():\n",
    "                zone_dict[feature] = zone_data[feature][:]\n",
    "            else:\n",
    "                logger.warning(f\"Feature {feature} not found in {zone}\")\n",
    "                zone_dict[feature] = np.zeros_like(labels)  # Default to zeros\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(zone_dict)\n",
    "        logger.info(f\"Loaded data from {zone}, shape: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data from {zone}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_ground_truth(zones_to_predict):\n",
    "    \"\"\"\n",
    "    Load ground truth labels for specified zones using parallelization.\n",
    "    \"\"\"\n",
    "    zarr_file = \"zones_data_2.zarr\"  # Use your main zarr file path\n",
    "    \n",
    "    # Use multiprocessing to load data in parallel\n",
    "    total_cpus = min(15, multiprocessing.cpu_count())\n",
    "    zone_data_list = joblib.Parallel(n_jobs=total_cpus)(\n",
    "        joblib.delayed(load_zone_data)(zarr_file, zone, ['label_3m']) \n",
    "        for zone in zones_to_predict\n",
    "    )\n",
    "    \n",
    "    # Remove None values and extract labels\n",
    "    ground_truth_dfs = [df for df in zone_data_list if df is not None]\n",
    "    \n",
    "    if not ground_truth_dfs:\n",
    "        logger.warning(\"No ground truth data loaded\")\n",
    "        return np.array([])\n",
    "    \n",
    "    # Combine all data and return only the labels\n",
    "    combined_df = pd.concat(ground_truth_dfs, ignore_index=True)\n",
    "    return combined_df['label_3m'].values\n",
    "\n",
    "def predict_for_batch(batch_idx, zones, rf_models, zarr_file, selected_features, results_dir):\n",
    "    \"\"\"Generate predictions for a batch of zones and save to zarr.\"\"\"\n",
    "    logger.info(f\"Generating predictions for batch {batch_idx+1}\")\n",
    "    \n",
    "    # Load data for this batch\n",
    "    zone_data_list = joblib.Parallel(n_jobs=min(15, multiprocessing.cpu_count()))(\n",
    "        joblib.delayed(load_zone_data)(zarr_file, zone, selected_features) \n",
    "        for zone in zones\n",
    "    )\n",
    "    \n",
    "    # Remove None values\n",
    "    zone_data_list = [df for df in zone_data_list if df is not None]\n",
    "    \n",
    "    if not zone_data_list:\n",
    "        logger.warning(f\"No valid data loaded for batch {batch_idx+1}\")\n",
    "        return False\n",
    "    \n",
    "    # Combine all zone data\n",
    "    combined_df = pd.concat(zone_data_list, ignore_index=True)\n",
    "    \n",
    "    # Prepare feature data\n",
    "    X = combined_df[selected_features].values\n",
    "    \n",
    "    # Make predictions using each model and average them\n",
    "    predictions = []\n",
    "    for model in rf_models:\n",
    "        predictions.append(model.predict(X))\n",
    "    \n",
    "    # Average predictions from all models\n",
    "    final_predictions = np.round(np.mean(predictions, axis=0)).astype(int)\n",
    "    \n",
    "    # Save predictions to zarr file\n",
    "    pred_file = f\"predicted_zones_batch_{batch_idx+1}.zarr\"\n",
    "    zarr_group = zarr.open(pred_file, mode=\"w\")\n",
    "    zarr_group.create_dataset(\"predictions\", data=final_predictions)\n",
    "    \n",
    "    # Add metadata\n",
    "    zarr_group.attrs[\"zones\"] = zones\n",
    "    zarr_group.attrs[\"prediction_date\"] = str(pd.Timestamp.now())\n",
    "    \n",
    "    logger.info(f\"Saved predictions for batch {batch_idx+1} to {pred_file}\")\n",
    "    return True\n",
    "\n",
    "def process_batch(batch_idx, zones_to_predict, results_dir):\n",
    "    \"\"\"Process a single batch with parallelization.\"\"\"\n",
    "    logger.info(f\"Processing batch {batch_idx+1}\")\n",
    "    \n",
    "    # Load predicted data \n",
    "    pred_file = f\"predicted_zones_batch_{batch_idx+1}.zarr\"\n",
    "    if not os.path.exists(pred_file):\n",
    "        logger.warning(f\"Prediction file {pred_file} not found. Skipping batch {batch_idx+1}.\")\n",
    "        return None\n",
    "    \n",
    "    zarr_group = zarr.open(pred_file, mode=\"r\")\n",
    "    pred_combined = zarr_group[\"predictions\"][:]\n",
    "    \n",
    "    # Load ground truth for metrics\n",
    "    y_test_batch = load_ground_truth(zones_to_predict)\n",
    "    \n",
    "    if len(y_test_batch) == 0:\n",
    "        logger.warning(f\"No ground truth data loaded for batch {batch_idx+1}. Skipping...\")\n",
    "        return None\n",
    "    \n",
    "    # Check that shapes match\n",
    "    if y_test_batch.shape != pred_combined.flatten().shape:\n",
    "        logger.warning(f\"Shape mismatch between ground truth ({y_test_batch.shape}) and predictions ({pred_combined.flatten().shape}). Skipping batch {batch_idx+1}.\")\n",
    "        return None\n",
    "    \n",
    "    # Compute binary classification metrics (Water vs Background)\n",
    "    binary_y_test = (y_test_batch > 0).astype(int)\n",
    "    binary_pred = (pred_combined.flatten() > 0).astype(int)\n",
    "    \n",
    "    binary_metrics = {\n",
    "        \"kappa\": cohen_kappa_score(binary_y_test, binary_pred),\n",
    "        \"accuracy\": accuracy_score(binary_y_test, binary_pred),\n",
    "        \"recall\": recall_score(binary_y_test, binary_pred),\n",
    "        \"precision\": precision_score(binary_y_test, binary_pred),\n",
    "        \"f1_score\": f1_score(binary_y_test, binary_pred),\n",
    "    }\n",
    "    \n",
    "    # Multi-class classification metrics\n",
    "    multi_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test_batch, pred_combined.flatten()),\n",
    "        \"weighted_f1\": f1_score(y_test_batch, pred_combined.flatten(), average=\"weighted\"),\n",
    "        \"macro_f1\": f1_score(y_test_batch, pred_combined.flatten(), average=\"macro\"),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test_batch, pred_combined.flatten()),\n",
    "    }\n",
    "    \n",
    "    multi_class_kappa = cohen_kappa_score(y_test_batch, pred_combined.flatten())\n",
    "    \n",
    "    # Save metrics for this batch\n",
    "    metrics_file = os.path.join(results_dir, f\"metrics_batch_{batch_idx+1}.joblib\")\n",
    "    joblib.dump({\"binary\": binary_metrics, \"multi\": multi_metrics}, metrics_file)\n",
    "    \n",
    "    logger.info(f\"Metrics for batch {batch_idx+1} saved to {metrics_file}\")\n",
    "    \n",
    "    # Generate and save plots for batch\n",
    "    logger.info(f\"Generating plots for batch {batch_idx+1}...\")\n",
    "\n",
    "    def plot_confusion_matrix(cm, class_names, filename):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.7)\n",
    "\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(x=j, y=i, s=cm[i, j], va=\"center\", ha=\"center\", fontsize=12)\n",
    "\n",
    "        ax.set_xlabel(\"Predicted Labels\")\n",
    "        ax.set_ylabel(\"True Labels\")\n",
    "        ax.set_title(\"Confusion Matrix\")\n",
    "        ax.set_xticks(range(len(class_names)))\n",
    "        ax.set_xticklabels(class_names)\n",
    "        ax.set_yticks(range(len(class_names)))\n",
    "        ax.set_yticklabels(class_names)\n",
    "\n",
    "        plt.savefig(os.path.join(results_dir, filename))\n",
    "        plt.close()\n",
    "\n",
    "    # Ensure confusion matrix exists\n",
    "    if \"confusion_matrix\" in multi_metrics:\n",
    "        plot_confusion_matrix(\n",
    "            multi_metrics[\"confusion_matrix\"], \n",
    "            [\"Background\", \"Stream\", \"Ditch\"], \n",
    "            f\"confusion_matrix_batch_{batch_idx+1}.png\"\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"Confusion matrix missing from multi_metrics!\")\n",
    "\n",
    "    # Reshape and visualize the streams (1) and ditches (2)\n",
    "    if pred_combined.shape[0] == 5000 * 5000:\n",
    "        pred_combined = pred_combined.reshape(5000, 5000)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected data shape: {pred_combined.shape}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(pred_combined, cmap='tab20c', interpolation='nearest', alpha=0.7)\n",
    "    plt.title(f\"Predicted Streams (1) and Ditches (2) for Batch {batch_idx+1}\")\n",
    "    plt.colorbar(label='Predicted Class')\n",
    "    plt.xlabel(\"Pixel X\")\n",
    "    plt.ylabel(\"Pixel Y\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"streams_and_ditches_batch_{batch_idx+1}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    logger.info(f\"Finished processing batch {batch_idx+1}.\")\n",
    "\n",
    "    return {\n",
    "        \"binary_metrics\": binary_metrics,\n",
    "        \"multi_metrics\": multi_metrics\n",
    "    }\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate mean and 95% confidence interval for given data.\"\"\"\n",
    "    import scipy.stats as st\n",
    "    \n",
    "    mean = np.mean(data)\n",
    "    # Calculate confidence interval\n",
    "    se = st.sem(data)\n",
    "    h = se * st.t.ppf((1 + confidence) / 2, len(data) - 1)\n",
    "    return mean, mean - h, mean + h\n",
    "\n",
    "def main():\n",
    "    # Define paths and settings\n",
    "    results_dir = \"../02_Results\"\n",
    "    zarr_file = \"zones_data_2.zarr\"\n",
    "    models_dir = os.path.join(results_dir, \"models\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Define zone batches (using your existing definition)\n",
    "    zone_batches = [\n",
    "        [\"zone_1\", \"zone_3\", \"zone_4\", \"zone_6\"], \n",
    "        [\"zone_7\", \"zone_8\", \"zone_10\"]\n",
    "    ]\n",
    "    \n",
    "    # Selected features for prediction\n",
    "    # Update your selected_features to include all 11 features\n",
    "    selected_features = [\n",
    "        'col_idx', 'row_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', \n",
    "        'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels'\n",
    "    ]\n",
    "        \n",
    "    # First load the models\n",
    "    logger.info(\"Loading trained models...\")\n",
    "    model_paths = [os.path.join(models_dir, f) for f in os.listdir(models_dir) if f.endswith(\".joblib\")]\n",
    "    rf_models = [joblib.load(model_path) for model_path in model_paths]\n",
    "    logger.info(f\"Loaded {len(rf_models)} models\")\n",
    "    \n",
    "    # Then generate predictions\n",
    "    logger.info(\"Generating predictions for all batches...\")\n",
    "    prediction_results = []\n",
    "    for i, zones in enumerate(zone_batches):\n",
    "        prediction_success = predict_for_batch(i, zones, rf_models, zarr_file, selected_features, results_dir)\n",
    "        prediction_results.append(prediction_success)\n",
    "\n",
    "    if not any(prediction_results):\n",
    "        logger.error(\"Failed to generate predictions for any batch. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    total_cpus = min(15, multiprocessing.cpu_count())\n",
    "    logger.info(f\"Using {total_cpus} CPU cores for processing\")\n",
    "    \n",
    "    # Process each batch in parallel using a pool\n",
    "    results = joblib.Parallel(n_jobs=min(len(zone_batches), total_cpus))(\n",
    "        joblib.delayed(process_batch)(i, zones, results_dir) \n",
    "        for i, zones in enumerate(zone_batches)\n",
    "    )\n",
    "    \n",
    "    # Filter out None results\n",
    "    results = [r for r in results if r is not None]\n",
    "    \n",
    "    if not results:\n",
    "        logger.warning(\"No valid results to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Extract metrics for confidence intervals\n",
    "    original_stats = [r[\"binary_metrics\"][\"f1_score\"] for r in results]\n",
    "    enhanced_stats = [r[\"multi_metrics\"][\"weighted_f1\"] for r in results]\n",
    "    \n",
    "    # Calculate confidence intervals and plot comparison\n",
    "    if original_stats and enhanced_stats:\n",
    "        # Calculate confidence intervals\n",
    "        orig_mean, orig_low, orig_high = mean_confidence_interval(original_stats)\n",
    "        enh_mean, enh_low, enh_high = mean_confidence_interval(enhanced_stats)\n",
    "        \n",
    "        print(f\"Original method performance: {orig_mean:.4f} (95% CI: {orig_low:.4f}-{orig_high:.4f})\")\n",
    "        print(f\"Enhanced method performance: {enh_mean:.4f} (95% CI: {enh_low:.4f}-{enh_high:.4f})\")\n",
    "        \n",
    "        # Plot comparison of metrics\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(['Original', 'Enhanced'], [orig_mean, enh_mean], yerr=[[orig_mean-orig_low, enh_mean-enh_low], \n",
    "                                                                 [orig_high-orig_mean, enh_high-enh_mean]])\n",
    "        plt.title('Performance Comparison with 95% Confidence Intervals')\n",
    "        plt.ylabel('Performance Metric')\n",
    "        plt.savefig(os.path.join(results_dir, 'performance_comparison.png'), dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        # Save overall metrics and confidence intervals\n",
    "        overall_results = {\n",
    "            \"original_metrics\": {\n",
    "                \"mean\": orig_mean,\n",
    "                \"ci_low\": orig_low,\n",
    "                \"ci_high\": orig_high\n",
    "            },\n",
    "            \"enhanced_metrics\": {\n",
    "                \"mean\": enh_mean,\n",
    "                \"ci_low\": enh_low,\n",
    "                \"ci_high\": enh_high\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        joblib.dump(overall_results, os.path.join(results_dir, \"overall_metrics.joblib\"))\n",
    "        logger.info(\"Analysis complete. Overall results saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      " └── predictions (100000000,) int64\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "results_batch1 = zarr.open('predicted_zones_batch_1.zarr', mode='r')\n",
    "# Explore the data structure\n",
    "print(results_batch1.tree())\n",
    "# Access specific arrays\n",
    "# example: data = results_batch1['your_array_name'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions to GeoTifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 11:47:42,533 - __main__ - INFO - Starting export from Zarr files\n",
      "2025-04-01 11:47:42,534 - __main__ - INFO - Processing batch 1\n",
      "2025-04-01 11:47:42,534 - __main__ - INFO - Processing batch 1 with zones: ['zone_1', 'zone_3', 'zone_4', 'zone_6']\n",
      "2025-04-01 11:47:43,250 - __main__ - INFO - Loaded predictions array with shape (100000000,)\n",
      "2025-04-01 11:47:43,250 - __main__ - INFO - Extracted predictions for zone_1 with shape (5000, 5000)\n",
      "2025-04-01 11:47:43,956 - __main__ - INFO - Saved classification map for zone_1 to ../02_Results/geotiff_results/zone_1_classification.tif\n",
      "2025-04-01 11:47:43,957 - __main__ - INFO - Extracted predictions for zone_3 with shape (5000, 5000)\n",
      "2025-04-01 11:47:44,379 - __main__ - INFO - Saved classification map for zone_3 to ../02_Results/geotiff_results/zone_3_classification.tif\n",
      "2025-04-01 11:47:44,380 - __main__ - INFO - Extracted predictions for zone_4 with shape (5000, 5000)\n",
      "2025-04-01 11:47:44,612 - __main__ - INFO - Saved classification map for zone_4 to ../02_Results/geotiff_results/zone_4_classification.tif\n",
      "2025-04-01 11:47:44,612 - __main__ - INFO - Extracted predictions for zone_6 with shape (5000, 5000)\n",
      "2025-04-01 11:47:44,840 - __main__ - INFO - Saved classification map for zone_6 to ../02_Results/geotiff_results/zone_6_classification.tif\n",
      "2025-04-01 11:47:44,884 - __main__ - INFO - Processing batch 2\n",
      "2025-04-01 11:47:44,885 - __main__ - INFO - Processing batch 2 with zones: ['zone_7', 'zone_8', 'zone_10']\n",
      "2025-04-01 11:47:45,186 - __main__ - INFO - Loaded predictions array with shape (75000000,)\n",
      "2025-04-01 11:47:45,187 - __main__ - INFO - Extracted predictions for zone_7 with shape (5000, 5000)\n",
      "2025-04-01 11:47:45,488 - __main__ - INFO - Saved classification map for zone_7 to ../02_Results/geotiff_results/zone_7_classification.tif\n",
      "2025-04-01 11:47:45,488 - __main__ - INFO - Extracted predictions for zone_8 with shape (5000, 5000)\n",
      "2025-04-01 11:47:45,715 - __main__ - INFO - Saved classification map for zone_8 to ../02_Results/geotiff_results/zone_8_classification.tif\n",
      "2025-04-01 11:47:45,715 - __main__ - INFO - Extracted predictions for zone_10 with shape (5000, 5000)\n",
      "2025-04-01 11:47:45,943 - __main__ - INFO - Saved classification map for zone_10 to ../02_Results/geotiff_results/zone_10_classification.tif\n",
      "2025-04-01 11:47:45,976 - __main__ - INFO - All exports completed successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import zarr\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define zone coordinates\n",
    "zone_boundaries = {\n",
    "    \"zone_1\": {\"upper_left\": (377982, 6854660), \"lower_right\": (380482, 6852160)},\n",
    "    \"zone_2\": {\"upper_left\": (377982, 6857160), \"lower_right\": (380482, 6854660)},\n",
    "    \"zone_3\": {\"upper_left\": (380482, 6857160), \"lower_right\": (382982, 6854660)},\n",
    "    \"zone_4\": {\"upper_left\": (375482, 6859660), \"lower_right\": (377982, 6857160)},\n",
    "    \"zone_5\": {\"upper_left\": (377982, 6859660), \"lower_right\": (380482, 6857160)},\n",
    "    \"zone_6\": {\"upper_left\": (380482, 6859660), \"lower_right\": (382982, 6857160)},\n",
    "    \"zone_7\": {\"upper_left\": (375482, 6862159.999999999), \"lower_right\": (377982, 6859660)},\n",
    "    \"zone_8\": {\"upper_left\": (377982, 6862159.999999999), \"lower_right\": (380482, 6859660)},\n",
    "    \"zone_9\": {\"upper_left\": (380482, 6862159.999999999), \"lower_right\": (382982, 6859660)},\n",
    "    \"zone_10\": {\"upper_left\": (372982, 6864660), \"lower_right\": (375482, 6862159.999999999)},\n",
    "    \"zone_11\": {\"upper_left\": (375482, 6864660), \"lower_right\": (377982, 6862159.999999999)},\n",
    "    \"zone_12\": {\"upper_left\": (377982, 6864660), \"lower_right\": (380482, 6862159.999999999)},\n",
    "    \"zone_13\": {\"upper_left\": (370482, 6867160), \"lower_right\": (372982, 6864660)},\n",
    "    \"zone_14\": {\"upper_left\": (372982, 6867160), \"lower_right\": (375482, 6864660)},\n",
    "    \"zone_15\": {\"upper_left\": (375482, 6867160), \"lower_right\": (377982, 6864660)},\n",
    "    \"zone_16\": {\"upper_left\": (377982, 6867160), \"lower_right\": (380482, 6864660)},\n",
    "    \"zone_17\": {\"upper_left\": (370482, 6869660), \"lower_right\": (372982, 6867160)},\n",
    "    \"zone_18\": {\"upper_left\": (372982, 6869660), \"lower_right\": (375482, 6867160)},\n",
    "    \"zone_19\": {\"upper_left\": (375482, 6869660), \"lower_right\": (377982, 6867160)},\n",
    "    \"zone_20\": {\"upper_left\": (372982, 6872160), \"lower_right\": (375482, 6869660)},\n",
    "    \"zone_21\": {\"upper_left\": (375482, 6872160), \"lower_right\": (377982, 6869660)}\n",
    "}\n",
    "\n",
    "def export_zarr_to_geotiff_by_batch(zarr_file, output_path, batch_number):\n",
    "    \"\"\"\n",
    "    Export predictions from a Zarr file to GeoTIFF format for zones in a specific batch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zarr_file : str\n",
    "        Path to the zarr file containing the prediction results\n",
    "    output_path : str\n",
    "        Directory to save the GeoTIFF files\n",
    "    batch_number : int\n",
    "        The batch number (1 or 2) to process\n",
    "    \"\"\"\n",
    "    # Define the zones in each batch\n",
    "    zone_batches = [\n",
    "        [\"zone_1\", \"zone_3\", \"zone_4\", \"zone_6\"], \n",
    "        [\"zone_7\", \"zone_8\", \"zone_10\"]\n",
    "    ]\n",
    "    \n",
    "    # Select the zones for the requested batch\n",
    "    if batch_number < 1 or batch_number > len(zone_batches):\n",
    "        logger.error(f\"Invalid batch number: {batch_number}. Should be between 1 and {len(zone_batches)}\")\n",
    "        return False\n",
    "    \n",
    "    target_zones = zone_batches[batch_number - 1]\n",
    "    logger.info(f\"Processing batch {batch_number} with zones: {target_zones}\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Open Zarr file\n",
    "        root = zarr.open(zarr_file, mode=\"r\")\n",
    "        \n",
    "        # Check for predictions array\n",
    "        if \"predictions\" not in root:\n",
    "            logger.error(\"No 'predictions' array found in Zarr file\")\n",
    "            return False\n",
    "            \n",
    "        predictions = root[\"predictions\"][:]\n",
    "        logger.info(f\"Loaded predictions array with shape {predictions.shape}\")\n",
    "        \n",
    "        # Standard grid size for all zones (5000x5000)\n",
    "        grid_width, grid_height = 5000, 5000\n",
    "        \n",
    "        # Calculate prediction indices for each zone\n",
    "        # For simplicity, we'll assume predictions are stored in same order as zones in the batch\n",
    "        total_pixels_per_zone = grid_width * grid_height\n",
    "        \n",
    "        for i, zone_name in enumerate(target_zones):\n",
    "            try:\n",
    "                # Get coordinates for this zone\n",
    "                if zone_name not in zone_boundaries:\n",
    "                    logger.error(f\"No coordinates found for {zone_name}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                boundaries = zone_boundaries[zone_name]\n",
    "                ul_x, ul_y = boundaries[\"upper_left\"]\n",
    "                lr_x, lr_y = boundaries[\"lower_right\"]\n",
    "                \n",
    "                # Calculate the index range for this zone\n",
    "                start_idx = i * total_pixels_per_zone\n",
    "                end_idx = start_idx + total_pixels_per_zone\n",
    "                \n",
    "                # Extract predictions for this zone\n",
    "                if end_idx <= len(predictions):\n",
    "                    # Reshape to 2D grid\n",
    "                    zone_predictions = predictions[start_idx:end_idx].reshape(grid_height, grid_width)\n",
    "                    logger.info(f\"Extracted predictions for {zone_name} with shape {zone_predictions.shape}\")\n",
    "                else:\n",
    "                    logger.error(f\"Not enough data for zone {zone_name}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Create transform\n",
    "                transform = from_bounds(ul_x, lr_y, lr_x, ul_y, grid_width, grid_height)\n",
    "                \n",
    "                # Save classification map\n",
    "                class_filename = f\"{output_path}/{zone_name}_classification.tif\"\n",
    "                meta = {\n",
    "                    'driver': 'GTiff',\n",
    "                    'height': grid_height,\n",
    "                    'width': grid_width,\n",
    "                    'count': 1,\n",
    "                    'dtype': str(zone_predictions.dtype),\n",
    "                    'crs': 'EPSG:3067',  # Finnish ETRS-TM35FIN coordinate system\n",
    "                    'transform': transform,\n",
    "                    'nodata': 0\n",
    "                }\n",
    "                \n",
    "                with rasterio.open(class_filename, 'w', **meta) as dst:\n",
    "                    dst.write(zone_predictions, 1)\n",
    "                \n",
    "                logger.info(f\"Saved classification map for {zone_name} to {class_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing zone {zone_name}: {e}\")\n",
    "                import traceback\n",
    "                logger.error(traceback.format_exc())\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting from Zarr to GeoTIFF: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "    \n",
    "    return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your Zarr files\n",
    "    zarr_file_batch1 = \"predicted_zones_batch_1.zarr\"\n",
    "    zarr_file_batch2 = \"predicted_zones_batch_2.zarr\"\n",
    "    \n",
    "    # Path for output GeoTIFFs\n",
    "    output_path = \"../02_Results/geotiff_results\"\n",
    "    \n",
    "    logger.info(\"Starting export from Zarr files\")\n",
    "    \n",
    "    # Export zones from batch 1\n",
    "    logger.info(\"Processing batch 1\")\n",
    "    success_batch1 = export_zarr_to_geotiff_by_batch(zarr_file_batch1, output_path, 1)\n",
    "    \n",
    "    # Export zones from batch 2\n",
    "    logger.info(\"Processing batch 2\")\n",
    "    success_batch2 = export_zarr_to_geotiff_by_batch(zarr_file_batch2, output_path, 2)\n",
    "    \n",
    "    if success_batch1 and success_batch2:\n",
    "        logger.info(\"All exports completed successfully\")\n",
    "    else:\n",
    "        logger.warning(\"Some exports failed, check the logs for details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGpt + Claude code, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 15:07:19,286 - __main__ - INFO - Loading trained models...\n",
      "/PUHTI_TYKKY_ZneHrnM/miniconda/envs/env1/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/PUHTI_TYKKY_ZneHrnM/miniconda/envs/env1/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-04-01 15:07:20,566 - __main__ - INFO - Loaded 9 models\n",
      "2025-04-01 15:07:20,567 - __main__ - INFO - Generating probabilities for all batches...\n",
      "2025-04-01 15:07:20,567 - __main__ - INFO - Generating probabilities for batch 1\n",
      "Feature zone_id not found in zone_1\n",
      "Feature spatial_shape not found in zone_1\n",
      "Feature zone_id not found in zone_6\n",
      "Feature zone_id not found in zone_3\n",
      "Feature zone_id not found in zone_4\n",
      "Feature spatial_shape not found in zone_6\n",
      "Feature spatial_shape not found in zone_3\n",
      "Feature spatial_shape not found in zone_4\n",
      "2025-04-01 15:09:20,043 - __main__ - INFO - Using spatial shape for prediction: 0\n",
      "2025-04-01 15:09:32,584 - __main__ - INFO - Input feature matrix shape: (100000000, 11)\n",
      "/PUHTI_TYKKY_ZneHrnM/miniconda/envs/env1/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zarr\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, precision_score, f1_score, \n",
    "    confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_spatial_indices(spatial_shape):\n",
    "    \"\"\"Generate row and column indices for a given spatial shape.\"\"\"\n",
    "    total_pixels = spatial_shape[0] * spatial_shape[1]\n",
    "    row_indices = np.array([i // spatial_shape[1] for i in range(total_pixels)])\n",
    "    col_indices = np.array([i % spatial_shape[1] for i in range(total_pixels)])\n",
    "    return row_indices, col_indices\n",
    "\n",
    "def load_zone_data(zarr_file, zone, selected_features):\n",
    "    \"\"\"Load data for a specific zone with selected features.\"\"\"\n",
    "    try:\n",
    "        root = zarr.open(zarr_file, mode='r')\n",
    "        zone_data = root[zone]\n",
    "        \n",
    "        if 'label_3m' not in zone_data.keys():\n",
    "            logger.warning(f\"Zone {zone} does not have label_3m, skipping\")\n",
    "            return None\n",
    "        \n",
    "        zone_dict = {'zone_id': []}\n",
    "        \n",
    "        # Get spatial dimensions from the data if available\n",
    "        if 'spatial_shape' in zone_data.attrs:\n",
    "            spatial_shape = zone_data.attrs['spatial_shape']\n",
    "            logger.info(f\"Using spatial shape from attributes: {spatial_shape}\")\n",
    "        elif 'row_idx' in zone_data and 'col_idx' in zone_data:\n",
    "            row_idx = zone_data['row_idx'][:]\n",
    "            col_idx = zone_data['col_idx'][:]\n",
    "            # Calculate approximate spatial shape based on max indices\n",
    "            max_row = np.max(row_idx) + 1\n",
    "            max_col = np.max(col_idx) + 1\n",
    "            spatial_shape = (max_row, max_col)\n",
    "            logger.info(f\"Calculated spatial shape from indices: {spatial_shape}\")\n",
    "        else:\n",
    "            spatial_shape = (5000, 5000)  # Default fallback\n",
    "            logger.warning(f\"No spatial information found for {zone}, using default shape: {spatial_shape}\")\n",
    "        \n",
    "        # Store spatial shape for later use\n",
    "        zone_dict['spatial_shape'] = spatial_shape\n",
    "        \n",
    "        if 'row_idx' in zone_data and 'col_idx' in zone_data:\n",
    "            row_idx = zone_data['row_idx'][:]\n",
    "            col_idx = zone_data['col_idx'][:]\n",
    "        else:\n",
    "            row_idx, col_idx = generate_spatial_indices(spatial_shape)\n",
    "        \n",
    "        labels = zone_data['label_3m'][:]\n",
    "        \n",
    "        try:\n",
    "            zone_id = int(zone.split('_')[1]) - 1\n",
    "            zone_dict['zone_id'] = [zone_id] * len(labels)\n",
    "        except (IndexError, ValueError):\n",
    "            logger.warning(f\"Could not extract zone ID from {zone}\")\n",
    "            zone_dict['zone_id'] = [0] * len(labels)\n",
    "        \n",
    "        zone_dict['row_idx'] = row_idx\n",
    "        zone_dict['col_idx'] = col_idx\n",
    "        zone_dict['label_3m'] = labels\n",
    "        \n",
    "        for feature in selected_features:\n",
    "            if feature in zone_data.keys():\n",
    "                zone_dict[feature] = zone_data[feature][:]\n",
    "            else:\n",
    "                logger.warning(f\"Feature {feature} not found in {zone}\")\n",
    "                zone_dict[feature] = np.zeros_like(labels)\n",
    "        \n",
    "        df = pd.DataFrame(zone_dict)\n",
    "        df['spatial_shape'] = df['spatial_shape'].astype(str)  # Convert tuple to string for DataFrame\n",
    "        logger.info(f\"Loaded data from {zone}, shape: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data from {zone}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_ground_truth(zones_to_predict):\n",
    "    zarr_file = \"zones_data_2.zarr\"\n",
    "    \n",
    "    total_cpus = min(15, multiprocessing.cpu_count())\n",
    "    zone_data_list = joblib.Parallel(n_jobs=total_cpus)(\n",
    "        joblib.delayed(load_zone_data)(zarr_file, zone, ['label_3m']) \n",
    "        for zone in zones_to_predict\n",
    "    )\n",
    "    \n",
    "    ground_truth_dfs = [df for df in zone_data_list if df is not None]\n",
    "    \n",
    "    if not ground_truth_dfs:\n",
    "        logger.warning(\"No ground truth data loaded\")\n",
    "        return np.array([])\n",
    "\n",
    "    combined_df = pd.concat(ground_truth_dfs, ignore_index=True)\n",
    "    return combined_df['label_3m'].values\n",
    "\n",
    "def predict_for_batch(batch_idx, zones, rf_models, zarr_file, selected_features, results_dir):\n",
    "    logger.info(f\"Generating probabilities for batch {batch_idx+1}\")\n",
    "\n",
    "    # Load data\n",
    "    zone_data_list = joblib.Parallel(n_jobs=min(15, multiprocessing.cpu_count()))(\n",
    "        joblib.delayed(load_zone_data)(zarr_file, zone, selected_features) \n",
    "        for zone in zones\n",
    "    )\n",
    "    \n",
    "    zone_data_list = [df for df in zone_data_list if df is not None]\n",
    "\n",
    "    if not zone_data_list:\n",
    "        logger.warning(f\"No valid data loaded for batch {batch_idx+1}\")\n",
    "        return False\n",
    "\n",
    "    combined_df = pd.concat(zone_data_list, ignore_index=True)\n",
    "    \n",
    "    # Extract spatial shape from the first zone (assuming all zones have the same shape)\n",
    "    # Convert string back to tuple if necessary\n",
    "    first_spatial_shape = combined_df['spatial_shape'].iloc[0]\n",
    "    if isinstance(first_spatial_shape, str):\n",
    "        import ast\n",
    "        spatial_shape = ast.literal_eval(first_spatial_shape)\n",
    "    else:\n",
    "        spatial_shape = first_spatial_shape\n",
    "        \n",
    "    logger.info(f\"Using spatial shape for prediction: {spatial_shape}\")\n",
    "    \n",
    "    # Remove spatial_shape from features for model input\n",
    "    feature_cols = [f for f in selected_features if f != 'spatial_shape']\n",
    "    X = combined_df[feature_cols].values\n",
    "\n",
    "    logger.info(f\"Input feature matrix shape: {X.shape}\")\n",
    "\n",
    "    # Generate predictions from each model\n",
    "    all_probabilities = [model.predict_proba(X) for model in rf_models]\n",
    "\n",
    "    fixed_probabilities = []\n",
    "    for i, prob in enumerate(all_probabilities):\n",
    "        if prob.ndim == 2:\n",
    "            if prob.shape[1] == 3:\n",
    "                fixed_probabilities.append(prob)  # Expected case (all 3 classes)\n",
    "            elif prob.shape[1] == 2:\n",
    "                logger.warning(f\"Model {i+1} returned 2-class probabilities. Assuming classes 0 & 1.\")\n",
    "                # Assume missing class 2 (ditches) → Add a column of zeros\n",
    "                prob_fixed = np.hstack([prob, np.zeros((prob.shape[0], 1))])\n",
    "                fixed_probabilities.append(prob_fixed)\n",
    "            elif prob.shape[1] == 1:\n",
    "                logger.warning(f\"Model {i+1} returned single-class probabilities. Assuming only class 2 (ditches).\")\n",
    "                # Assume this is predicting only ditches (class 2) → Add two zero columns for classes 0 & 1\n",
    "                prob_fixed = np.hstack([np.zeros((prob.shape[0], 2)), prob])\n",
    "                fixed_probabilities.append(prob_fixed)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected probability shape from model {i+1}: {prob.shape}\")\n",
    "        elif prob.ndim == 1:\n",
    "            logger.warning(f\"Model {i+1} returned a 1D probability array. Assuming class 2 only.\")\n",
    "            prob_fixed = np.hstack([np.zeros((prob.shape[0], 2)), prob.reshape(-1, 1)])\n",
    "            fixed_probabilities.append(prob_fixed)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {i+1} returned an unexpected shape: {prob.shape}\")\n",
    "\n",
    "    # Ensure all probability arrays are the same length\n",
    "    prob_lengths = [len(p) for p in fixed_probabilities]\n",
    "    if len(set(prob_lengths)) > 1:\n",
    "        raise ValueError(f\"Inconsistent probability array lengths: {set(prob_lengths)}\")\n",
    "\n",
    "    all_probabilities = fixed_probabilities\n",
    "\n",
    "    for i, prob in enumerate(all_probabilities):\n",
    "        logger.info(f\"Model {i+1} probability shape: {prob.shape}\")\n",
    "\n",
    "    # Averaging probabilities\n",
    "    averaged_probabilities = np.mean(all_probabilities, axis=0)\n",
    "\n",
    "    # Debugging: Print shape before slicing\n",
    "    logger.info(f\"Averaged probabilities shape: {averaged_probabilities.shape}\")\n",
    "\n",
    "    # Extract probabilities for streams (1) and ditches (2)\n",
    "    y_prob_final = averaged_probabilities[:, 1:3]  # Take only class 1 and 2\n",
    "\n",
    "    # Calculate the expected size based on actual data dimensions\n",
    "    expected_size = spatial_shape[0] * spatial_shape[1] * 2  # height * width * 2 classes\n",
    "    actual_size = y_prob_final.size\n",
    "    logger.info(f\"Expected size based on spatial shape: {expected_size}, Actual size: {actual_size}\")\n",
    "\n",
    "    # Check if we need to pad or truncate data to match expected size\n",
    "    if actual_size != expected_size:\n",
    "        logger.warning(f\"Size mismatch. Attempting to adjust data to match spatial dimensions.\")\n",
    "        \n",
    "        # If we have fewer samples than expected (missing data)\n",
    "        if actual_size < expected_size:\n",
    "            # Calculate how many samples we're missing\n",
    "            pixels_expected = spatial_shape[0] * spatial_shape[1]\n",
    "            pixels_actual = y_prob_final.shape[0]\n",
    "            pixels_missing = pixels_expected - pixels_actual\n",
    "            \n",
    "            logger.warning(f\"Missing {pixels_missing} pixels. Padding with zeros.\")\n",
    "            padding = np.zeros((pixels_missing, 2))\n",
    "            y_prob_final = np.vstack([y_prob_final, padding])\n",
    "        \n",
    "        # If we have more samples than expected (extra data)\n",
    "        elif actual_size > expected_size:\n",
    "            logger.warning(f\"Extra data detected. Truncating to fit spatial dimensions.\")\n",
    "            pixels_expected = spatial_shape[0] * spatial_shape[1]\n",
    "            y_prob_final = y_prob_final[:pixels_expected, :]\n",
    "    \n",
    "    # Reshape to spatial dimensions\n",
    "    try:\n",
    "        final_probabilities = y_prob_final.reshape(spatial_shape[0], spatial_shape[1], 2).astype(np.float32)\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Reshape failed: {e}\")\n",
    "        # Alternative: Save unreshaped data\n",
    "        logger.info(\"Saving unreshaped probabilities instead\")\n",
    "        final_probabilities = y_prob_final.astype(np.float32)\n",
    "        \n",
    "    # Save probabilities\n",
    "    prob_file = os.path.join(results_dir, f\"probabilities_zones_batch_{batch_idx+1}.zarr\")\n",
    "    zarr_group = zarr.open(prob_file, mode=\"w\")\n",
    "    zarr_group.create_dataset(\"probabilities\", data=final_probabilities, dtype=np.float32)\n",
    "    zarr_group.attrs[\"zones\"] = zones\n",
    "    zarr_group.attrs[\"spatial_shape\"] = spatial_shape\n",
    "    zarr_group.attrs[\"prediction_date\"] = str(pd.Timestamp.now())\n",
    "\n",
    "    logger.info(f\"Saved probabilities for batch {batch_idx+1} to {prob_file}\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    results_dir = \"../02_Results\"\n",
    "    zarr_file = \"zones_data_2.zarr\"\n",
    "    models_dir = os.path.join(results_dir, \"models\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    zone_batches = [[\"zone_1\", \"zone_3\", \"zone_4\", \"zone_6\"], [\"zone_7\", \"zone_8\", \"zone_10\"]]\n",
    "    \n",
    "    # Add spatial_shape to selected features to track the dimensions\n",
    "    selected_features = [\n",
    "        'col_idx', 'row_idx', 'impoundment_amplified', 'zone_id', 'skyview_gabor', \n",
    "        'impoundment_raw', 'conic_mean', 'hpmf_raw', 'skyview_raw', 'hpmf_f', 'slope_channels',\n",
    "        'spatial_shape'\n",
    "    ]\n",
    "        \n",
    "    logger.info(\"Loading trained models...\")\n",
    "    model_paths = [os.path.join(models_dir, f) for f in os.listdir(models_dir) if f.endswith(\".joblib\")]\n",
    "    rf_models = [joblib.load(model_path) for model_path in model_paths]\n",
    "    logger.info(f\"Loaded {len(rf_models)} models\")\n",
    "\n",
    "    logger.info(\"Generating probabilities for all batches...\")\n",
    "    prediction_results = []\n",
    "    for i, zones in enumerate(zone_batches):\n",
    "        prediction_success = predict_for_batch(i, zones, rf_models, zarr_file, selected_features, results_dir)\n",
    "        prediction_results.append(prediction_success)\n",
    "\n",
    "    if not any(prediction_results):\n",
    "        logger.error(\"Failed to generate probabilities for any batch. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Processing complete. Results saved.\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities to GeoTifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 13:14:05,129 - __main__ - INFO - Starting export from Zarr files\n",
      "2025-04-01 13:14:05,130 - __main__ - INFO - Processing batch 1\n",
      "2025-04-01 13:14:05,131 - __main__ - INFO - Processing batch 1 with zones: ['zone_1', 'zone_3', 'zone_4', 'zone_6']\n",
      "2025-04-01 13:14:05,135 - __main__ - INFO - Loaded probabilities array with shape (5000,)\n",
      "2025-04-01 13:14:05,136 - __main__ - ERROR - Not enough data for zone zone_1, skipping\n",
      "2025-04-01 13:14:05,136 - __main__ - ERROR - Not enough data for zone zone_3, skipping\n",
      "2025-04-01 13:14:05,136 - __main__ - ERROR - Not enough data for zone zone_4, skipping\n",
      "2025-04-01 13:14:05,137 - __main__ - ERROR - Not enough data for zone zone_6, skipping\n",
      "2025-04-01 13:14:05,137 - __main__ - INFO - Processing batch 2\n",
      "2025-04-01 13:14:05,137 - __main__ - INFO - Processing batch 2 with zones: ['zone_7', 'zone_8', 'zone_10']\n",
      "2025-04-01 13:14:05,139 - __main__ - INFO - Loaded probabilities array with shape (5000,)\n",
      "2025-04-01 13:14:05,140 - __main__ - ERROR - Not enough data for zone zone_7, skipping\n",
      "2025-04-01 13:14:05,140 - __main__ - ERROR - Not enough data for zone zone_8, skipping\n",
      "2025-04-01 13:14:05,140 - __main__ - ERROR - Not enough data for zone zone_10, skipping\n",
      "2025-04-01 13:14:05,140 - __main__ - INFO - All exports completed successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import zarr\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define zone coordinates\n",
    "zone_boundaries = {\n",
    "    \"zone_1\": {\"upper_left\": (377982, 6854660), \"lower_right\": (380482, 6852160)},\n",
    "    \"zone_2\": {\"upper_left\": (377982, 6857160), \"lower_right\": (380482, 6854660)},\n",
    "    \"zone_3\": {\"upper_left\": (380482, 6857160), \"lower_right\": (382982, 6854660)},\n",
    "    \"zone_4\": {\"upper_left\": (375482, 6859660), \"lower_right\": (377982, 6857160)},\n",
    "    \"zone_5\": {\"upper_left\": (377982, 6859660), \"lower_right\": (380482, 6857160)},\n",
    "    \"zone_6\": {\"upper_left\": (380482, 6859660), \"lower_right\": (382982, 6857160)},\n",
    "    \"zone_7\": {\"upper_left\": (375482, 6862159.999999999), \"lower_right\": (377982, 6859660)},\n",
    "    \"zone_8\": {\"upper_left\": (377982, 6862159.999999999), \"lower_right\": (380482, 6859660)},\n",
    "    \"zone_9\": {\"upper_left\": (380482, 6862159.999999999), \"lower_right\": (382982, 6859660)},\n",
    "    \"zone_10\": {\"upper_left\": (372982, 6864660), \"lower_right\": (375482, 6862159.999999999)},\n",
    "    \"zone_11\": {\"upper_left\": (375482, 6864660), \"lower_right\": (377982, 6862159.999999999)},\n",
    "    \"zone_12\": {\"upper_left\": (377982, 6864660), \"lower_right\": (380482, 6862159.999999999)},\n",
    "    \"zone_13\": {\"upper_left\": (370482, 6867160), \"lower_right\": (372982, 6864660)},\n",
    "    \"zone_14\": {\"upper_left\": (372982, 6867160), \"lower_right\": (375482, 6864660)},\n",
    "    \"zone_15\": {\"upper_left\": (375482, 6867160), \"lower_right\": (377982, 6864660)},\n",
    "    \"zone_16\": {\"upper_left\": (377982, 6867160), \"lower_right\": (380482, 6864660)},\n",
    "    \"zone_17\": {\"upper_left\": (370482, 6869660), \"lower_right\": (372982, 6867160)},\n",
    "    \"zone_18\": {\"upper_left\": (372982, 6869660), \"lower_right\": (375482, 6867160)},\n",
    "    \"zone_19\": {\"upper_left\": (375482, 6869660), \"lower_right\": (377982, 6867160)},\n",
    "    \"zone_20\": {\"upper_left\": (372982, 6872160), \"lower_right\": (375482, 6869660)},\n",
    "    \"zone_21\": {\"upper_left\": (375482, 6872160), \"lower_right\": (377982, 6869660)}\n",
    "}\n",
    "\n",
    "def export_zarr_to_geotiff_by_batch(zarr_file, output_path, batch_number):\n",
    "    \"\"\"\n",
    "    Export probabilities from a Zarr file to GeoTIFF format for zones in a specific batch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zarr_file : str\n",
    "        Path to the zarr file containing the probabilities results\n",
    "    output_path : str\n",
    "        Directory to save the GeoTIFF files\n",
    "    batch_number : int\n",
    "        The batch number (1 or 2) to process\n",
    "    \"\"\"\n",
    "    # Define the zones in each batch\n",
    "    zone_batches = [\n",
    "        [\"zone_1\", \"zone_3\", \"zone_4\", \"zone_6\"], \n",
    "        [\"zone_7\", \"zone_8\", \"zone_10\"]\n",
    "    ]\n",
    "    \n",
    "    # Select the zones for the requested batch\n",
    "    if batch_number < 1 or batch_number > len(zone_batches):\n",
    "        logger.error(f\"Invalid batch number: {batch_number}. Should be between 1 and {len(zone_batches)}\")\n",
    "        return False\n",
    "    \n",
    "    target_zones = zone_batches[batch_number - 1]\n",
    "    logger.info(f\"Processing batch {batch_number} with zones: {target_zones}\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Open Zarr file\n",
    "        root = zarr.open(zarr_file, mode=\"r\")\n",
    "        \n",
    "        # Check for probabilities array\n",
    "        if \"probabilities\" not in root:\n",
    "            logger.error(\"No 'probabilities' array found in Zarr file\")\n",
    "            return False\n",
    "            \n",
    "        probabilities = root[\"probabilities\"][:]\n",
    "        logger.info(f\"Loaded probabilities array with shape {probabilities.shape}\")\n",
    "        \n",
    "        # Standard grid size for all zones (5000x5000)\n",
    "        grid_width, grid_height = 5000, 5000\n",
    "        \n",
    "        # Calculate probabilities indices for each zone\n",
    "        # For simplicity, we'll assume probabilities are stored in same order as zones in the batch\n",
    "        total_pixels_per_zone = grid_width * grid_height\n",
    "        \n",
    "        for i, zone_name in enumerate(target_zones):\n",
    "            try:\n",
    "                # Get coordinates for this zone\n",
    "                if zone_name not in zone_boundaries:\n",
    "                    logger.error(f\"No coordinates found for {zone_name}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                boundaries = zone_boundaries[zone_name]\n",
    "                ul_x, ul_y = boundaries[\"upper_left\"]\n",
    "                lr_x, lr_y = boundaries[\"lower_right\"]\n",
    "                \n",
    "                # Calculate the index range for this zone\n",
    "                start_idx = i * total_pixels_per_zone\n",
    "                end_idx = start_idx + total_pixels_per_zone\n",
    "                \n",
    "                # Extract probabilities for this zone\n",
    "                if end_idx <= len(probabilities):\n",
    "                    # Reshape to 2D grid\n",
    "                    zone_probabilities = probabilities[start_idx:end_idx].reshape(grid_height, grid_width)\n",
    "                    logger.info(f\"Extracted probabilities for {zone_name} with shape {zone_probabilities.shape}\")\n",
    "                else:\n",
    "                    logger.error(f\"Not enough data for zone {zone_name}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Create transform\n",
    "                transform = from_bounds(ul_x, lr_y, lr_x, ul_y, grid_width, grid_height)\n",
    "                \n",
    "                # Save classification map\n",
    "                class_filename = f\"{output_path}/{zone_name}_classification.tif\"\n",
    "                meta = {\n",
    "                    'driver': 'GTiff',\n",
    "                    'height': grid_height,\n",
    "                    'width': grid_width,\n",
    "                    'count': 1,\n",
    "                    'dtype': str(zone_probabilities.dtype),\n",
    "                    'crs': 'EPSG:3067',  # Finnish ETRS-TM35FIN coordinate system\n",
    "                    'transform': transform,\n",
    "                    'nodata': 0\n",
    "                }\n",
    "                \n",
    "                with rasterio.open(class_filename, 'w', **meta) as dst:\n",
    "                    dst.write(zone_probabilities, 1)\n",
    "                \n",
    "                logger.info(f\"Saved classification map for {zone_name} to {class_filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing zone {zone_name}: {e}\")\n",
    "                import traceback\n",
    "                logger.error(traceback.format_exc())\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting from Zarr to GeoTIFF: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "    \n",
    "    return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your Zarr files\n",
    "    zarr_file_batch1 = \"probabilities_zones_batch_1.zarr\"\n",
    "    zarr_file_batch2 = \"probabilities_zones_batch_2.zarr\"\n",
    "    \n",
    "    # Path for output GeoTIFFs\n",
    "    output_path = \"../02_Results/geotiff_results\"\n",
    "    \n",
    "    logger.info(\"Starting export from Zarr files\")\n",
    "    \n",
    "    # Export zones from batch 1\n",
    "    logger.info(\"Processing batch 1\")\n",
    "    success_batch1 = export_zarr_to_geotiff_by_batch(zarr_file_batch1, output_path, 1)\n",
    "    \n",
    "    # Export zones from batch 2\n",
    "    logger.info(\"Processing batch 2\")\n",
    "    success_batch2 = export_zarr_to_geotiff_by_batch(zarr_file_batch2, output_path, 2)\n",
    "    \n",
    "    if success_batch1 and success_batch2:\n",
    "        logger.info(\"All exports completed successfully\")\n",
    "    else:\n",
    "        logger.warning(\"Some exports failed, check the logs for details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(<zarr.hierarchy.Group '/zone_1' read-only>)\n"
     ]
    }
   ],
   "source": [
    "zone = \"zone_1\"  # Adjust this to check specific zones\n",
    "zone_data = root[zone]\n",
    "print(zone_data.keys())  # Check which features are available for this zone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
