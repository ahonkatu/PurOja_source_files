{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pickles having the needed features\n",
    "This is first time I'm doing things like this. It was unclear, based on the original study, in what phase all the pickles were made?\n",
    "I'm making them in this file.\n",
    "\n",
    "Lot of debugging included..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_arr shape (after trim): (5000, 5000)\n",
      "Input shape: (5000, 5000)\n",
      "Result shape: (5000, 5000)\n",
      "arr shape: (5000, 5000)\n",
      "result shape: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Feature_Creation.ipynb\n",
    "# Imported libraries\n",
    "import datetime\n",
    "import dask.array as da\n",
    "import dask\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from skimage.filters import gabor\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.ndimage import generic_filter as gf\n",
    "from dask_image.ndfilters import generic_filter as d_gf\n",
    "import pickle\n",
    "# Import the required functions from feature_creation.py in Functions directory\n",
    "from Functions import feature_creation\n",
    "from Functions import feature_creation2\n",
    "from Functions import general_functions\n",
    "#from Functions import post_processing\n",
    "from Functions.general_functions import create_circular_mask\n",
    "import os\n",
    "\n",
    "from numba import config\n",
    "config.DEBUG = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Functions.feature_creation' from 'e:\\\\Gradu\\\\PurOja\\\\02_Analysis\\\\01_Scripts\\\\Functions\\\\feature_creation.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Functions.feature_creation2\n",
    "importlib.reload(Functions.feature_creation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to feature files\n",
    "features_path = \"../../01_Data/01_Raw/features/features\" \n",
    "output_path = \"../../01_Data/01_Raw/features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:asyncio:Using selector: EpollSelector\n",
      "/users/ahonkatu/.local/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 38873 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Dashboard: http://127.0.0.1:38873/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 02:57:09,207 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 03:43:52,833 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:37797'.\n",
      "2025-01-24 03:43:52,931 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:39387'.\n",
      "2025-01-24 03:43:56,202 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-01-24 03:43:56,203 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-01-24 06:03:47,194 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:34389'.\n",
      "2025-01-24 06:04:13,192 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:39997'.\n",
      "2025-01-24 06:04:13,193 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46007'.\n",
      "2025-01-24 06:04:13,198 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:42011'.\n",
      "2025-01-24 06:04:13,205 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f195145ec10>, <Task finished name='Task-13558159' coro=<BaseTCPListener._handle_stream() done, defined at /users/ahonkatu/.local/lib/python3.9/site-packages/distributed/comm/tcp.py:654> exception=ValueError('invalid operation on non-started TCPListener')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/ahonkatu/.local/lib/python3.9/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/users/ahonkatu/.local/lib/python3.9/site-packages/tornado/tcpserver.py\", line 387, in <lambda>\n",
      "    gen.convert_yielded(future), lambda f: f.result()\n",
      "  File \"/users/ahonkatu/.local/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 660, in _handle_stream\n",
      "    logger.debug(\"Incoming connection from %r to %r\", address, self.contact_address)\n",
      "  File \"/users/ahonkatu/.local/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 696, in contact_address\n",
      "    host, port = self.get_host_port()\n",
      "  File \"/users/ahonkatu/.local/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 677, in get_host_port\n",
      "    self._check_started()\n",
      "  File \"/users/ahonkatu/.local/lib/python3.9/site-packages/distributed/comm/tcp.py\", line 652, in _check_started\n",
      "    raise ValueError(\"invalid operation on non-started TCPListener\")\n",
      "ValueError: invalid operation on non-started TCPListener\n",
      "2025-01-24 06:04:13,688 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46485'.\n",
      "2025-01-24 06:04:17,205 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-01-24 06:04:19,192 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:38893'.\n",
      "2025-01-24 08:48:46,557 - tornado.application - ERROR - Uncaught exception GET /status/ws (127.0.0.1)\n",
      "HTTPServerRequest(protocol='http', host='r06c53.bullx:58633', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/ahonkatu/.local/lib/python3.9/site-packages/tornado/websocket.py\", line 938, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "  File \"/users/ahonkatu/.local/lib/python3.9/site-packages/tornado/web.py\", line 3301, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/users/ahonkatu/.local/lib/python3.9/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired.\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired.\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "\n",
    "# Set up a Dask cluster with adaptive scaling\n",
    "cluster = LocalCluster(\n",
    "    processes=True,                # Use separate processes for parallelism\n",
    "    n_workers=10,                  # Match with available CPU cores\n",
    "    threads_per_worker=1,          # One thread per worker\n",
    "    memory_limit=\"20GB\"            # Memory limit per worker\n",
    ")\n",
    "\n",
    "# Enable adaptive scaling\n",
    "cluster.adapt(minimum=1, maximum=15)  # Adjust workers dynamically (1 to 15 workers)\n",
    "\n",
    "# Connect the client to the cluster\n",
    "client = Client(cluster)\n",
    "\n",
    "# Configure memory management\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.7,  # Spill memory starts at 70% usage\n",
    "    'distributed.worker.memory.spill': 0.8,  # Spill to disk when 80% memory is used\n",
    "    'distributed.worker.memory.pause': 0.95, # Pause computation at 95% memory usage\n",
    "})\n",
    "\n",
    "# Check cluster dashboard link\n",
    "print(\"Dask Dashboard:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ahonkatu/.local/lib/python3.9/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 190.74 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files for zone 9:\n",
      "Labels shape: (5000, 5000)\n",
      "HPMF shape: (5000, 5000)\n",
      "Skyview shape: (5000, 5000)\n",
      "Impoundment shape: (5000, 5000)\n",
      "Slope shape: (5000, 5000)\n",
      "DEM shape: (5000, 5000)\n",
      "Processing DEM features... 2025-01-24 05:21:53.749686\n",
      "Added DEM ditches detected.\n",
      "Calculating conic_mean...\n",
      "Added conic_mean to the DataFrame.\n",
      "Calculating skyview_gabor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ahonkatu/.local/lib/python3.9/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 190.74 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "2025-01-24 05:46:59,592 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:46:59,600 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:46:59,608 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:46:59,620 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:46:59,631 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:46:59,691 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:47:30,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:47:30,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:47:30,206 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:47:30,208 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:47:30,209 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:47:30,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:48:44,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:48:44,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:48:44,204 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:48:44,206 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:48:44,543 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:49:20,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:49:20,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:49:20,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:49:20,562 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:49:38,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:49:38,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:49:38,534 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:52:23,200 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:52:23,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:52:23,433 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:52:54,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:52:54,204 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:52:54,616 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:53:40,200 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:53:40,766 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:54:52,200 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:54:53,696 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 05:57:10,222 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added skyview_gabor to the DataFrame.\n",
      "Calculating skyview ditch...\n",
      "Added skyview_ditch to the DataFrame.\n",
      "starting-impoundment:     2025-01-24 05:57:18.681553\n",
      "Processing Impoundment features...\n",
      "Calculating impoundment_amplified...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ahonkatu/.local/lib/python3.9/site-packages/dask/base.py:1487: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'impoundment_amplified' to the DataFrame.\n",
      "starting-hpmf manual:     2025-01-24 06:01:13.408433\n",
      "Calculating hpmf_f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ahonkatu/.local/lib/python3.9/site-packages/dask/base.py:1487: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'hpmf_f' to the DataFrame.\n",
      "starting-slope manual:     2025-01-24 06:03:38.257885\n",
      "Calculating slope_non_ditch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/ahonkatu/.local/lib/python3.9/site-packages/distributed/client.py:3362: UserWarning: Sending large graph of size 95.38 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "2025-01-24 06:03:47,195 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-01-24 06:03:47,200 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 06:03:47,215 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 06:04:06,309 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 06:04:12,218 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 06:04:12,220 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'slope_non_ditch' to the DataFrame.\n",
      "Feature generation completed for zone 9.\n",
      "          label_3m  hpmf_raw  skyview_raw  impoundment_raw  slope_raw  \\\n",
      "0            False  0.026563     0.999275         0.007577   0.515802   \n",
      "1            False  0.012188     0.999142         0.008359   0.515802   \n",
      "2            False  0.003125     0.999083         0.012735   6.455696   \n",
      "3            False -0.000625     0.999103         0.020704   6.557220   \n",
      "4            False -0.004375     0.999174         0.028673   1.718288   \n",
      "...            ...       ...          ...              ...        ...   \n",
      "24999995     False -0.006250     0.996247         0.011841  11.965354   \n",
      "24999996     False -0.003750     0.996172         0.008904  11.639751   \n",
      "24999997     False -0.001250     0.995967         0.005966  11.639751   \n",
      "24999998     False  0.019844     0.996002         0.003935  13.925039   \n",
      "24999999     False  0.059531     0.996535         0.002811  13.047624   \n",
      "\n",
      "               DEM_raw  DEM_ditch_detection  DEM_ditch_detection_streams  \\\n",
      "0           127.527000             0.000000                     0.000000   \n",
      "1           127.527000             0.000000                     0.000000   \n",
      "2           127.527000             0.000000                     0.000000   \n",
      "3           127.419998             0.107002                     0.128403   \n",
      "4           127.419998             0.107002                     0.128403   \n",
      "...                ...                  ...                          ...   \n",
      "24999995 -99999.000000             0.000000                     0.000000   \n",
      "24999996 -99999.000000             0.000000                     0.000000   \n",
      "24999997 -99999.000000             0.000000                     0.000000   \n",
      "24999998 -99999.000000             0.000000                     0.000000   \n",
      "24999999 -99999.000000             0.000000                     0.000000   \n",
      "\n",
      "          conic_mean  skyview_gabor  skyview_ditch  impoundment_amplified  \\\n",
      "0           0.999275       0.067926       1.000000             430.975555   \n",
      "1           0.999142       0.067906       1.000000             430.975555   \n",
      "2           0.999083       0.067869       1.000000             433.525726   \n",
      "3           0.999103       0.067824       1.000000             434.112061   \n",
      "4           0.999174       0.067780       1.000000             435.340149   \n",
      "...              ...            ...            ...                    ...   \n",
      "24999995    0.996247       0.065191       0.996247             113.645897   \n",
      "24999996    0.996172       0.065120       0.996172             113.535942   \n",
      "24999997    0.995967       0.065092       0.995967             113.374863   \n",
      "24999998    0.996002       0.065087       0.996002             113.374863   \n",
      "24999999    0.996535       0.065089       0.996535             113.175972   \n",
      "\n",
      "            hpmf_f  slope_non_ditch  \n",
      "0         0.335113              0.0  \n",
      "1         0.335113              0.0  \n",
      "2         0.335113              0.0  \n",
      "3         0.335316              0.0  \n",
      "4         0.335316              0.0  \n",
      "...            ...              ...  \n",
      "24999995  0.337393             25.0  \n",
      "24999996  0.337025             25.0  \n",
      "24999997  0.336531             25.0  \n",
      "24999998  0.336515             20.0  \n",
      "24999999  0.336515             20.0  \n",
      "\n",
      "[25000000 rows x 14 columns]\n",
      "Shape of skyview_raw: (25000000,)\n",
      "Number of dimensions of skyview_raw: 1\n",
      "skyview_raw is 1D\n",
      "Starting feature computation:  2025-01-24 06:04:12.719670\n",
      "Generating feature: skyview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 06:04:13,193 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-01-24 06:04:13,194 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-01-24 06:04:13,198 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-01-24 06:04:13,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n",
      "2025-01-24 06:04:13,204 - distributed.core - INFO - Connection to tcp://127.0.0.1:34349 has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added column: skyview\n",
      "Generating feature: impoundment\n",
      "Added column: impoundment\n",
      "Generating feature: hpmf\n",
      "Added column: hpmf\n",
      "Generating feature: slope\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 06:04:13,689 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added column: slope\n",
      "Generating feature: skyview_max_2\n",
      "Added column: skyview_max_2\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_max_2 with radius 2\n",
      "Generating feature: skyview_max_4\n",
      "Added column: skyview_max_4\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_max_4 with radius 4\n",
      "Generating feature: skyview_max_6\n",
      "Added column: skyview_max_6\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_max_6 with radius 6\n",
      "Generating feature: skyview_min_6\n",
      "Added column: skyview_min_6\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_min_6 with radius 6\n",
      "Generating feature: skyview_median_2\n",
      "Added column: skyview_median_2\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_median_2 with radius 2\n",
      "Generating feature: skyview_median_4\n",
      "Added column: skyview_median_4\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_median_4 with radius 4\n",
      "Generating feature: skyview_median_6\n",
      "Added column: skyview_median_6\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_median_6 with radius 6\n",
      "Generating feature: skyview_std_6\n",
      "Added column: skyview_std_6\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_std_6 with radius 6\n",
      "Generating feature: skyview_skew_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 06:04:19,193 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added column: skyview_skew_2\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_skew_2 with radius 2\n",
      "Generating feature: impoundment_mean_3\n",
      "Added column: impoundment_mean_3\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_mean_3 with radius 3\n",
      "Generating feature: impoundment_mean_2\n",
      "Added column: impoundment_mean_2\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_mean_2 with radius 2\n",
      "Generating feature: impoundment_mean_6\n",
      "Added column: impoundment_mean_6\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_mean_6 with radius 6\n",
      "Generating feature: impoundment_median_2\n",
      "Added column: impoundment_median_2\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_median_2 with radius 2\n",
      "Generating feature: impoundment_median_4\n",
      "Added column: impoundment_median_4\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_median_4 with radius 4\n",
      "Generating feature: impoundment_median_6\n",
      "Added column: impoundment_median_6\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_median_6 with radius 6\n",
      "Generating feature: impoundment_max_6\n",
      "Added column: impoundment_max_6\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_max_6 with radius 6\n",
      "Generating feature: impoundment_std_4\n",
      "Added column: impoundment_std_4\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_std_4 with radius 4\n",
      "Generating feature: impoundment_std_6\n",
      "Added column: impoundment_std_6\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_std_6 with radius 6\n",
      "Generating feature: impoundment_skew_2\n",
      "Added column: impoundment_skew_2\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_skew_2 with radius 2\n",
      "Generating feature: hpmf_mean_2\n",
      "Added column: hpmf_mean_2\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_mean_2 with radius 2\n",
      "Generating feature: hpmf_mean_3\n",
      "Added column: hpmf_mean_3\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_mean_3 with radius 3\n",
      "Generating feature: hpmf_mean_4\n",
      "Added column: hpmf_mean_4\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_mean_4 with radius 4\n",
      "Generating feature: hpmf_mean_6\n",
      "Added column: hpmf_mean_6\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_mean_6 with radius 6\n",
      "Generating feature: hpmf_min_4\n",
      "Added column: hpmf_min_4\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_min_4 with radius 4\n",
      "Generating feature: hpmf_min_3\n",
      "Added column: hpmf_min_3\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_min_3 with radius 3\n",
      "Generating feature: hpmf_median_4\n",
      "Added column: hpmf_median_4\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_median_4 with radius 4\n",
      "Generating feature: hpmf_std_6\n",
      "Added column: hpmf_std_6\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_std_6 with radius 6\n",
      "Generating feature: hpmf_skew_2\n",
      "Added column: hpmf_skew_2\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_skew_2 with radius 2\n",
      "Generating feature: slope_median_6\n",
      "Added column: slope_median_6\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_median_6 with radius 6\n",
      "Generating feature: slope_min_2\n",
      "Added column: slope_min_2\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_min_2 with radius 2\n",
      "Generating feature: slope_min_4\n",
      "Added column: slope_min_4\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_min_4 with radius 4\n",
      "Generating feature: slope_min_6\n",
      "Added column: slope_min_6\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_min_6 with radius 6\n",
      "Generating feature: slope_std_4\n",
      "Added column: slope_std_4\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_std_4 with radius 4\n",
      "Generating feature: slope_std_6\n",
      "Added column: slope_std_6\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_std_6 with radius 6\n",
      "Generating feature: slope_skew_2\n",
      "Added column: slope_skew_2\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_skew_2 with radius 2\n",
      "Feature computation completed:  2025-01-24 06:04:33.065423\n",
      "Generated columns in data_frame: Index(['label_3m', 'hpmf_raw', 'skyview_raw', 'impoundment_raw', 'slope_raw',\n",
      "       'DEM_raw', 'DEM_ditch_detection', 'DEM_ditch_detection_streams',\n",
      "       'conic_mean', 'skyview_gabor', 'skyview_ditch', 'impoundment_amplified',\n",
      "       'hpmf_f', 'slope_non_ditch', 'skyview', 'impoundment', 'hpmf', 'slope',\n",
      "       'skyview_max_2', 'skyview_max_4', 'skyview_max_6', 'skyview_min_6',\n",
      "       'skyview_median_2', 'skyview_median_4', 'skyview_median_6',\n",
      "       'skyview_std_6', 'skyview_skew_2', 'impoundment_mean_3',\n",
      "       'impoundment_mean_2', 'impoundment_mean_6', 'impoundment_median_2',\n",
      "       'impoundment_median_4', 'impoundment_median_6', 'impoundment_max_6',\n",
      "       'impoundment_std_4', 'impoundment_std_6', 'impoundment_skew_2',\n",
      "       'hpmf_mean_2', 'hpmf_mean_3', 'hpmf_mean_4', 'hpmf_mean_6',\n",
      "       'hpmf_min_4', 'hpmf_min_3', 'hpmf_median_4', 'hpmf_std_6',\n",
      "       'hpmf_skew_2', 'slope_median_6', 'slope_min_2', 'slope_min_4',\n",
      "       'slope_min_6', 'slope_std_4', 'slope_std_6', 'slope_skew_2'],\n",
      "      dtype='object')\n",
      "Warning: The following columns are missing and will be excluded: {'hpmf_f, slope_non_ditch', 'hpmf_min_2', 'skyview_non_ditch', 'hpmf_filter', 'slope_mean_6', 'impoundment_mean_4', 'hpmf_gabor'}\n",
      "Zone 9 features saved to pickle.\n"
     ]
    }
   ],
   "source": [
    "def d_gf(array, func, footprint):\n",
    "        \"\"\"\n",
    "        Wrapper function to safely apply a mask or function with Dask arrays.\n",
    "        Handles edge cases for padding and ensures indices are valid.\n",
    "        \"\"\"\n",
    "        depth = footprint.shape[0] // 2\n",
    "        depth = max(0, depth)  # Ensure depth is non-negative\n",
    "\n",
    "        # Apply the mask with proper overlap and safe boundary handling\n",
    "        return da.map_overlap(\n",
    "        func, array, depth=depth, boundary=\"reflect\", trim=True\n",
    "    )\n",
    "        # Helper function to add feature columns conditionally\n",
    "\n",
    "\n",
    "def load_raw(zone_number, chunk_size=(1666, 1666)):\n",
    "        global features_path, output_path\n",
    "        data_frame = None  # Ensure data_frame is explicitly initialized\n",
    "\n",
    "        try:\n",
    "            # Load raw feature files\n",
    "            labels = np.load(f\"{features_path}/Label3m_{zone_number}.npy\")\n",
    "            hpmf = np.load(f\"{features_path}/HPMF_{zone_number}.npy\")\n",
    "            skyview = np.load(f\"{features_path}/SVF_{zone_number}.npy\").astype(np.float64)\n",
    "            impoundment = np.load(f\"{features_path}/Impoundment_{zone_number}.npy\")\n",
    "            slope = np.load(f\"{features_path}/slope_{zone_number}.npy\")\n",
    "            DEM = np.load(f\"{features_path}/DEM_{zone_number}.npy\")\n",
    "            # Replace -99999 with np.nan in DEM using Dask\n",
    "            dask_dem = da.from_array(DEM, chunks=chunk_size)\n",
    "            dask_dem = da.where(dask_dem == -99999, np.nan, dask_dem)\n",
    "\n",
    "            # Trigger computation and persist changes\n",
    "            dask_dem = dask_dem.persist()  # Persist in memory for future use\n",
    "\n",
    "            print(f\"Loaded files for zone {zone_number}:\")\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "            print(f\"HPMF shape: {hpmf.shape}\")\n",
    "            print(f\"Skyview shape: {skyview.shape}\")\n",
    "            print(f\"Impoundment shape: {impoundment.shape}\")\n",
    "            print(f\"Slope shape: {slope.shape}\")\n",
    "            print(f\"DEM shape: {DEM.shape}\")\n",
    "\n",
    "            # Assertions to validate inputs\n",
    "            assert labels is not None, \"Error: 'labels' is not defined or is None.\"\n",
    "            assert hpmf is not None, \"Error: 'hpmf' is not defined or is None.\"\n",
    "            assert skyview is not None, \"Error: 'skyview' is not defined or is None.\"\n",
    "\n",
    "            # Feature creation\n",
    "            stream_amp = feature_creation2.stream_amplification(impoundment)\n",
    "\n",
    "            data_frame = pd.DataFrame({\n",
    "                \"label_3m\": labels.reshape(-1),\n",
    "                \"hpmf_raw\": hpmf.reshape(-1),\n",
    "                \"skyview_raw\": skyview.reshape(-1),\n",
    "                \"impoundment_raw\": impoundment.reshape(-1),\n",
    "                \"slope_raw\": slope.reshape(-1),\n",
    "            })\n",
    "\n",
    "            # Add additional features (e.g., DEM)\n",
    "            if \"DEM_raw\" not in data_frame.columns:\n",
    "                data_frame[\"DEM_raw\"] = DEM.flatten()\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"FileNotFoundError: {e}\")\n",
    "            return None\n",
    "        except AssertionError as e:\n",
    "            print(f\"AssertionError: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"General Error: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Final fallback check\n",
    "        if data_frame is None:\n",
    "            raise RuntimeError(\"Failed to create or load 'data_frame'. Check input data and processing steps.\")\n",
    "\n",
    "\n",
    "    #------------- MANUAL ----------------\n",
    "    #------------- Dem -------------------\n",
    "    # Add DEM feature if not already in DataFrame\n",
    "        def add_features(data_frame, DEM, skyview, impoundment, hpmf, slope, zone_number):\n",
    "                if \"DEM_raw\" not in data_frame.columns:  \n",
    "                    data_frame[\"DEM_raw\"] = dask_dem.compute().flatten() # -99999 values to Nan\n",
    "\n",
    "                    print(\"Defining tasks...\")\n",
    "\n",
    "        #------------- Helper function adds features to the data_frame -------------------\n",
    "\n",
    "        # Add features derived from DEM\n",
    "        print(\"Processing DEM features...\", datetime.datetime.now())\n",
    "        # Ensure the shape of the DEM array matches expected dimensions\n",
    "        dem_shape = dask_dem.shape\n",
    "        assert dem_shape == (5000, 5000), f\"Unexpected DEM shape: {dem_shape}\"\n",
    "        if \"DEM_ditch_detection\" not in data_frame.columns:\n",
    "            data_frame[\"DEM_ditch_detection\"] = feature_creation2.dem_ditch_detection(dask_dem.compute()).reshape(-1) # -99999 to Nan\n",
    "\n",
    "        # I'm interested in all the streams\n",
    "        if \"DEM_ditch_detection_streams\" not in data_frame.columns:\n",
    "            data_frame[\"DEM_ditch_detection_streams\"] = feature_creation2.impoundment_DEM_stream_strenghten(data_frame[\"DEM_ditch_detection\"].values.reshape((5000,5000)), stream_amp).reshape(-1)\n",
    "            print(\"Added DEM ditches detected.\")\n",
    "\n",
    "                    # I'm interested in all the streams\n",
    "                    #------------- Sky view factor -------------------\n",
    "                    # Add features derived from Skyview\n",
    "            #conic means from sky view factor\n",
    "        if \"conic_mean\" not in data_frame.columns:\n",
    "            print (\"Calculating conic_mean...\")\n",
    "            count_conic_mean = feature_creation2.conic_mean(skyview, 6, 0.9894350171089172)\n",
    "            data_frame[\"conic_mean\"] = count_conic_mean.flatten()\n",
    "            print(\"Added conic_mean to the DataFrame.\")\n",
    "        #skyview gabor\n",
    "        if \"skyview_gabor\" not in data_frame:\n",
    "            print (\"Calculating skyview_gabor...\")\n",
    "            data_frame[\"skyview_gabor\"] = feature_creation2.sky_view_gabor(skyview).reshape(-1)\n",
    "            print(\"Added skyview_gabor to the DataFrame.\")\n",
    "        \n",
    "        #if \"skyview_ditch\" not in data_frame.columns:\n",
    "        #    print (\"Calculating skyview ditch...\")\n",
    "        #    data_frame[\"skyview_ditch\"] = feature_creation2._reclassify_sky_view_non_ditch_amp(skyview).reshape(-1)\n",
    "        #    print(\"Added skyview_ditch to the DataFrame.\")\n",
    "\n",
    "        print(\"starting-impoundment:    \", datetime.datetime.now())\n",
    "\n",
    "        print(\"Processing Impoundment features...\")\n",
    "        if \"impoundment_amplified\" not in data_frame.columns:\n",
    "            print(\"Calculating impoundment_amplified...\")\n",
    "            data_frame[\"impoundment_amplified\"] = feature_creation2.impoundment_amplification(impoundment).reshape(-1)\n",
    "            print(\"Added 'impoundment_amplified' to the DataFrame.\")\n",
    "\n",
    "        print(\"starting-hpmf manual:    \", datetime.datetime.now())\n",
    "\n",
    "        if \"hpmf_f\" not in data_frame.columns:\n",
    "            print(\"Calculating hpmf_f...\")\n",
    "            count_hpmf_f = feature_creation2.process_hpmf(hpmf, 5)\n",
    "            data_frame[\"hpmf_f\"] = count_hpmf_f.flatten()\n",
    "            print(\"Added 'hpmf_f' to the DataFrame.\")\n",
    "\n",
    "        print(\"starting-slope manual:    \", datetime.datetime.now())\n",
    "        if \"slope_channels\" not in data_frame:\n",
    "            print(\"Calculating slope_channels...\")\n",
    "            slope_processed_dask = feature_creation2.process_slope_for_channels(slope, chunk_size=chunk_size)\n",
    "            data_frame[\"slope_channels\"] = slope_processed_dask.compute().reshape(-1) \n",
    "            print(\"Added 'slope_channels' to the DataFrame.\")\n",
    "            \n",
    "            print(f\"Feature generation completed for zone {zone_number}.\")\n",
    "                    \n",
    "        # Return the final DataFrame once features are added\n",
    "            #Final fallback check for undefined data_frame\n",
    "            if 'data_frame' not in locals():\n",
    "                raise RuntimeError(\"Failed to create or load 'data_frame'. Please check input data and pickle files.\")\n",
    "     \n",
    "            return data_frame\n",
    "#------------- AUTOMATED --------------------------\n",
    "#------------- Process dataframe-------------------\n",
    "def process_dataframe(data_frame, skyview, impoundment, hpmf, slope, chunk_size):\n",
    "    print(\"Starting feature computation: \", datetime.datetime.now())\n",
    "\n",
    "    def add_feature(column_name_base, compute_func, array, radii, *args):\n",
    "        \"\"\"\n",
    "        Add a feature to the data frame, ensuring consistent naming.\n",
    "        \"\"\"\n",
    "        for radius in radii:\n",
    "            column_name = f\"{column_name_base}_{radius}\"  # Construct consistent column names\n",
    "            if column_name not in data_frame.columns:\n",
    "                print(f\"Generating feature: {column_name}\")\n",
    "                data_frame[column_name] = compute_func(array, *args).flatten()\n",
    "\n",
    "        print(f\"Feature generation completed for zone {zone_number}.\")\n",
    "        print(f\"Shape of feature array before flattening: {array.shape}\")\n",
    "        print(f\"Shape after computation: {compute_func(array, *args).shape}\")\n",
    "\n",
    "    # Handling NaN values for np.amax (replace NaNs with -infinity)\n",
    "    skyview_no_nan = np.nan_to_num(skyview, nan=-np.inf)\n",
    "\n",
    "    # List of features to generate\n",
    "    features = [\n",
    "        (\"skyview\", skyview, [2, 4, 6], np.nanmax, \"max\"),\n",
    "        (\"skyview\", skyview, [6], np.nanmin, \"min\"),\n",
    "        (\"skyview\", skyview, [2, 4, 6], np.nanmedian, \"median\"),\n",
    "        (\"skyview\", skyview, [6], np.nanstd, \"std\"),\n",
    "        (\"skyview\", skyview, [2], skew, \"skew\"),\n",
    "        (\"impoundment\", impoundment, [3, 2, 6], np.nanmean, \"mean\"),\n",
    "        (\"impoundment\", impoundment, [2, 4, 6], np.nanmedian, \"median\"),\n",
    "        (\"impoundment\", impoundment, [6], np.amax, \"max\"),\n",
    "        (\"impoundment\", impoundment, [4, 6], np.nanstd, \"std\"),\n",
    "        (\"impoundment\", impoundment, [2], skew, \"skew\"),\n",
    "        (\"hpmf\", hpmf, [2, 3, 4, 6], np.nanmean, \"mean\"),\n",
    "        (\"hpmf\", hpmf, [3, 4], np.amin, \"min\"),\n",
    "        (\"hpmf\", hpmf, [4], np.nanmedian, \"median\"),\n",
    "        (\"hpmf\", hpmf, [6], np.nanstd, \"std\"),\n",
    "        (\"hpmf\", hpmf, [2], skew, \"skew\"),\n",
    "        (\"slope\", slope, [6], np.nanmedian, \"median\"),\n",
    "        (\"slope\", slope, [2, 4, 6], np.amin, \"min\"),\n",
    "        (\"slope\", slope, [4, 6], np.nanstd, \"std\"),\n",
    "        (\"slope\", slope, [2], skew, \"skew\"),\n",
    "    ]\n",
    "\n",
    "    # Loop through features to calculate them\n",
    "    for col_name, array, feature_sizes, func, func_name in features:\n",
    "        # Check if the column already exists\n",
    "        if col_name not in data_frame.columns:\n",
    "            print(f\"Generating feature: {col_name}\")\n",
    "\n",
    "            # Apply the function to generate the feature\n",
    "            result = func(array)\n",
    "\n",
    "            # Check if result is a scalar and handle accordingly\n",
    "            if np.isscalar(result):  # If it's a scalar\n",
    "                data_frame[col_name] = np.full(len(data_frame), result)  # Fill the column with the scalar value\n",
    "            else:\n",
    "                # Ensure the result is a NumPy array and flatten it\n",
    "                if not isinstance(result, np.ndarray):\n",
    "                    result = np.array(result)  # Convert to numpy array if necessary\n",
    "                data_frame[col_name] = result.flatten()  # Flatten the array and assign to DataFrame\n",
    "\n",
    "            print(f\"Added column: {col_name}\")\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "    # Feature computation for regular features\n",
    "    for col_name, array, feature_sizes, func, func_name in features:\n",
    "        if col_name not in data_frame.columns:\n",
    "            print(f\"Generating feature: {col_name}\")\n",
    "            \n",
    "            # If the feature requires a circular mask and chunked computation\n",
    "            if func in [np.amax, np.nanmean, np.nanstd, np.nanmedian, skew]:  # Add the relevant functions\n",
    "                for radius in feature_sizes:\n",
    "                    col_name_with_radius = f\"{col_name}_{radius}\"\n",
    "                    if col_name_with_radius not in data_frame.columns:\n",
    "                        print(f\"Generating feature: {col_name_with_radius}\")\n",
    "                        result = d_gf(da.from_array(array, chunks=chunk_size), func, footprint=general_functions.create_circular_mask(radius)).compute(scheduler='processes').reshape(-1)\n",
    "                        data_frame[col_name_with_radius] = result.flatten()\n",
    "                        print(f\"Added column: {col_name_with_radius}\")\n",
    "            else:\n",
    "                result = func(array)\n",
    "                if np.isscalar(result):\n",
    "                    result = np.full(len(data_frame), result)\n",
    "                data_frame[col_name] = result.flatten()\n",
    "                print(f\"Added column: {col_name}\")\n",
    "\n",
    "    # Handling scalar features with radius\n",
    "    for prefix, array, radii, func, suffix in features:\n",
    "        for radius in radii:\n",
    "            col_name = f\"{prefix}_{suffix}_{radius}\"\n",
    "            if col_name not in data_frame.columns:\n",
    "                print(f\"Generating feature: {col_name}\")\n",
    "\n",
    "                # Compute the feature (mean, std, etc.)\n",
    "                feature = func(array)\n",
    "\n",
    "                # If the result is a scalar, broadcast it to match the DataFrame length\n",
    "                if np.isscalar(feature):\n",
    "                    feature = np.full(len(data_frame), feature)\n",
    "\n",
    "                # Add the feature to the DataFrame\n",
    "                data_frame[col_name] = feature.flatten()\n",
    "                print(f\"Added column: {col_name}\")\n",
    "\n",
    "                # Apply circular mask (if needed)\n",
    "                mask = general_functions.create_circular_mask(radius)\n",
    "                array_2d = array.reshape((5000, 5000))  # Assuming 5000x5000 shape (adjust accordingly)\n",
    "                array_dask = da.from_array(array_2d, chunks=chunk_size)\n",
    "\n",
    "                # Define the function to apply mask to each block\n",
    "                def apply_mask_to_block(block, mask):\n",
    "                    \"\"\"\n",
    "                    Applies a mask to the given block.\n",
    "                    Pads the mask if necessary to match the block size.\n",
    "                    \"\"\"\n",
    "                    block_height, block_width = block.shape\n",
    "                    mask_height, mask_width = mask.shape\n",
    "\n",
    "                    # Pad the mask to fit the block dimensions\n",
    "                    padded_mask = np.pad(\n",
    "                        mask, \n",
    "                        ((0, max(0, block_height - mask_height)), (0, max(0, block_width - mask_width))),\n",
    "                        mode='constant', \n",
    "                        constant_values=0\n",
    "                    )\n",
    "                    return block * padded_mask[:block_height, :block_width]\n",
    "\n",
    "                # Apply the masking function to each block\n",
    "\n",
    "                    masked_array = array_dask.map_blocks(\n",
    "                        apply_mask_to_block,\n",
    "                        mask=mask,  # Pass the precomputed mask as a keyword argument\n",
    "                        dtype=np.float32  # Explicitly specify the dtype\n",
    "                    ).compute()\n",
    "\n",
    "                print(\"Applied mask to the array.\")\n",
    "                print(f\"Applied mask for {col_name} with radius {radius}\")\n",
    "\n",
    "\n",
    "    print(\"Feature computation completed: \", datetime.datetime.now())\n",
    "    return data_frame\n",
    "\n",
    "#------------- Ordering dataframe-------------------\n",
    "def set_order(data_frame):\n",
    "    \"\"\"\n",
    "    Reorder columns for final DataFrame, checking for missing columns.\n",
    "    \"\"\"\n",
    "    ordered_columns = ordered_columns = [\n",
    "        'label_3m', \n",
    "\t\t'hpmf_raw', \n",
    "\t\t'skyview_raw', \n",
    "\t\t'impoundment_raw', \n",
    "\t\t'slope_raw', \n",
    "\t\t'DEM_ditch_detection', \n",
    "\t\t'DEM_ditch_detection_streams',\n",
    "\t\t'conic_mean',\n",
    "\t\t'skyview_gabor', \n",
    "        #'skyview_ditch', \n",
    "\t\t#'skyview_non_ditch', \n",
    "\t\t'skyview_min_6', \n",
    "\t\t'skyview_median_2', \n",
    "\t\t'skyview_median_6', \n",
    "\t\t'skyview_max_2', \n",
    "\t\t'skyview_max_4', \n",
    "\t\t'skyview_max_6',\n",
    "\t\t'skyview_std_6', \n",
    "\t\t'impoundment_amplified', \n",
    "\t\t'impoundment_mean_2', \n",
    "        'impoundment_mean_3', \n",
    "\t\t'impoundment_mean_4',\n",
    "\t\t'impoundment_mean_6',\n",
    "\t\t'impoundment_median_2', \t\t\n",
    "\t\t'impoundment_median_4', \n",
    " \t\t'impoundment_median_6', \n",
    "\t\t'impoundment_max_6', \n",
    "        'impoundment_std_4', \n",
    "\t\t'impoundment_std_6'\n",
    "\t\t'hpmf_f', \n",
    "        'hpmf_filter',\n",
    "\t\t'hpmf_gabor', \t\t\n",
    "\t\t'hpmf_min_2', \n",
    "        'hpmf_min_4',\n",
    "\t\t'hpmf_mean_3', \t\t\n",
    "\t\t'hpmf_mean_6', \n",
    "\t\t'hpmf_mean_4',\n",
    "        'hpmf_median_4', \n",
    "\t\t'hpmf_std_6', \n",
    "\t\t'slope_channels',\n",
    "        'slope_min_2', \t\n",
    "\t\t'slope_min_4', \t\t\n",
    "\t\t'slope_min_6',\n",
    "\t\t'slope_median_6',\n",
    "\t\t'slope_mean_6',\n",
    "\t\t'slope_std_4', \n",
    "\t\t'slope_std_6', \n",
    "    ]\n",
    "\n",
    "    # Check the columns of the data frame before reordering\n",
    "    print(\"Generated columns in data_frame:\", data_frame.columns)\n",
    "\n",
    "    # Check if all ordered columns exist in the DataFrame\n",
    "    existing_columns = [col for col in ordered_columns if col in data_frame.columns]\n",
    "\n",
    "    # Log any missing columns\n",
    "    missing_columns = set(ordered_columns) - set(existing_columns)\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: The following columns are missing and will be excluded: {missing_columns}\")\n",
    "\n",
    "    # If there are missing columns, we will return the data frame with the available columns only\n",
    "    ordered_columns_with_existing = [col for col in ordered_columns if col in existing_columns]\n",
    "\n",
    "    # Return the DataFrame with only the available columns in the desired order\n",
    "    return data_frame[ordered_columns_with_existing]\n",
    "\n",
    "# Define the range of zone numbers\n",
    "zone_numbers = range(9, 10)  # set the zone numbers for range in here\n",
    "chunk_size = (1666, 1666)\n",
    "\n",
    "# Loop through the zone numbers\n",
    "for zone_number in zone_numbers:\n",
    "    # Define the path to the pickle file\n",
    "    pickle_file_path = f\"{output_path}/zone_{zone_number}.pickle\"\n",
    "    \n",
    "    # Delete the existing pickle file if it exists (to ensure a fresh start)\n",
    "    if os.path.exists(pickle_file_path):\n",
    "        try:\n",
    "            os.remove(pickle_file_path)\n",
    "            print(f\"Deleted corrupted or incomplete pickle file for zone {zone_number}. Starting fresh.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting file for zone {zone_number}: {e}\")\n",
    "\n",
    "    # Check zone_with_features before using it\n",
    "    zone_with_features = load_raw(zone_number, chunk_size)\n",
    "    print(zone_with_features)  # This should print the DataFrame or an informative message\n",
    "\n",
    "    if zone_with_features is None:\n",
    "        raise ValueError(\"Failed to load raw data; 'zone_with_features' is None\")\n",
    "\n",
    "    # Check the shape and dimensions of the skyview data\n",
    "    skyview_data = zone_with_features[\"skyview_raw\"].values\n",
    "\n",
    "    print(\"Shape of skyview_raw:\", skyview_data.shape)  # Prints the shape of the data\n",
    "    print(\"Number of dimensions of skyview_raw:\", skyview_data.ndim)  # Prints the number of dimensions (1D, 2D, or 3D)\n",
    "\n",
    "    # Conditional check for dimensionality\n",
    "    if skyview_data.ndim == 1:\n",
    "        print(\"skyview_raw is 1D\")\n",
    "    elif skyview_data.ndim == 2:\n",
    "        print(\"skyview_raw is 2D\")\n",
    "    elif skyview_data.ndim == 3:\n",
    "        print(\"skyview_raw is 3D\")\n",
    "    else:\n",
    "        print(\"skyview_raw has more than 3 dimensions\")\n",
    "\n",
    "    # Now you can process the dataframe\n",
    "    zone_with_features = process_dataframe(\n",
    "        zone_with_features,\n",
    "        skyview=zone_with_features[\"skyview_raw\"].values,\n",
    "        impoundment=zone_with_features[\"impoundment_raw\"].values,\n",
    "        hpmf=zone_with_features[\"hpmf_raw\"].values,\n",
    "        slope=zone_with_features[\"slope_raw\"].values,\n",
    "        chunk_size=chunk_size\n",
    "    )\n",
    "\n",
    "    # Set the order for the features\n",
    "    zone_with_features = set_order(zone_with_features)\n",
    "\n",
    "# Call the function to process the zones and get the DataFrame\n",
    "data_frame = zone_with_features\n",
    "\n",
    "# Save the processed features to a new pickle file\n",
    "pickle.dump(zone_with_features, open(pickle_file_path, \"wb\"))\n",
    "\n",
    "# Create a list of futures (delayed tasks)\n",
    "\n",
    "print(f\"Zone {zone_number} features saved to pickle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "# Create a cluster\n",
    "cluster = LocalCluster()\n",
    "\n",
    "# Connect to the Dask cluster (if not already connected)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Close the client and cluster\n",
    "client.close()  # Closes the client\n",
    "cluster.shutdown()  # Shuts down the cluster completely\n",
    "\n",
    "# Optionally check if all workers are stopped\n",
    "print(\"Cluster workers:\", cluster.scheduler_info()['workers'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the test pickle\n",
    "StreamAmp result viewing. Should the treshold be tested?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in the pickle file: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "pickle_file = '../../01_Data/01_Raw/features/zone_1.pickle'  # Path to the pickle file\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Print the keys of the data to inspect its structure\n",
    "print(\"Available keys in the pickle file:\", data.keys())\n",
    "\n",
    "# If the data is a large structure, you may also want to inspect a sample of the values\n",
    "for key in data.keys():\n",
    "    print(f\"Sample data for key '{key}':\", data[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type: <class 'pandas.core.frame.DataFrame'>\n",
      "Content preview: Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "\n",
      "[25000000 rows x 0 columns]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the pickle file\n",
    "pickle_file = '../../01_Data/01_Raw/features/zone_1.pickle'  # Path to the pickle file\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Step 2: Inspect the loaded data\n",
    "if isinstance(data, np.ndarray):  # If it's an array\n",
    "    print(\"Loaded data is an ndarray with shape:\", data.shape)\n",
    "    print(\"Sample values:\", data)\n",
    "\n",
    "    # Visualize as a raster plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(data, cmap='Blues', interpolation='nearest')\n",
    "    plt.colorbar(label='Values')\n",
    "    plt.title('Raster Plot of Zone 1 Data')\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.show()\n",
    "\n",
    "elif isinstance(data, list):  # If it's a list\n",
    "    print(\"Loaded data is a list with length:\", len(data))\n",
    "    print(\"Sample first entry:\", data[0])\n",
    "\n",
    "    # If the first entry is an array, visualize it\n",
    "    if isinstance(data[0], np.ndarray):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(data[0], cmap='Blues', interpolation='nearest')\n",
    "        plt.colorbar(label='Values')\n",
    "        plt.title('Raster Plot of First Array in List')\n",
    "        plt.xlabel('X-coordinate')\n",
    "        plt.ylabel('Y-coordinate')\n",
    "        plt.show()\n",
    "elif isinstance(data, dict):  # If it's a dictionary\n",
    "    print(\"Loaded data is a dictionary with keys:\", data.keys())\n",
    "    for key in data:\n",
    "        print(f\"Key '{key}' contains data of type {type(data[key])}\")\n",
    "\n",
    "else:  # If it's something else\n",
    "    print(\"Loaded data type:\", type(data))\n",
    "    print(\"Content preview:\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 501 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"File size:\", os.path.getsize('../../01_Data/01_Raw/features/zone_1.pickle'), \"bytes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
