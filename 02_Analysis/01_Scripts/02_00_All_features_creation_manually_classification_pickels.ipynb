{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pickles having the needed features\n",
    "This is first time I'm doing things like this. It was unclear, based on the original study, in what phase all the pickles were made?\n",
    "I'm making them in this file.\n",
    "\n",
    "Lot of debugging included..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature_Creation.ipynb\n",
    "# Imported libraries\n",
    "import datetime\n",
    "import dask.array as da\n",
    "import dask\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from skimage.filters import gabor\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.ndimage import generic_filter as gf\n",
    "from dask_image.ndfilters import generic_filter as d_gf\n",
    "from scipy.ndimage import gaussian_filter, sobel\n",
    "import pickle\n",
    "# Import the required functions from feature_creation.py in Functions directory\n",
    "from Functions import feature_creation\n",
    "from Functions import feature_creation2\n",
    "from Functions import general_functions\n",
    "#from Functions import post_processing\n",
    "from Functions.general_functions import create_circular_mask\n",
    "import os\n",
    "\n",
    "from numba import config\n",
    "config.DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Functions.feature_creation' from 'e:\\\\Gradu\\\\PurOja\\\\02_Analysis\\\\01_Scripts\\\\Functions\\\\feature_creation.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import Functions.feature_creation2\n",
    "importlib.reload(Functions.feature_creation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to feature files\n",
    "features_path = \"../../01_Data/01_Raw/features/features\" \n",
    "output_path = \"../../01_Data/01_Raw/features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Dashboard: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 06:26:21,535 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f7792c45000>, <Task finished name='Task-2185' coro=<BaseTCPListener._handle_stream() done, defined at /PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py:654> exception=ValueError('invalid operation on non-started TCPListener')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/tcpserver.py\", line 387, in <lambda>\n",
      "    gen.convert_yielded(future), lambda f: f.result()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 660, in _handle_stream\n",
      "    logger.debug(\"Incoming connection from %r to %r\", address, self.contact_address)\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 696, in contact_address\n",
      "    host, port = self.get_host_port()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 677, in get_host_port\n",
      "    self._check_started()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 652, in _check_started\n",
      "    raise ValueError(\"invalid operation on non-started TCPListener\")\n",
      "ValueError: invalid operation on non-started TCPListener\n",
      "2025-03-25 06:26:21,538 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f7792c44ca0>, <Task finished name='Task-2186' coro=<BaseTCPListener._handle_stream() done, defined at /PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py:654> exception=ValueError('invalid operation on non-started TCPListener')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/tcpserver.py\", line 387, in <lambda>\n",
      "    gen.convert_yielded(future), lambda f: f.result()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 660, in _handle_stream\n",
      "    logger.debug(\"Incoming connection from %r to %r\", address, self.contact_address)\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 696, in contact_address\n",
      "    host, port = self.get_host_port()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 677, in get_host_port\n",
      "    self._check_started()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 652, in _check_started\n",
      "    raise ValueError(\"invalid operation on non-started TCPListener\")\n",
      "ValueError: invalid operation on non-started TCPListener\n",
      "2025-03-25 06:26:21,540 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f7792c44af0>, <Task finished name='Task-2187' coro=<BaseTCPListener._handle_stream() done, defined at /PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py:654> exception=ValueError('invalid operation on non-started TCPListener')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/tcpserver.py\", line 387, in <lambda>\n",
      "    gen.convert_yielded(future), lambda f: f.result()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 660, in _handle_stream\n",
      "    logger.debug(\"Incoming connection from %r to %r\", address, self.contact_address)\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 696, in contact_address\n",
      "    host, port = self.get_host_port()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 677, in get_host_port\n",
      "    self._check_started()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 652, in _check_started\n",
      "    raise ValueError(\"invalid operation on non-started TCPListener\")\n",
      "ValueError: invalid operation on non-started TCPListener\n",
      "2025-03-25 06:26:21,541 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f7792c44f70>, <Task finished name='Task-2188' coro=<BaseTCPListener._handle_stream() done, defined at /PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py:654> exception=ValueError('invalid operation on non-started TCPListener')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/tcpserver.py\", line 387, in <lambda>\n",
      "    gen.convert_yielded(future), lambda f: f.result()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 660, in _handle_stream\n",
      "    logger.debug(\"Incoming connection from %r to %r\", address, self.contact_address)\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 696, in contact_address\n",
      "    host, port = self.get_host_port()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 677, in get_host_port\n",
      "    self._check_started()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 652, in _check_started\n",
      "    raise ValueError(\"invalid operation on non-started TCPListener\")\n",
      "ValueError: invalid operation on non-started TCPListener\n",
      "2025-03-25 06:26:21,738 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:33507'.\n",
      "2025-03-25 06:26:21,740 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:33165'.\n",
      "2025-03-25 06:26:21,740 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 06:26:21,741 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 06:26:21,742 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:39523'.\n",
      "2025-03-25 06:26:21,743 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:38007'.\n",
      "2025-03-25 06:26:21,743 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 06:26:21,744 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:37215'.\n",
      "2025-03-25 06:26:21,744 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 06:26:21,745 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 06:26:21,746 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:33199'.\n",
      "2025-03-25 06:26:21,747 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:34477'.\n",
      "2025-03-25 06:26:21,747 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 06:26:21,748 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 06:26:21,763 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:44001'.\n",
      "2025-03-25 06:26:21,764 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 06:26:21,767 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:44025'.\n",
      "2025-03-25 06:26:21,768 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 06:26:24,732 - distributed.nanny - WARNING - Worker process still alive after 3.1999987792968754 seconds, killing\n",
      "2025-03-25 06:26:24,733 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing\n",
      "2025-03-25 06:26:24,734 - distributed.nanny - WARNING - Worker process still alive after 3.1999990844726565 seconds, killing\n",
      "2025-03-25 06:26:24,735 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 06:26:24,736 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 06:26:24,737 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 06:26:24,738 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2025-03-25 06:26:24,739 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 06:26:24,739 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 07:09:14,490 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:44507'.\n",
      "2025-03-25 07:09:17,520 - distributed.nanny - WARNING - Worker process still alive after 3.1999989318847657 seconds, killing\n",
      "2025-03-25 07:16:03,450 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:39293'.\n",
      "2025-03-25 07:16:06,392 - distributed.nanny - WARNING - Worker process still alive after 3.1999990844726565 seconds, killing\n",
      "2025-03-25 08:06:35,316 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:43625'.\n",
      "2025-03-25 08:06:35,326 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:37203'.\n",
      "2025-03-25 08:23:20,315 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46601'.\n",
      "2025-03-25 08:23:20,800 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:43021'.\n",
      "2025-03-25 08:23:23,520 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2025-03-25 09:00:08,967 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:41197'.\n",
      "2025-03-25 09:00:12,076 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing\n",
      "2025-03-25 09:05:18,312 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:44921'.\n",
      "2025-03-25 10:07:58,670 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:34377'.\n",
      "2025-03-25 10:13:38,264 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f7790116170>, <Task finished name='Task-7962592' coro=<BaseTCPListener._handle_stream() done, defined at /PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py:654> exception=ValueError('invalid operation on non-started TCPListener')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/tcpserver.py\", line 387, in <lambda>\n",
      "    gen.convert_yielded(future), lambda f: f.result()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 660, in _handle_stream\n",
      "    logger.debug(\"Incoming connection from %r to %r\", address, self.contact_address)\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 696, in contact_address\n",
      "    host, port = self.get_host_port()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 677, in get_host_port\n",
      "    self._check_started()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 652, in _check_started\n",
      "    raise ValueError(\"invalid operation on non-started TCPListener\")\n",
      "ValueError: invalid operation on non-started TCPListener\n",
      "2025-03-25 10:13:38,695 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:45901'.\n",
      "2025-03-25 10:13:41,464 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing\n",
      "2025-03-25 10:56:30,653 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:44791'.\n",
      "2025-03-25 10:56:33,024 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 11:10:55,155 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:42937'.\n",
      "2025-03-25 11:10:57,516 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing\n",
      "2025-03-25 11:11:00,319 - tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f7748fe5e10>, <Task finished name='Task-9147795' coro=<BaseTCPListener._handle_stream() done, defined at /PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py:654> exception=ValueError('invalid operation on non-started TCPListener')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/ioloop.py\", line 750, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/tornado/tcpserver.py\", line 387, in <lambda>\n",
      "    gen.convert_yielded(future), lambda f: f.result()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 660, in _handle_stream\n",
      "    logger.debug(\"Incoming connection from %r to %r\", address, self.contact_address)\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 696, in contact_address\n",
      "    host, port = self.get_host_port()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 677, in get_host_port\n",
      "    self._check_started()\n",
      "  File \"/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 652, in _check_started\n",
      "    raise ValueError(\"invalid operation on non-started TCPListener\")\n",
      "ValueError: invalid operation on non-started TCPListener\n",
      "2025-03-25 11:11:00,660 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:44233'.\n",
      "2025-03-25 11:11:00,662 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:36279'.\n",
      "2025-03-25 11:11:00,663 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:39435'.\n",
      "2025-03-25 11:11:00,674 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:37509'.\n",
      "2025-03-25 11:11:00,675 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:41413'.\n",
      "2025-03-25 11:11:00,678 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:34425'.\n",
      "2025-03-25 11:11:00,689 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:41329'.\n",
      "2025-03-25 11:11:03,517 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 11:11:03,518 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 11:11:03,519 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing\n",
      "2025-03-25 11:11:03,519 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 11:11:03,520 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 11:11:03,521 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n",
      "2025-03-25 11:11:03,521 - distributed.nanny - WARNING - Worker process still alive after 3.199999694824219 seconds, killing\n",
      "2025-03-25 11:48:10,758 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:39219'.\n",
      "2025-03-25 11:48:13,896 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "# Set up a Dask cluster with adaptive scaling\n",
    "cluster = LocalCluster(\n",
    "    processes=True,                # Use separate processes for parallelism\n",
    "    n_workers=10,                  # Match with available CPU cores\n",
    "    threads_per_worker=1,          # One thread per worker\n",
    "    memory_limit=\"20GB\"            # Memory limit per worker\n",
    ")\n",
    "\n",
    "# Enable adaptive scaling\n",
    "cluster.adapt(minimum=1, maximum=15)  # Adjust workers dynamically (1 to 15 workers)\n",
    "\n",
    "# Connect the client to the cluster\n",
    "client = Client(cluster)\n",
    "\n",
    "# Configure memory management\n",
    "dask.config.set({\n",
    "    'distributed.worker.memory.target': 0.7,  # Spill memory starts at 70% usage\n",
    "    'distributed.worker.memory.spill': 0.8,  # Spill to disk when 80% memory is used\n",
    "    'distributed.worker.memory.pause': 0.95, # Pause computation at 95% memory usage\n",
    "})\n",
    "\n",
    "# Check cluster dashboard link\n",
    "print(\"Dask Dashboard:\", client.dashboard_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/client.py:3157: UserWarning: Sending large graph of size 190.74 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files for zone 10:\n",
      "Labels shape: (5000, 5000)\n",
      "HPMF shape: (5000, 5000)\n",
      "Skyview shape: (5000, 5000)\n",
      "Impoundment shape: (5000, 5000)\n",
      "Slope shape: (5000, 5000)\n",
      "DEM shape: (5000, 5000)\n",
      "Processing DEM features... 2025-03-25 11:20:59.545567\n",
      "Detected ditches: 3335157 pixels\n",
      "Added DEM ditches detected.\n",
      "Calculating conic_mean...\n",
      "Added conic_mean to the DataFrame.\n",
      "Calculating skyview_gabor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/client.py:3157: UserWarning: Sending large graph of size 190.74 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2025-03-25 11:48:10,586 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:48:10,640 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:48:10,701 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:48:10,759 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.\n",
      "2025-03-25 11:49:03,324 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:03,325 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:03,330 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:03,529 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:12,332 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:12,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:12,462 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:12,464 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:57,321 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:57,322 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:57,322 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:49:57,677 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:50:08,318 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:50:08,319 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:50:08,557 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:50:23,318 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:50:23,659 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:50:28,320 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:50:28,788 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:50:33,932 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:50:46,215 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 11:51:55,868 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added skyview_gabor to the DataFrame.\n",
      "starting-impoundment:     2025-03-25 11:55:58.757157\n",
      "Processing Impoundment features...\n",
      "Calculating impoundment_amplified...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/dask/base.py:1462: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'impoundment_amplified' to the DataFrame.\n",
      "starting-hpmf manual:     2025-03-25 12:01:13.588287\n",
      "Calculating hpmf_f...\n",
      "Added 'hpmf_f' to the DataFrame.\n",
      "Calculating hpmf_f_visualisation...\n",
      "Added 'hpmf_f_visualisation' to the DataFrame.\n",
      "starting-slope manual:     2025-03-25 12:01:28.946231\n",
      "Calculating slope_channels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PUHTI_TYKKY_FRQGCcR/miniconda/envs/env1/lib/python3.10/site-packages/distributed/client.py:3157: UserWarning: Sending large graph of size 95.37 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2025-03-25 12:01:38,427 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 12:01:39,336 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 12:01:40,348 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n",
      "2025-03-25 12:01:40,348 - distributed.core - INFO - Connection to tcp://127.0.0.1:35693 has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'slope_channels' to the DataFrame.\n",
      "Feature generation completed for zone 10.\n",
      "          label_3m  hpmf_raw  skyview_raw  impoundment_raw  slope_raw  \\\n",
      "0                0  0.110000     0.983332              0.0   6.843359   \n",
      "1                0  0.115000     0.982089              0.0   6.843359   \n",
      "2                0  0.121406     0.981108              0.0  16.459957   \n",
      "3                0  0.129219     0.980160              0.0  21.390993   \n",
      "4                0  0.137031     0.978938              0.0  20.957382   \n",
      "...            ...       ...          ...              ...        ...   \n",
      "24999995         0 -0.010000     1.000000              0.0   0.000874   \n",
      "24999996         0 -0.010000     1.000000              0.0   0.000874   \n",
      "24999997         0 -0.010000     1.000000              0.0   0.000874   \n",
      "24999998         0 -0.010000     1.000000              0.0   0.000874   \n",
      "24999999         0 -0.010000     1.000000              0.0   0.000874   \n",
      "\n",
      "               DEM_raw  DEM_ditch_detection  DEM_ditch_detection_streams  \\\n",
      "0           134.899994                  0.0                          0.0   \n",
      "1           134.899994                  0.0                          0.0   \n",
      "2           134.899994                  0.0                          0.0   \n",
      "3           135.063995                  0.0                          0.0   \n",
      "4           135.063995                  0.0                          0.0   \n",
      "...                ...                  ...                          ...   \n",
      "24999995 -99999.000000                  0.0                          0.0   \n",
      "24999996 -99999.000000                  0.0                          0.0   \n",
      "24999997 -99999.000000                  0.0                          0.0   \n",
      "24999998 -99999.000000                  0.0                          0.0   \n",
      "24999999 -99999.000000                  0.0                          0.0   \n",
      "\n",
      "          conic_mean  skyview_gabor  impoundment_amplified    hpmf_f  \\\n",
      "0           0.969966       0.081471               0.088127  0.124688   \n",
      "1           0.968770       0.081022               0.088127  0.279302   \n",
      "2           0.966994       0.080045               0.090230  0.244389   \n",
      "3           0.964163       0.078388               0.101671  0.386533   \n",
      "4           0.959949       0.075853               0.132941  0.668329   \n",
      "...              ...            ...                    ...       ...   \n",
      "24999995    1.000000       0.068032               0.000000  0.000000   \n",
      "24999996    1.000000       0.068032               0.000000  0.000000   \n",
      "24999997    1.000000       0.068032               0.000000  0.000000   \n",
      "24999998    1.000000       0.068032               0.000000  0.000000   \n",
      "24999999    1.000000       0.068032               0.000000  0.000000   \n",
      "\n",
      "          hpmf_f_visualisation  slope_channels  \n",
      "0                        False            10.0  \n",
      "1                        False             6.0  \n",
      "2                        False             6.0  \n",
      "3                        False            10.0  \n",
      "4                         True            10.0  \n",
      "...                        ...             ...  \n",
      "24999995                 False             1.0  \n",
      "24999996                 False             1.0  \n",
      "24999997                 False             1.0  \n",
      "24999998                 False             1.0  \n",
      "24999999                 False             1.0  \n",
      "\n",
      "[25000000 rows x 14 columns]\n",
      "Shape of skyview_raw: (25000000,)\n",
      "Number of dimensions of skyview_raw: 1\n",
      "skyview_raw is 1D\n",
      "Starting feature computation:  2025-03-25 12:01:41.275241\n",
      "Generating feature: skyview\n",
      "Added column: skyview\n",
      "Generating feature: impoundment\n",
      "Added column: impoundment\n",
      "Generating feature: hpmf\n",
      "Added column: hpmf\n",
      "Generating feature: slope\n",
      "Added column: slope\n",
      "Generating feature: skyview_max_2\n",
      "Added column: skyview_max_2\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_max_2 with radius 2\n",
      "Generating feature: skyview_max_4\n",
      "Added column: skyview_max_4\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_max_4 with radius 4\n",
      "Generating feature: skyview_max_6\n",
      "Added column: skyview_max_6\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_max_6 with radius 6\n",
      "Generating feature: skyview_min_6\n",
      "Added column: skyview_min_6\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_min_6 with radius 6\n",
      "Generating feature: skyview_median_2\n",
      "Added column: skyview_median_2\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_median_2 with radius 2\n",
      "Generating feature: skyview_median_4\n",
      "Added column: skyview_median_4\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_median_4 with radius 4\n",
      "Generating feature: skyview_median_6\n",
      "Added column: skyview_median_6\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_median_6 with radius 6\n",
      "Generating feature: skyview_std_6\n",
      "Added column: skyview_std_6\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_std_6 with radius 6\n",
      "Generating feature: skyview_skew_2\n",
      "Added column: skyview_skew_2\n",
      "Applied mask to the array.\n",
      "Applied mask for skyview_skew_2 with radius 2\n",
      "Generating feature: impoundment_mean_3\n",
      "Added column: impoundment_mean_3\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_mean_3 with radius 3\n",
      "Generating feature: impoundment_mean_2\n",
      "Added column: impoundment_mean_2\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_mean_2 with radius 2\n",
      "Generating feature: impoundment_mean_6\n",
      "Added column: impoundment_mean_6\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_mean_6 with radius 6\n",
      "Generating feature: impoundment_median_2\n",
      "Added column: impoundment_median_2\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_median_2 with radius 2\n",
      "Generating feature: impoundment_median_4\n",
      "Added column: impoundment_median_4\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_median_4 with radius 4\n",
      "Generating feature: impoundment_median_6\n",
      "Added column: impoundment_median_6\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_median_6 with radius 6\n",
      "Generating feature: impoundment_max_6\n",
      "Added column: impoundment_max_6\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_max_6 with radius 6\n",
      "Generating feature: impoundment_std_4\n",
      "Added column: impoundment_std_4\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_std_4 with radius 4\n",
      "Generating feature: impoundment_std_6\n",
      "Added column: impoundment_std_6\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_std_6 with radius 6\n",
      "Generating feature: impoundment_skew_2\n",
      "Added column: impoundment_skew_2\n",
      "Applied mask to the array.\n",
      "Applied mask for impoundment_skew_2 with radius 2\n",
      "Generating feature: hpmf_mean_2\n",
      "Added column: hpmf_mean_2\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_mean_2 with radius 2\n",
      "Generating feature: hpmf_mean_3\n",
      "Added column: hpmf_mean_3\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_mean_3 with radius 3\n",
      "Generating feature: hpmf_mean_4\n",
      "Added column: hpmf_mean_4\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_mean_4 with radius 4\n",
      "Generating feature: hpmf_mean_6\n",
      "Added column: hpmf_mean_6\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_mean_6 with radius 6\n",
      "Generating feature: hpmf_min_3\n",
      "Added column: hpmf_min_3\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_min_3 with radius 3\n",
      "Generating feature: hpmf_min_4\n",
      "Added column: hpmf_min_4\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_min_4 with radius 4\n",
      "Generating feature: hpmf_median_4\n",
      "Added column: hpmf_median_4\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_median_4 with radius 4\n",
      "Generating feature: hpmf_std_6\n",
      "Added column: hpmf_std_6\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_std_6 with radius 6\n",
      "Generating feature: hpmf_skew_2\n",
      "Added column: hpmf_skew_2\n",
      "Applied mask to the array.\n",
      "Applied mask for hpmf_skew_2 with radius 2\n",
      "Generating feature: slope_median_6\n",
      "Added column: slope_median_6\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_median_6 with radius 6\n",
      "Generating feature: slope_min_2\n",
      "Added column: slope_min_2\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_min_2 with radius 2\n",
      "Generating feature: slope_min_4\n",
      "Added column: slope_min_4\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_min_4 with radius 4\n",
      "Generating feature: slope_min_6\n",
      "Added column: slope_min_6\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_min_6 with radius 6\n",
      "Generating feature: slope_std_4\n",
      "Added column: slope_std_4\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_std_4 with radius 4\n",
      "Generating feature: slope_std_6\n",
      "Added column: slope_std_6\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_std_6 with radius 6\n",
      "Generating feature: slope_skew_2\n",
      "Added column: slope_skew_2\n",
      "Applied mask to the array.\n",
      "Applied mask for slope_skew_2 with radius 2\n",
      "Feature computation completed:  2025-03-25 12:02:02.624298\n",
      "Generated columns in data_frame: Index(['label_3m', 'hpmf_raw', 'skyview_raw', 'impoundment_raw', 'slope_raw',\n",
      "       'DEM_raw', 'DEM_ditch_detection', 'DEM_ditch_detection_streams',\n",
      "       'conic_mean', 'skyview_gabor', 'impoundment_amplified', 'hpmf_f',\n",
      "       'hpmf_f_visualisation', 'slope_channels', 'skyview', 'impoundment',\n",
      "       'hpmf', 'slope', 'skyview_max_2', 'skyview_max_4', 'skyview_max_6',\n",
      "       'skyview_min_6', 'skyview_median_2', 'skyview_median_4',\n",
      "       'skyview_median_6', 'skyview_std_6', 'skyview_skew_2',\n",
      "       'impoundment_mean_3', 'impoundment_mean_2', 'impoundment_mean_6',\n",
      "       'impoundment_median_2', 'impoundment_median_4', 'impoundment_median_6',\n",
      "       'impoundment_max_6', 'impoundment_std_4', 'impoundment_std_6',\n",
      "       'impoundment_skew_2', 'hpmf_mean_2', 'hpmf_mean_3', 'hpmf_mean_4',\n",
      "       'hpmf_mean_6', 'hpmf_min_3', 'hpmf_min_4', 'hpmf_median_4',\n",
      "       'hpmf_std_6', 'hpmf_skew_2', 'slope_median_6', 'slope_min_2',\n",
      "       'slope_min_4', 'slope_min_6', 'slope_std_4', 'slope_std_6',\n",
      "       'slope_skew_2'],\n",
      "      dtype='object')\n",
      "Warning: The following columns are missing and will be excluded: {'hpmf_gabor', 'impoundment_std_6hpmf_f', 'impoundment_mean_4', 'slope_mean_6', 'hpmf_min_2', 'hpmf_filter'}\n",
      "Zone 10 features saved to pickle.\n"
     ]
    }
   ],
   "source": [
    "def d_gf(array, func, footprint):\n",
    "        \"\"\"\n",
    "        Wrapper function to safely apply a mask or function with Dask arrays.\n",
    "        Handles edge cases for padding and ensures indices are valid.\n",
    "        \"\"\"\n",
    "        depth = footprint.shape[0] // 2\n",
    "        depth = max(0, depth)  # Ensure depth is non-negative\n",
    "\n",
    "        # Apply the mask with proper overlap and safe boundary handling\n",
    "        return da.map_overlap(\n",
    "        func, array, depth=depth, boundary=\"reflect\", trim=True\n",
    "    )\n",
    "        # Helper function to add feature columns conditionally\n",
    "\n",
    "\n",
    "def load_raw(zone_number, chunk_size=(1666, 1666)):\n",
    "        global features_path, output_path\n",
    "        data_frame = None  # Ensure data_frame is explicitly initialized\n",
    "\n",
    "        try:\n",
    "            # Load raw feature files\n",
    "            labels = np.load(f\"{features_path}/Label3m_{zone_number}.npy\")\n",
    "            hpmf = np.load(f\"{features_path}/HPMF_{zone_number}.npy\")\n",
    "            skyview = np.load(f\"{features_path}/SVF_{zone_number}.npy\").astype(np.float64)\n",
    "            impoundment = np.load(f\"{features_path}/Impoundment_{zone_number}.npy\")\n",
    "            slope = np.load(f\"{features_path}/slope_{zone_number}.npy\")\n",
    "            DEM = np.load(f\"{features_path}/DEM_{zone_number}.npy\")\n",
    "            # Replace -99999 with np.nan in DEM using Dask\n",
    "            dask_dem = da.from_array(DEM, chunks=chunk_size)\n",
    "            dask_dem = da.where(dask_dem == -99999, np.nan, dask_dem)\n",
    "\n",
    "            # Trigger computation and persist changes\n",
    "            dask_dem = dask_dem.persist()  # Persist in memory for future use\n",
    "\n",
    "            print(f\"Loaded files for zone {zone_number}:\")\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "            print(f\"HPMF shape: {hpmf.shape}\")\n",
    "            print(f\"Skyview shape: {skyview.shape}\")\n",
    "            print(f\"Impoundment shape: {impoundment.shape}\")\n",
    "            print(f\"Slope shape: {slope.shape}\")\n",
    "            print(f\"DEM shape: {DEM.shape}\")\n",
    "\n",
    "            # Assertions to validate inputs\n",
    "            assert labels is not None, \"Error: 'labels' is not defined or is None.\"\n",
    "            assert hpmf is not None, \"Error: 'hpmf' is not defined or is None.\"\n",
    "            assert skyview is not None, \"Error: 'skyview' is not defined or is None.\"\n",
    "\n",
    "            # Feature creation\n",
    "            stream_amp = feature_creation2.stream_amplification(impoundment)\n",
    "\n",
    "            data_frame = pd.DataFrame({\n",
    "                \"label_3m\": labels.reshape(-1),\n",
    "                \"hpmf_raw\": hpmf.reshape(-1),\n",
    "                \"skyview_raw\": skyview.reshape(-1),\n",
    "                \"impoundment_raw\": impoundment.reshape(-1),\n",
    "                \"slope_raw\": slope.reshape(-1),\n",
    "            })\n",
    "\n",
    "            # Add additional features (e.g., DEM)\n",
    "            if \"DEM_raw\" not in data_frame.columns:\n",
    "                data_frame[\"DEM_raw\"] = DEM.flatten()\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"FileNotFoundError: {e}\")\n",
    "            return None\n",
    "        except AssertionError as e:\n",
    "            print(f\"AssertionError: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"General Error: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Final fallback check\n",
    "        if data_frame is None:\n",
    "            raise RuntimeError(\"Failed to create or load 'data_frame'. Check input data and processing steps.\")\n",
    "\n",
    "\n",
    "    #------------- MANUAL ----------------\n",
    "    #------------- Dem -------------------\n",
    "    # Add DEM feature if not already in DataFrame\n",
    "        def add_features(data_frame, DEM, skyview, impoundment, hpmf, slope, zone_number):\n",
    "                if \"DEM_raw\" not in data_frame.columns:  \n",
    "                    data_frame[\"DEM_raw\"] = dask_dem.compute().flatten() # -99999 values to Nan\n",
    "\n",
    "                    print(\"Defining tasks...\")\n",
    "\n",
    "        #------------- Helper function adds features to the data_frame -------------------\n",
    "\n",
    "        # Add features derived from DEM\n",
    "        print(\"Processing DEM features...\", datetime.datetime.now())\n",
    "        # Ensure the shape of the DEM array matches expected dimensions\n",
    "        dem_shape = dask_dem.shape\n",
    "        assert dem_shape == (5000, 5000), f\"Unexpected DEM shape: {dem_shape}\"\n",
    "        if \"DEM_ditch_detection\" not in data_frame.columns:\n",
    "            data_frame[\"DEM_ditch_detection\"] = feature_creation2.dem_ditch_detection(dask_dem.compute()).reshape(-1) # -99999 to Nan\n",
    "\n",
    "        # I'm interested in all the streams\n",
    "        if \"DEM_ditch_detection_streams\" not in data_frame.columns:\n",
    "            data_frame[\"DEM_ditch_detection_streams\"] = feature_creation2.impoundment_DEM_stream_strenghten(data_frame[\"DEM_ditch_detection\"].values.reshape((5000,5000)), stream_amp).reshape(-1)\n",
    "            print(\"Added DEM ditches detected.\")\n",
    "\n",
    "                    # I'm interested in all the streams\n",
    "                    #------------- Sky view factor -------------------\n",
    "                    # Add features derived from Skyview\n",
    "            #conic means from sky view factor\n",
    "        if \"conic_mean\" not in data_frame.columns:\n",
    "            print (\"Calculating conic_mean...\")\n",
    "            count_conic_mean = feature_creation2.conic_mean(skyview, 6, 0.9894350171089172)\n",
    "            data_frame[\"conic_mean\"] = count_conic_mean.flatten()\n",
    "            print(\"Added conic_mean to the DataFrame.\")\n",
    "        #skyview gabor\n",
    "        if \"skyview_gabor\" not in data_frame:\n",
    "            print (\"Calculating skyview_gabor...\")\n",
    "            data_frame[\"skyview_gabor\"] = feature_creation2.sky_view_gabor(skyview).reshape(-1)\n",
    "            print(\"Added skyview_gabor to the DataFrame.\")\n",
    "        \n",
    "        #if \"skyview_ditch\" not in data_frame.columns:\n",
    "        #    print (\"Calculating skyview ditch...\")\n",
    "        #    data_frame[\"skyview_ditch\"] = feature_creation2._reclassify_sky_view_non_ditch_amp(skyview).reshape(-1)\n",
    "        #    print(\"Added skyview_ditch to the DataFrame.\")\n",
    "\n",
    "        print(\"starting-impoundment:    \", datetime.datetime.now())\n",
    "\n",
    "        print(\"Processing Impoundment features...\")\n",
    "        if \"impoundment_amplified\" not in data_frame.columns:\n",
    "            print(\"Calculating impoundment_amplified...\")\n",
    "            data_frame[\"impoundment_amplified\"] = feature_creation2.impoundment_amplification(impoundment).reshape(-1)\n",
    "            print(\"Added 'impoundment_amplified' to the DataFrame.\")\n",
    "\n",
    "        print(\"starting-hpmf manual:    \", datetime.datetime.now())\n",
    "\n",
    "        if \"hpmf_f\" not in data_frame.columns:\n",
    "            print(\"Calculating hpmf_f...\")\n",
    "            count_hpmf_f = feature_creation2.process_hpmf(hpmf, 5)\n",
    "            data_frame[\"hpmf_f\"] = count_hpmf_f.flatten()\n",
    "            print(\"Added 'hpmf_f' to the DataFrame.\")\n",
    "        \n",
    "        if \"hpmf_f_visualisation\" not in data_frame.columns: \n",
    "            print(\"Calculating hpmf_f_visualisation...\")\n",
    "            count_hpmf_fv = feature_creation2.hpmf_f_visualisation(hpmf, 6) \n",
    "            data_frame[\"hpmf_f_visualisation\"] = count_hpmf_fv.flatten() \n",
    "            print(\"Added 'hpmf_f_visualisation' to the DataFrame.\")\n",
    "\n",
    "        print(\"starting-slope manual:    \", datetime.datetime.now())\n",
    "        if \"slope_channels\" not in data_frame:\n",
    "            print(\"Calculating slope_channels...\")\n",
    "            slope_processed_dask = feature_creation2.process_slope_for_channels(slope, chunk_size=chunk_size)\n",
    "            data_frame[\"slope_channels\"] = slope_processed_dask.compute().reshape(-1) \n",
    "            print(\"Added 'slope_channels' to the DataFrame.\")\n",
    "            \n",
    "            print(f\"Feature generation completed for zone {zone_number}.\")\n",
    "                    \n",
    "        # Return the final DataFrame once features are added\n",
    "            #Final fallback check for undefined data_frame\n",
    "            if 'data_frame' not in locals():\n",
    "                raise RuntimeError(\"Failed to create or load 'data_frame'. Please check input data and pickle files.\")\n",
    "     \n",
    "            return data_frame\n",
    "#------------- AUTOMATED --------------------------\n",
    "#------------- Process dataframe-------------------\n",
    "def process_dataframe(data_frame, skyview, impoundment, hpmf, slope, chunk_size):\n",
    "    print(\"Starting feature computation: \", datetime.datetime.now())\n",
    "\n",
    "    def add_feature(column_name_base, compute_func, array, radii, *args):\n",
    "        \"\"\"\n",
    "        Add a feature to the data frame, ensuring consistent naming.\n",
    "        \"\"\"\n",
    "        for radius in radii:\n",
    "            column_name = f\"{column_name_base}_{radius}\"  # Construct consistent column names\n",
    "            if column_name not in data_frame.columns:\n",
    "                print(f\"Generating feature: {column_name}\")\n",
    "                data_frame[column_name] = compute_func(array, *args).flatten()\n",
    "\n",
    "        print(f\"Feature generation completed for zone {zone_number}.\")\n",
    "        print(f\"Shape of feature array before flattening: {array.shape}\")\n",
    "        print(f\"Shape after computation: {compute_func(array, *args).shape}\")\n",
    "\n",
    "    # Handling NaN values for np.amax (replace NaNs with -infinity)\n",
    "    skyview_no_nan = np.nan_to_num(skyview, nan=-np.inf)\n",
    "\n",
    "    # List of features to generate\n",
    "    features = [\n",
    "        (\"skyview\", skyview, [2, 4, 6], np.nanmax, \"max\"),\n",
    "        (\"skyview\", skyview, [6], np.nanmin, \"min\"),\n",
    "        (\"skyview\", skyview, [2, 4, 6], np.nanmedian, \"median\"),\n",
    "        (\"skyview\", skyview, [6], np.nanstd, \"std\"),\n",
    "        (\"skyview\", skyview, [2], skew, \"skew\"),\n",
    "        (\"impoundment\", impoundment, [3, 2, 6], np.nanmean, \"mean\"),\n",
    "        (\"impoundment\", impoundment, [2, 4, 6], np.nanmedian, \"median\"),\n",
    "        (\"impoundment\", impoundment, [6], np.amax, \"max\"),\n",
    "        (\"impoundment\", impoundment, [4, 6], np.nanstd, \"std\"),\n",
    "        (\"impoundment\", impoundment, [2], skew, \"skew\"),\n",
    "        (\"hpmf\", hpmf, [2, 3, 4, 6], np.nanmean, \"mean\"),\n",
    "        (\"hpmf\", hpmf, [3, 4], np.amin, \"min\"),\n",
    "        (\"hpmf\", hpmf, [4], np.nanmedian, \"median\"),\n",
    "        (\"hpmf\", hpmf, [6], np.nanstd, \"std\"),\n",
    "        (\"hpmf\", hpmf, [2], skew, \"skew\"),\n",
    "        (\"slope\", slope, [6], np.nanmedian, \"median\"),\n",
    "        (\"slope\", slope, [2, 4, 6], np.amin, \"min\"),\n",
    "        (\"slope\", slope, [4, 6], np.nanstd, \"std\"),\n",
    "        (\"slope\", slope, [2], skew, \"skew\"),\n",
    "    ]\n",
    "\n",
    "    # Loop through features to calculate them\n",
    "    for col_name, array, feature_sizes, func, func_name in features:\n",
    "        # Check if the column already exists\n",
    "        if col_name not in data_frame.columns:\n",
    "            print(f\"Generating feature: {col_name}\")\n",
    "\n",
    "            # Apply the function to generate the feature\n",
    "            result = func(array)\n",
    "\n",
    "            # Check if result is a scalar and handle accordingly\n",
    "            if np.isscalar(result):  # If it's a scalar\n",
    "                data_frame[col_name] = np.full(len(data_frame), result)  # Fill the column with the scalar value\n",
    "            else:\n",
    "                # Ensure the result is a NumPy array and flatten it\n",
    "                if not isinstance(result, np.ndarray):\n",
    "                    result = np.array(result)  # Convert to numpy array if necessary\n",
    "                data_frame[col_name] = result.flatten()  # Flatten the array and assign to DataFrame\n",
    "\n",
    "            print(f\"Added column: {col_name}\")\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "    # Feature computation for regular features\n",
    "    for col_name, array, feature_sizes, func, func_name in features:\n",
    "        if col_name not in data_frame.columns:\n",
    "            print(f\"Generating feature: {col_name}\")\n",
    "            \n",
    "            # If the feature requires a circular mask and chunked computation\n",
    "            if func in [np.amax, np.nanmean, np.nanstd, np.nanmedian, skew]:  # Add the relevant functions\n",
    "                for radius in feature_sizes:\n",
    "                    col_name_with_radius = f\"{col_name}_{radius}\"\n",
    "                    if col_name_with_radius not in data_frame.columns:\n",
    "                        print(f\"Generating feature: {col_name_with_radius}\")\n",
    "                        result = d_gf(da.from_array(array, chunks=chunk_size), func, footprint=general_functions.create_circular_mask(radius)).compute(scheduler='processes').reshape(-1)\n",
    "                        data_frame[col_name_with_radius] = result.flatten()\n",
    "                        print(f\"Added column: {col_name_with_radius}\")\n",
    "            else:\n",
    "                result = func(array)\n",
    "                if np.isscalar(result):\n",
    "                    result = np.full(len(data_frame), result)\n",
    "                data_frame[col_name] = result.flatten()\n",
    "                print(f\"Added column: {col_name}\")\n",
    "\n",
    "    # Handling scalar features with radius\n",
    "    for prefix, array, radii, func, suffix in features:\n",
    "        for radius in radii:\n",
    "            col_name = f\"{prefix}_{suffix}_{radius}\"\n",
    "            if col_name not in data_frame.columns:\n",
    "                print(f\"Generating feature: {col_name}\")\n",
    "\n",
    "                # Compute the feature (mean, std, etc.)\n",
    "                feature = func(array)\n",
    "\n",
    "                # If the result is a scalar, broadcast it to match the DataFrame length\n",
    "                if np.isscalar(feature):\n",
    "                    feature = np.full(len(data_frame), feature)\n",
    "\n",
    "                # Add the feature to the DataFrame\n",
    "                data_frame[col_name] = feature.flatten()\n",
    "                print(f\"Added column: {col_name}\")\n",
    "\n",
    "                # Apply circular mask (if needed)\n",
    "                mask = general_functions.create_circular_mask(radius)\n",
    "                array_2d = array.reshape((5000, 5000))  # Assuming 5000x5000 shape (adjust accordingly)\n",
    "                array_dask = da.from_array(array_2d, chunks=chunk_size)\n",
    "\n",
    "                # Define the function to apply mask to each block\n",
    "                def apply_mask_to_block(block, mask):\n",
    "                    \"\"\"\n",
    "                    Applies a mask to the given block.\n",
    "                    Pads the mask if necessary to match the block size.\n",
    "                    \"\"\"\n",
    "                    block_height, block_width = block.shape\n",
    "                    mask_height, mask_width = mask.shape\n",
    "\n",
    "                    # Pad the mask to fit the block dimensions\n",
    "                    padded_mask = np.pad(\n",
    "                        mask, \n",
    "                        ((0, max(0, block_height - mask_height)), (0, max(0, block_width - mask_width))),\n",
    "                        mode='constant', \n",
    "                        constant_values=0\n",
    "                    )\n",
    "                    return block * padded_mask[:block_height, :block_width]\n",
    "\n",
    "                # Apply the masking function to each block\n",
    "\n",
    "                    masked_array = array_dask.map_blocks(\n",
    "                        apply_mask_to_block,\n",
    "                        mask=mask,  # Pass the precomputed mask as a keyword argument\n",
    "                        dtype=np.float32  # Explicitly specify the dtype\n",
    "                    ).compute()\n",
    "\n",
    "                print(\"Applied mask to the array.\")\n",
    "                print(f\"Applied mask for {col_name} with radius {radius}\")\n",
    "\n",
    "\n",
    "    print(\"Feature computation completed: \", datetime.datetime.now())\n",
    "    return data_frame\n",
    "\n",
    "#------------- Ordering dataframe-------------------\n",
    "def set_order(data_frame):\n",
    "    \"\"\"\n",
    "    Reorder columns for final DataFrame, checking for missing columns.\n",
    "    \"\"\"\n",
    "    ordered_columns = ordered_columns = [\n",
    "        'label_3m', \n",
    "\t\t'hpmf_raw', \n",
    "\t\t'skyview_raw', \n",
    "\t\t'impoundment_raw', \n",
    "\t\t'slope_raw', \n",
    "\t\t'DEM_ditch_detection', \n",
    "\t\t'DEM_ditch_detection_streams',\n",
    "\t\t'conic_mean',\n",
    "\t\t'skyview_gabor', \n",
    "\t\t'skyview_min_6', \n",
    "\t\t'skyview_median_2', \n",
    "\t\t'skyview_median_6', \n",
    "\t\t'skyview_max_2', \n",
    "\t\t'skyview_max_4', \n",
    "\t\t'skyview_max_6',\n",
    "\t\t'skyview_std_6', \n",
    "\t\t'impoundment_amplified', \n",
    "\t\t'impoundment_mean_2', \n",
    "        'impoundment_mean_3', \n",
    "\t\t'impoundment_mean_4',\n",
    "\t\t'impoundment_mean_6',\n",
    "\t\t'impoundment_median_2', \t\t\n",
    "\t\t'impoundment_median_4', \n",
    " \t\t'impoundment_median_6', \n",
    "\t\t'impoundment_max_6', \n",
    "        'impoundment_std_4', \n",
    "\t\t'impoundment_std_6',\n",
    "        'impoundment_std_6hpmf_f',\n",
    "\t\t'hpmf_f', \n",
    "        'hpmf_f_visualisation',\n",
    "        'hpmf_filter',\n",
    "\t\t'hpmf_gabor', \t\t\n",
    "\t\t'hpmf_min_2', \n",
    "        'hpmf_min_4',\n",
    "\t\t'hpmf_mean_3', \t\t\n",
    "\t\t'hpmf_mean_6', \n",
    "\t\t'hpmf_mean_4',\n",
    "        'hpmf_median_4', \n",
    "\t\t'hpmf_std_6', \n",
    "\t\t'slope_channels',\n",
    "        'slope_min_2', \t\n",
    "\t\t'slope_min_4', \t\t\n",
    "\t\t'slope_min_6',\n",
    "\t\t'slope_median_6',\n",
    "\t\t'slope_mean_6',\n",
    "\t\t'slope_std_4', \n",
    "\t\t'slope_std_6', \n",
    "    ]\n",
    "\n",
    "    # Check the columns of the data frame before reordering\n",
    "    print(\"Generated columns in data_frame:\", data_frame.columns)\n",
    "\n",
    "    # Check if all ordered columns exist in the DataFrame\n",
    "    existing_columns = [col for col in ordered_columns if col in data_frame.columns]\n",
    "\n",
    "    # Log any missing columns\n",
    "    missing_columns = set(ordered_columns) - set(existing_columns)\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: The following columns are missing and will be excluded: {missing_columns}\")\n",
    "\n",
    "    # If there are missing columns, we will return the data frame with the available columns only\n",
    "    ordered_columns_with_existing = [col for col in ordered_columns if col in existing_columns]\n",
    "\n",
    "    # Return the DataFrame with only the available columns in the desired order\n",
    "    return data_frame[ordered_columns_with_existing]\n",
    "\n",
    "# Define the range of zone numbers\n",
    "zone_numbers = range(10, 11)  # set the zone number range in here\n",
    "chunk_size = (1666, 1666) # this is basically size 5000/3, it's close enough to keep the shape\n",
    "\n",
    "# Loop through the zone numbers\n",
    "for zone_number in zone_numbers:\n",
    "    # Define the path to the pickle file\n",
    "    pickle_file_path = f\"{output_path}/zone_{zone_number}.pickle\"\n",
    "    \n",
    "    # Delete the existing pickle file if it exists (to ensure a fresh start)\n",
    "    if os.path.exists(pickle_file_path):\n",
    "        try:\n",
    "            os.remove(pickle_file_path)\n",
    "            print(f\"Deleted corrupted or incomplete pickle file for zone {zone_number}. Starting fresh.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting file for zone {zone_number}: {e}\")\n",
    "\n",
    "    # Check zone_with_features before using it\n",
    "    zone_with_features = load_raw(zone_number, chunk_size)\n",
    "    print(zone_with_features)  # This should print the DataFrame or an informative message\n",
    "\n",
    "    if zone_with_features is None:\n",
    "        raise ValueError(\"Failed to load raw data; 'zone_with_features' is None\")\n",
    "\n",
    "    # Check the shape and dimensions of the skyview data\n",
    "    skyview_data = zone_with_features[\"skyview_raw\"].values\n",
    "\n",
    "    print(\"Shape of skyview_raw:\", skyview_data.shape)  # Prints the shape of the data\n",
    "    print(\"Number of dimensions of skyview_raw:\", skyview_data.ndim)  # Prints the number of dimensions (1D, 2D, or 3D)\n",
    "\n",
    "    # Conditional check for dimensionality\n",
    "    if skyview_data.ndim == 1:\n",
    "        print(\"skyview_raw is 1D\")\n",
    "    elif skyview_data.ndim == 2:\n",
    "        print(\"skyview_raw is 2D\")\n",
    "    elif skyview_data.ndim == 3:\n",
    "        print(\"skyview_raw is 3D\")\n",
    "    else:\n",
    "        print(\"skyview_raw has more than 3 dimensions\")\n",
    "\n",
    "    # Now you can process the dataframe\n",
    "    zone_with_features = process_dataframe(\n",
    "        zone_with_features,\n",
    "        skyview=zone_with_features[\"skyview_raw\"].values,\n",
    "        impoundment=zone_with_features[\"impoundment_raw\"].values,\n",
    "        hpmf=zone_with_features[\"hpmf_raw\"].values,\n",
    "        slope=zone_with_features[\"slope_raw\"].values,\n",
    "        chunk_size=chunk_size\n",
    "    )\n",
    "\n",
    "    # Set the order for the features\n",
    "    zone_with_features = set_order(zone_with_features)\n",
    "\n",
    "# Call the function to process the zones and get the DataFrame\n",
    "data_frame = zone_with_features\n",
    "\n",
    "# Save the processed features to a new pickle file\n",
    "pickle.dump(zone_with_features, open(pickle_file_path, \"wb\"))\n",
    "\n",
    "# Create a list of futures (delayed tasks)\n",
    "\n",
    "print(f\"Zone {zone_number} features saved to pickle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "# Create a cluster\n",
    "cluster = LocalCluster()\n",
    "\n",
    "# Connect to the Dask cluster (if not already connected)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Close the client and cluster\n",
    "client.close()  # Closes the client\n",
    "cluster.shutdown()  # Shuts down the cluster completely\n",
    "\n",
    "# Optionally check if all workers are stopped\n",
    "print(\"Cluster workers:\", cluster.scheduler_info()['workers'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the test pickle\n",
    "StreamAmp result viewing. Should the treshold be tested?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in the pickle file: Index(['label_3m', 'hpmf_raw', 'skyview_raw', 'impoundment_raw', 'slope_raw',\n",
      "       'DEM_ditch_detection', 'DEM_ditch_detection_streams', 'conic_mean',\n",
      "       'skyview_gabor', 'skyview_min_6', 'skyview_median_2',\n",
      "       'skyview_median_6', 'skyview_max_2', 'skyview_max_4', 'skyview_max_6',\n",
      "       'skyview_std_6', 'impoundment_amplified', 'impoundment_mean_2',\n",
      "       'impoundment_mean_3', 'impoundment_mean_6', 'impoundment_median_2',\n",
      "       'impoundment_median_4', 'impoundment_median_6', 'impoundment_max_6',\n",
      "       'impoundment_std_4', 'impoundment_std_6', 'hpmf_f',\n",
      "       'hpmf_f_visualisation', 'hpmf_min_4', 'hpmf_mean_3', 'hpmf_mean_6',\n",
      "       'hpmf_mean_4', 'hpmf_median_4', 'hpmf_std_6', 'slope_channels',\n",
      "       'slope_min_2', 'slope_min_4', 'slope_min_6', 'slope_median_6',\n",
      "       'slope_std_4', 'slope_std_6'],\n",
      "      dtype='object')\n",
      "Sample data for key 'label_3m': 0           0\n",
      "1           0\n",
      "2           0\n",
      "3           0\n",
      "4           0\n",
      "           ..\n",
      "24999995    0\n",
      "24999996    0\n",
      "24999997    0\n",
      "24999998    0\n",
      "24999999    0\n",
      "Name: label_3m, Length: 25000000, dtype: uint8\n",
      "Sample data for key 'hpmf_raw': 0           0.072656\n",
      "1           0.079219\n",
      "2           0.073750\n",
      "3           0.056250\n",
      "4           0.038750\n",
      "              ...   \n",
      "24999995    0.108438\n",
      "24999996    0.081563\n",
      "24999997    0.054688\n",
      "24999998    0.039531\n",
      "24999999    0.036094\n",
      "Name: hpmf_raw, Length: 25000000, dtype: float64\n",
      "Sample data for key 'skyview_raw': 0           0.985343\n",
      "1           0.987487\n",
      "2           0.989449\n",
      "3           0.991004\n",
      "4           0.992041\n",
      "              ...   \n",
      "24999995    0.992455\n",
      "24999996    0.993031\n",
      "24999997    0.993530\n",
      "24999998    0.993864\n",
      "24999999    0.994098\n",
      "Name: skyview_raw, Length: 25000000, dtype: float64\n",
      "Sample data for key 'impoundment_raw': 0           0.000000\n",
      "1           0.000000\n",
      "2           0.000000\n",
      "3           0.000000\n",
      "4           0.000000\n",
      "              ...   \n",
      "24999995    0.000000\n",
      "24999996    0.000000\n",
      "24999997    0.000000\n",
      "24999998    0.002672\n",
      "24999999    0.008017\n",
      "Name: impoundment_raw, Length: 25000000, dtype: float32\n",
      "Sample data for key 'slope_raw': 0            4.346354\n",
      "1            4.346354\n",
      "2           19.598230\n",
      "3           19.391432\n",
      "4            0.286319\n",
      "              ...    \n",
      "24999995    16.324064\n",
      "24999996     9.982392\n",
      "24999997     9.982392\n",
      "24999998    12.995975\n",
      "24999999    14.270212\n",
      "Name: slope_raw, Length: 25000000, dtype: float32\n",
      "Sample data for key 'DEM_ditch_detection': 0           0.352005\n",
      "1           0.352005\n",
      "2           0.352005\n",
      "3           0.000000\n",
      "4           0.000000\n",
      "              ...   \n",
      "24999995    0.000000\n",
      "24999996    0.000000\n",
      "24999997    0.000000\n",
      "24999998    0.000000\n",
      "24999999    0.000000\n",
      "Name: DEM_ditch_detection, Length: 25000000, dtype: float64\n",
      "Sample data for key 'DEM_ditch_detection_streams': 0           0.422406\n",
      "1           0.422406\n",
      "2           0.422406\n",
      "3           0.000000\n",
      "4           0.000000\n",
      "              ...   \n",
      "24999995    0.000000\n",
      "24999996    0.000000\n",
      "24999997    0.000000\n",
      "24999998    0.000000\n",
      "24999999    0.000000\n",
      "Name: DEM_ditch_detection_streams, Length: 25000000, dtype: float64\n",
      "Sample data for key 'conic_mean': 0           0.977083\n",
      "1           0.975128\n",
      "2           0.974784\n",
      "3           0.975421\n",
      "4           0.976026\n",
      "              ...   \n",
      "24999995    0.992455\n",
      "24999996    0.993031\n",
      "24999997    0.993530\n",
      "24999998    0.993864\n",
      "24999999    0.994098\n",
      "Name: conic_mean, Length: 25000000, dtype: float64\n",
      "Sample data for key 'skyview_gabor': 0           0.066817\n",
      "1           0.067805\n",
      "2           0.069603\n",
      "3           0.071881\n",
      "4           0.074208\n",
      "              ...   \n",
      "24999995    0.075637\n",
      "24999996    0.076815\n",
      "24999997    0.077801\n",
      "24999998    0.078512\n",
      "24999999    0.078884\n",
      "Name: skyview_gabor, Length: 25000000, dtype: float64\n",
      "Sample data for key 'skyview_min_6': 0           0.776088\n",
      "1           0.776088\n",
      "2           0.776088\n",
      "3           0.776088\n",
      "4           0.776088\n",
      "              ...   \n",
      "24999995    0.776088\n",
      "24999996    0.776088\n",
      "24999997    0.776088\n",
      "24999998    0.776088\n",
      "24999999    0.776088\n",
      "Name: skyview_min_6, Length: 25000000, dtype: float64\n",
      "Sample data for key 'skyview_median_2': 0           0.995262\n",
      "1           0.995262\n",
      "2           0.995262\n",
      "3           0.995262\n",
      "4           0.995262\n",
      "              ...   \n",
      "24999995    0.995262\n",
      "24999996    0.995262\n",
      "24999997    0.995262\n",
      "24999998    0.995262\n",
      "24999999    0.995262\n",
      "Name: skyview_median_2, Length: 25000000, dtype: float64\n",
      "Sample data for key 'skyview_median_6': 0           0.995262\n",
      "1           0.995262\n",
      "2           0.995262\n",
      "3           0.995262\n",
      "4           0.995262\n",
      "              ...   \n",
      "24999995    0.995262\n",
      "24999996    0.995262\n",
      "24999997    0.995262\n",
      "24999998    0.995262\n",
      "24999999    0.995262\n",
      "Name: skyview_median_6, Length: 25000000, dtype: float64\n",
      "Sample data for key 'skyview_max_2': 0           1.0\n",
      "1           1.0\n",
      "2           1.0\n",
      "3           1.0\n",
      "4           1.0\n",
      "           ... \n",
      "24999995    1.0\n",
      "24999996    1.0\n",
      "24999997    1.0\n",
      "24999998    1.0\n",
      "24999999    1.0\n",
      "Name: skyview_max_2, Length: 25000000, dtype: float64\n",
      "Sample data for key 'skyview_max_4': 0           1.0\n",
      "1           1.0\n",
      "2           1.0\n",
      "3           1.0\n",
      "4           1.0\n",
      "           ... \n",
      "24999995    1.0\n",
      "24999996    1.0\n",
      "24999997    1.0\n",
      "24999998    1.0\n",
      "24999999    1.0\n",
      "Name: skyview_max_4, Length: 25000000, dtype: float64\n",
      "Sample data for key 'skyview_max_6': 0           1.0\n",
      "1           1.0\n",
      "2           1.0\n",
      "3           1.0\n",
      "4           1.0\n",
      "           ... \n",
      "24999995    1.0\n",
      "24999996    1.0\n",
      "24999997    1.0\n",
      "24999998    1.0\n",
      "24999999    1.0\n",
      "Name: skyview_max_6, Length: 25000000, dtype: float64\n",
      "Sample data for key 'skyview_std_6': 0           0.013506\n",
      "1           0.013506\n",
      "2           0.013506\n",
      "3           0.013506\n",
      "4           0.013506\n",
      "              ...   \n",
      "24999995    0.013506\n",
      "24999996    0.013506\n",
      "24999997    0.013506\n",
      "24999998    0.013506\n",
      "24999999    0.013506\n",
      "Name: skyview_std_6, Length: 25000000, dtype: float64\n",
      "Sample data for key 'impoundment_amplified': 0           614.834045\n",
      "1           617.806763\n",
      "2           620.537354\n",
      "3           634.250793\n",
      "4           635.771301\n",
      "               ...    \n",
      "24999995    340.486389\n",
      "24999996    326.133118\n",
      "24999997    321.764191\n",
      "24999998    318.552216\n",
      "24999999    318.552216\n",
      "Name: impoundment_amplified, Length: 25000000, dtype: float32\n",
      "Sample data for key 'impoundment_mean_2': 0           0.009293\n",
      "1           0.009293\n",
      "2           0.009293\n",
      "3           0.009293\n",
      "4           0.009293\n",
      "              ...   \n",
      "24999995    0.009293\n",
      "24999996    0.009293\n",
      "24999997    0.009293\n",
      "24999998    0.009293\n",
      "24999999    0.009293\n",
      "Name: impoundment_mean_2, Length: 25000000, dtype: float32\n",
      "Sample data for key 'impoundment_mean_3': 0           0.009293\n",
      "1           0.009293\n",
      "2           0.009293\n",
      "3           0.009293\n",
      "4           0.009293\n",
      "              ...   \n",
      "24999995    0.009293\n",
      "24999996    0.009293\n",
      "24999997    0.009293\n",
      "24999998    0.009293\n",
      "24999999    0.009293\n",
      "Name: impoundment_mean_3, Length: 25000000, dtype: float32\n",
      "Sample data for key 'impoundment_mean_6': 0           0.009293\n",
      "1           0.009293\n",
      "2           0.009293\n",
      "3           0.009293\n",
      "4           0.009293\n",
      "              ...   \n",
      "24999995    0.009293\n",
      "24999996    0.009293\n",
      "24999997    0.009293\n",
      "24999998    0.009293\n",
      "24999999    0.009293\n",
      "Name: impoundment_mean_6, Length: 25000000, dtype: float32\n",
      "Sample data for key 'impoundment_median_2': 0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "           ... \n",
      "24999995    0.0\n",
      "24999996    0.0\n",
      "24999997    0.0\n",
      "24999998    0.0\n",
      "24999999    0.0\n",
      "Name: impoundment_median_2, Length: 25000000, dtype: float32\n",
      "Sample data for key 'impoundment_median_4': 0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "           ... \n",
      "24999995    0.0\n",
      "24999996    0.0\n",
      "24999997    0.0\n",
      "24999998    0.0\n",
      "24999999    0.0\n",
      "Name: impoundment_median_4, Length: 25000000, dtype: float32\n",
      "Sample data for key 'impoundment_median_6': 0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "           ... \n",
      "24999995    0.0\n",
      "24999996    0.0\n",
      "24999997    0.0\n",
      "24999998    0.0\n",
      "24999999    0.0\n",
      "Name: impoundment_median_6, Length: 25000000, dtype: float32\n",
      "Sample data for key 'impoundment_max_6': 0           0.951036\n",
      "1           0.951036\n",
      "2           0.951036\n",
      "3           0.951036\n",
      "4           0.951036\n",
      "              ...   \n",
      "24999995    0.951036\n",
      "24999996    0.951036\n",
      "24999997    0.951036\n",
      "24999998    0.951036\n",
      "24999999    0.951036\n",
      "Name: impoundment_max_6, Length: 25000000, dtype: float32\n",
      "Sample data for key 'impoundment_std_4': 0           0.026098\n",
      "1           0.026098\n",
      "2           0.026098\n",
      "3           0.026098\n",
      "4           0.026098\n",
      "              ...   \n",
      "24999995    0.026098\n",
      "24999996    0.026098\n",
      "24999997    0.026098\n",
      "24999998    0.026098\n",
      "24999999    0.026098\n",
      "Name: impoundment_std_4, Length: 25000000, dtype: float32\n",
      "Sample data for key 'impoundment_std_6': 0           0.026098\n",
      "1           0.026098\n",
      "2           0.026098\n",
      "3           0.026098\n",
      "4           0.026098\n",
      "              ...   \n",
      "24999995    0.026098\n",
      "24999996    0.026098\n",
      "24999997    0.026098\n",
      "24999998    0.026098\n",
      "24999999    0.026098\n",
      "Name: impoundment_std_6, Length: 25000000, dtype: float32\n",
      "Sample data for key 'hpmf_f': 0           0.129103\n",
      "1           0.617068\n",
      "2           1.000000\n",
      "3           1.000000\n",
      "4           1.000000\n",
      "              ...   \n",
      "24999995    0.231947\n",
      "24999996    0.091904\n",
      "24999997    0.284464\n",
      "24999998    0.474836\n",
      "24999999    0.115974\n",
      "Name: hpmf_f, Length: 25000000, dtype: float32\n",
      "Sample data for key 'hpmf_f_visualisation': 0           False\n",
      "1           False\n",
      "2           False\n",
      "3           False\n",
      "4           False\n",
      "            ...  \n",
      "24999995    False\n",
      "24999996    False\n",
      "24999997    False\n",
      "24999998    False\n",
      "24999999    False\n",
      "Name: hpmf_f_visualisation, Length: 25000000, dtype: bool\n",
      "Sample data for key 'hpmf_min_4': 0          -0.804063\n",
      "1          -0.804063\n",
      "2          -0.804063\n",
      "3          -0.804063\n",
      "4          -0.804063\n",
      "              ...   \n",
      "24999995   -0.804063\n",
      "24999996   -0.804063\n",
      "24999997   -0.804063\n",
      "24999998   -0.804063\n",
      "24999999   -0.804063\n",
      "Name: hpmf_min_4, Length: 25000000, dtype: float64\n",
      "Sample data for key 'hpmf_mean_3': 0           0.065638\n",
      "1           0.065638\n",
      "2           0.065638\n",
      "3           0.065638\n",
      "4           0.065638\n",
      "              ...   \n",
      "24999995    0.065638\n",
      "24999996    0.065638\n",
      "24999997    0.065638\n",
      "24999998    0.065638\n",
      "24999999    0.065638\n",
      "Name: hpmf_mean_3, Length: 25000000, dtype: float64\n",
      "Sample data for key 'hpmf_mean_6': 0           0.065638\n",
      "1           0.065638\n",
      "2           0.065638\n",
      "3           0.065638\n",
      "4           0.065638\n",
      "              ...   \n",
      "24999995    0.065638\n",
      "24999996    0.065638\n",
      "24999997    0.065638\n",
      "24999998    0.065638\n",
      "24999999    0.065638\n",
      "Name: hpmf_mean_6, Length: 25000000, dtype: float64\n",
      "Sample data for key 'hpmf_mean_4': 0           0.065638\n",
      "1           0.065638\n",
      "2           0.065638\n",
      "3           0.065638\n",
      "4           0.065638\n",
      "              ...   \n",
      "24999995    0.065638\n",
      "24999996    0.065638\n",
      "24999997    0.065638\n",
      "24999998    0.065638\n",
      "24999999    0.065638\n",
      "Name: hpmf_mean_4, Length: 25000000, dtype: float64\n",
      "Sample data for key 'hpmf_median_4': 0           0.043594\n",
      "1           0.043594\n",
      "2           0.043594\n",
      "3           0.043594\n",
      "4           0.043594\n",
      "              ...   \n",
      "24999995    0.043594\n",
      "24999996    0.043594\n",
      "24999997    0.043594\n",
      "24999998    0.043594\n",
      "24999999    0.043594\n",
      "Name: hpmf_median_4, Length: 25000000, dtype: float64\n",
      "Sample data for key 'hpmf_std_6': 0           0.082442\n",
      "1           0.082442\n",
      "2           0.082442\n",
      "3           0.082442\n",
      "4           0.082442\n",
      "              ...   \n",
      "24999995    0.082442\n",
      "24999996    0.082442\n",
      "24999997    0.082442\n",
      "24999998    0.082442\n",
      "24999999    0.082442\n",
      "Name: hpmf_std_6, Length: 25000000, dtype: float64\n",
      "Sample data for key 'slope_channels': 0           10.0\n",
      "1            6.0\n",
      "2            6.0\n",
      "3           10.0\n",
      "4           10.0\n",
      "            ... \n",
      "24999995    13.0\n",
      "24999996    13.0\n",
      "24999997    13.0\n",
      "24999998     6.0\n",
      "24999999     6.0\n",
      "Name: slope_channels, Length: 25000000, dtype: float32\n",
      "Sample data for key 'slope_min_2': 0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "           ... \n",
      "24999995    0.0\n",
      "24999996    0.0\n",
      "24999997    0.0\n",
      "24999998    0.0\n",
      "24999999    0.0\n",
      "Name: slope_min_2, Length: 25000000, dtype: float32\n",
      "Sample data for key 'slope_min_4': 0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "           ... \n",
      "24999995    0.0\n",
      "24999996    0.0\n",
      "24999997    0.0\n",
      "24999998    0.0\n",
      "24999999    0.0\n",
      "Name: slope_min_4, Length: 25000000, dtype: float32\n",
      "Sample data for key 'slope_min_6': 0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "           ... \n",
      "24999995    0.0\n",
      "24999996    0.0\n",
      "24999997    0.0\n",
      "24999998    0.0\n",
      "24999999    0.0\n",
      "Name: slope_min_6, Length: 25000000, dtype: float32\n",
      "Sample data for key 'slope_median_6': 0           5.426882\n",
      "1           5.426882\n",
      "2           5.426882\n",
      "3           5.426882\n",
      "4           5.426882\n",
      "              ...   \n",
      "24999995    5.426882\n",
      "24999996    5.426882\n",
      "24999997    5.426882\n",
      "24999998    5.426882\n",
      "24999999    5.426882\n",
      "Name: slope_median_6, Length: 25000000, dtype: float32\n",
      "Sample data for key 'slope_std_4': 0           10.537041\n",
      "1           10.537041\n",
      "2           10.537041\n",
      "3           10.537041\n",
      "4           10.537041\n",
      "              ...    \n",
      "24999995    10.537041\n",
      "24999996    10.537041\n",
      "24999997    10.537041\n",
      "24999998    10.537041\n",
      "24999999    10.537041\n",
      "Name: slope_std_4, Length: 25000000, dtype: float32\n",
      "Sample data for key 'slope_std_6': 0           10.537041\n",
      "1           10.537041\n",
      "2           10.537041\n",
      "3           10.537041\n",
      "4           10.537041\n",
      "              ...    \n",
      "24999995    10.537041\n",
      "24999996    10.537041\n",
      "24999997    10.537041\n",
      "24999998    10.537041\n",
      "24999999    10.537041\n",
      "Name: slope_std_6, Length: 25000000, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "pickle_file = '../../01_Data/01_Raw/features/zone_1.pickle'  # Path to the pickle file\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Print the keys of the data to inspect its structure\n",
    "print(\"Available keys in the pickle file:\", data.keys())\n",
    "\n",
    "# If the data is a large structure, you may also want to inspect a sample of the values\n",
    "for key in data.keys():\n",
    "    print(f\"Sample data for key '{key}':\", data[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type: <class 'pandas.core.frame.DataFrame'>\n",
      "Content preview:           label_3m  hpmf_raw  skyview_raw  impoundment_raw  slope_raw  \\\n",
      "0                0  0.072656     0.985343         0.000000   4.346354   \n",
      "1                0  0.079219     0.987487         0.000000   4.346354   \n",
      "2                0  0.073750     0.989449         0.000000  19.598230   \n",
      "3                0  0.056250     0.991004         0.000000  19.391432   \n",
      "4                0  0.038750     0.992041         0.000000   0.286319   \n",
      "...            ...       ...          ...              ...        ...   \n",
      "24999995         0  0.108438     0.992455         0.000000  16.324064   \n",
      "24999996         0  0.081563     0.993031         0.000000   9.982392   \n",
      "24999997         0  0.054688     0.993530         0.000000   9.982392   \n",
      "24999998         0  0.039531     0.993864         0.002672  12.995975   \n",
      "24999999         0  0.036094     0.994098         0.008017  14.270212   \n",
      "\n",
      "          DEM_ditch_detection  DEM_ditch_detection_streams  conic_mean  \\\n",
      "0                    0.352005                     0.422406    0.977083   \n",
      "1                    0.352005                     0.422406    0.975128   \n",
      "2                    0.352005                     0.422406    0.974784   \n",
      "3                    0.000000                     0.000000    0.975421   \n",
      "4                    0.000000                     0.000000    0.976026   \n",
      "...                       ...                          ...         ...   \n",
      "24999995             0.000000                     0.000000    0.992455   \n",
      "24999996             0.000000                     0.000000    0.993031   \n",
      "24999997             0.000000                     0.000000    0.993530   \n",
      "24999998             0.000000                     0.000000    0.993864   \n",
      "24999999             0.000000                     0.000000    0.994098   \n",
      "\n",
      "          skyview_gabor  skyview_min_6  ...  hpmf_mean_4  hpmf_median_4  \\\n",
      "0              0.066817       0.776088  ...     0.065638       0.043594   \n",
      "1              0.067805       0.776088  ...     0.065638       0.043594   \n",
      "2              0.069603       0.776088  ...     0.065638       0.043594   \n",
      "3              0.071881       0.776088  ...     0.065638       0.043594   \n",
      "4              0.074208       0.776088  ...     0.065638       0.043594   \n",
      "...                 ...            ...  ...          ...            ...   \n",
      "24999995       0.075637       0.776088  ...     0.065638       0.043594   \n",
      "24999996       0.076815       0.776088  ...     0.065638       0.043594   \n",
      "24999997       0.077801       0.776088  ...     0.065638       0.043594   \n",
      "24999998       0.078512       0.776088  ...     0.065638       0.043594   \n",
      "24999999       0.078884       0.776088  ...     0.065638       0.043594   \n",
      "\n",
      "          hpmf_std_6  slope_channels  slope_min_2  slope_min_4  slope_min_6  \\\n",
      "0           0.082442            10.0          0.0          0.0          0.0   \n",
      "1           0.082442             6.0          0.0          0.0          0.0   \n",
      "2           0.082442             6.0          0.0          0.0          0.0   \n",
      "3           0.082442            10.0          0.0          0.0          0.0   \n",
      "4           0.082442            10.0          0.0          0.0          0.0   \n",
      "...              ...             ...          ...          ...          ...   \n",
      "24999995    0.082442            13.0          0.0          0.0          0.0   \n",
      "24999996    0.082442            13.0          0.0          0.0          0.0   \n",
      "24999997    0.082442            13.0          0.0          0.0          0.0   \n",
      "24999998    0.082442             6.0          0.0          0.0          0.0   \n",
      "24999999    0.082442             6.0          0.0          0.0          0.0   \n",
      "\n",
      "          slope_median_6  slope_std_4  slope_std_6  \n",
      "0               5.426882    10.537041    10.537041  \n",
      "1               5.426882    10.537041    10.537041  \n",
      "2               5.426882    10.537041    10.537041  \n",
      "3               5.426882    10.537041    10.537041  \n",
      "4               5.426882    10.537041    10.537041  \n",
      "...                  ...          ...          ...  \n",
      "24999995        5.426882    10.537041    10.537041  \n",
      "24999996        5.426882    10.537041    10.537041  \n",
      "24999997        5.426882    10.537041    10.537041  \n",
      "24999998        5.426882    10.537041    10.537041  \n",
      "24999999        5.426882    10.537041    10.537041  \n",
      "\n",
      "[25000000 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the pickle file\n",
    "pickle_file = '../../01_Data/01_Raw/features/zone_1.pickle'  # Path to the pickle file\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Step 2: Inspect the loaded data\n",
    "if isinstance(data, np.ndarray):  # If it's an array\n",
    "    print(\"Loaded data is an ndarray with shape:\", data.shape)\n",
    "    print(\"Sample values:\", data)\n",
    "\n",
    "    # Visualize as a raster plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(data, cmap='Blues', interpolation='nearest')\n",
    "    plt.colorbar(label='Values')\n",
    "    plt.title('Raster Plot of Zone 1 Data')\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.show()\n",
    "\n",
    "elif isinstance(data, list):  # If it's a list\n",
    "    print(\"Loaded data is a list with length:\", len(data))\n",
    "    print(\"Sample first entry:\", data[0])\n",
    "\n",
    "    # If the first entry is an array, visualize it\n",
    "    if isinstance(data[0], np.ndarray):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(data[0], cmap='Blues', interpolation='nearest')\n",
    "        plt.colorbar(label='Values')\n",
    "        plt.title('Raster Plot of First Array in List')\n",
    "        plt.xlabel('X-coordinate')\n",
    "        plt.ylabel('Y-coordinate')\n",
    "        plt.show()\n",
    "elif isinstance(data, dict):  # If it's a dictionary\n",
    "    print(\"Loaded data is a dictionary with keys:\", data.keys())\n",
    "    for key in data:\n",
    "        print(f\"Key '{key}' contains data of type {type(data[key])}\")\n",
    "\n",
    "else:  # If it's something else\n",
    "    print(\"Loaded data type:\", type(data))\n",
    "    print(\"Content preview:\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 5825003859 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"File size:\", os.path.getsize('../../01_Data/01_Raw/features/zone_5.pickle'), \"bytes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
